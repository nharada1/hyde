<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Partially Observable and Q-Learning &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Partially Observable and Q-Learning</h1>
  <span class="post-date">13 Apr 2015</span>
  <h2 id="optimal-control">Optimal Control</h2>
<p>Remember that in the planning setting we did value iteration, where we knew everything except the optimal values. This includes the reward function as well as the transition probabilities. Value iteration just iterates, updating the policy as we reevaluate the value function. Can we prove that this algorithm converges? Yes</p>

<p>Similar to previous proof, somehow has to do with contraction. See picture or result in book.</p>

<h2 id="policy-iteration">Policy Iteration</h2>
<p>Start with an arbitrary policy. Do policy evaluation to get the value function for this policy. Then update the policy using the greedy update procedure and repeat. Stop when the value function stops changing. Why is this guaranteed to improve?</p>

<h2 id="q-learning">Q-Learning</h2>
<p>Let’s say that Q* is the best you can do in terms of rewards at state S taking action A. We can do Q-value iteration, which is the Q value version of value iteration. Again, we assume we know the reward function and transition probabilities. To update we do something similar to temporal difference. We mix the old value plus a new guess to get the next iteration. </p>

<p><strong>Exam Question</strong>: stochastic approximation conditions</p>

<p>In Q-Learning, the non-linear max operator is inside the expectation, so we can get a non-biased estimate. How do we initialize? Optimistic initialization. If we do this then we explore early. </p>

<h2 id="sarsa">SARSA</h2>
<p>Very similar to Q learning. Q learning considers actions you could take in the next state, while SARSA considers actions you actually take. Basically just no max. This algorithm converges to the expected value of the discounted rewards you’d get if you started in state s and took action a then followed policy pi thereafter. </p>

<p>How can we pick actions to get Q*? We could pick actions greedily, but this would get us stuck. Behave with an epsilon-greedy policy, shrinking epsilon over time. SARSA is <em>on-policy</em> while Q-learning is <em>off-policy</em>, meaning that SARSA converges to the value of the policy you are behaving on.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
