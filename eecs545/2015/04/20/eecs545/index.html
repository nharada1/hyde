<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RL Review &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">RL Review</h1>
  <span class="post-date">20 Apr 2015</span>
  <h2 id="review">Review</h2>
<p>Supervised Learning Problems:</p>

<ul>
  <li>Generalizability</li>
</ul>

<p>Reinforcement Learning Problems:</p>

<ul>
  <li>Exploration-exploitation problem</li>
  <li>Credit assignment/delayed reward</li>
</ul>

<h3 id="bandit-problem">Bandit Problem</h3>
<p>Bandit problem suffers from exploration-exploitation problem. At the limit, you’ll be picking the optimal arm with 1-e probability. You are <em>not</em> picking optimally because at the limit you can still choose randomly. If you want to always pick the optimal arm you need to decrease epsilon to zero at the appropriate rate. The harmonic series works for this. </p>

<p>Sum of epsilon should be infinity, sum of epsilon squared should be finite.</p>

<p>Contextual bandits add the problem of generalization to the problem of exploration-exploitation. In this case, we get a context vector at each time and have to pick one of the arms. In this case we need a model by making the assumption that each arm has a theta vector (regression problem) that is time invariant. </p>

<script type="math/tex; mode=display"> x^T \Theta_i = E[R_i \mid x] </script>

<h3 id="ucb">UCB</h3>
<p>At least know what this <strong>is</strong> because we did cover it.</p>

<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>First we say that there are transition probabilities given the past actions and observations. Then we make the Markov assumption, which (first order) says that we only rely on the previous time step. When we are in a Markov setting we replace the O (observation) with S (state space). The problem is to find a policy that maps from histories to actions. We want to maximize the expected value of the discounted sum of rewards over time. </p>

<p>** Exam Question: ** What happens in a k-th order Markov process?</p>

<p>Let’s talk discount factors. If we have a gamma of one, we have a problem where an infinite horizon occurs and then our expected reward can diverge or become infinite. If we have an <em>absorbing state</em> then we can use a gamma of one, because that represents an <em>episode</em> where thing eventually end. For example, in chess there is an end and after that there are no more moves possible. In many real world problems you will use gamma of one and have an episodic problem.</p>

<ul>
  <li>Fixed finite horizon: You have n steps to act</li>
  <li>Episodic: Act forever, but stop if you hit a certain state</li>
</ul>

<h3 id="policy-evaluation-problem">Policy Evaluation Problem</h3>
<p>We’re given a policy, now evaluate its quality. We discussed three algorithms. Planning setting is matrix inversion and iteration, learning is temporal difference. Monte Carlo can be applied to the learning setting, but is generally planning. Planning means we have reward function and transition probabilities, in learning we have to act in the world to gain information:</p>

<p>** Exam Question: ** How does the proof for convergence work?</p>

<ul>
  <li>Monte Carlo - $O(n)$. Start in state i, we get a trajectory, and eventually we stop. We average the rewards we get for many trajectories. This algorithm is model based because we are required to come back to state i, which we can’t always do in the real world.</li>
  <li>Matrix Inversion - $O(n^3)$. Allows us to exactly solve the problem in one step, but takes a long time depending on size of the state space.</li>
  <li>Iteration - $O(n^2)$. Iterative method to solve matrix inversion indirectly. Faster per iteration but requires many steps.</li>
  <li>Temporal Difference (indirect) - Estimate our values by acting and observing and then estimating the expected reward at each state. We look ahead a certain amount.</li>
</ul>

<h3 id="optimal-control">Optimal Control</h3>
<p>We want to find the best policy for an MDP.</p>

<ul>
  <li>Value Iteration - Use the Bellman optimality equation. We replace the iterating equation in policy evaluation with a similar function but instead we take the max action value. Note that Q-value iteration is similar here, but that the max is inside the expectation which allows us to get unbiased samples. In Q-learning, we can prove that we will converge to the optimal policy as long as we decrease alpha similar to epsilon in the bandit problem. Similarly, epsilon greedy gets infinite exploration, and like bandits does not get optimal control without decreasing epsilon.</li>
  <li>Policy Iteration - Two step process, we calculate the policy, then improve the policy iteratively. This converges much more quickly than value iteration and has definitive stopping conditions</li>
  <li>SARSA - Similar to Q-learning, but is on-policy, meaning that we must behave in a special way for convergence. This allows us to converge faster if we behave correctly. In order to converge to the optimal value we have to be greedy in the limit. Greedily in the limit with infinite exploration (GLIE). Note that SARSA has no max at all, because we evaluate based on what we are doing at the time.</li>
</ul>

<h3 id="depth-d-planningreceeding-horizon-control">Depth D Planning/Receeding Horizon Control</h3>
<p>Build out a tree of actions of depth D, and do backwards induction to determine the best option. Basically we plan ahead for D steps and take an action, then the horizon receeds, and we repeat (we still plan ahead D steps). The horizon is exponential, but we can also do a sampling based version which allows us to compute this much more quickly. When we do this we lose dependence on the size of the state space (but we are still exponential on size of horizon).</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
