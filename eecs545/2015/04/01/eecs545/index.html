<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Reinforcement Learning &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Reinforcement Learning</h1>
  <span class="post-date">01 Apr 2015</span>
  <h2 id="bandits">Bandits</h2>
<p>Recall the bandit problem. We don’t know what the distribution of each bandit is, but we want to maximize our reward. This will transition into MDPs. First let’s talk about notation:</p>

<ul>
  <li>Action values (Q) is the average reward for a specific action.</li>
</ul>

<p>Epsilon greedy algorithm for playing bandits:</p>

<p>We select a greedy arm most of the time, and a random arm some other percentage of the time. We can also start with a high epsilon and decrease it. With a very small epsilon we’ll find almost the optimal. How fast can we shrink epsilon though? We need to explore but if we shrink too slow we’ll perform worse. We’ll call this change in the parameter the <strong>schedule</strong>. </p>

<p>For episilon-decreasing, we can bound our sum on epsilon by a few things. One, we want the sum to be infinity. Two, we want the sum of epsilon to be finite. A harmonic series satisfies this property. In practice we have bigger steps and we reduce the learning rate, because in practice 1/t is too slow. </p>

<p>Softmax Procedure:</p>

<p>Another version is to set the probability of choosing a lever as the softmax of the lever. Temperature is a parameter of the softmax, and with a higher temperature you’ll have a more even probability. </p>

<p>Bandits are a pure exploration-exploitation problem. They have no generalization problems.</p>

<h2 id="final-and-logistics">Final and logistics</h2>

<p>No project. Two more homeworks. LOL. Remaining 20 percent will go to the better of the exams. </p>

<h2 id="contextual-bandits">Contextual-bandits</h2>
<p>We can choose levers based on context, and then we can generalize the actions based on new contexts. </p>

<h2 id="markov-decision-process">Markov Decision Process</h2>
<p>Notation</p>

<ul>
  <li>Ot is the observation at time t</li>
  <li>Rt is the reward at time t</li>
  <li>At is the action at time t</li>
</ul>

<p>The agent wants to maximize a reward given observations by taking actions. There is some discount factor because doing something now is better than later. </p>

<p>How does a world satisfy a Markov property? If we have the last observation and action, then in a Markov world this is all we need to characterize the world. If observations are perfect, then we call the observation the state.</p>

<p>An MDP is defined by:</p>

<ul>
  <li>A set of states</li>
  <li>A set of actions</li>
  <li>Transition probabilities</li>
  <li>A reward function</li>
  <li>A discount factor</li>
</ul>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
