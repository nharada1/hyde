<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Classification &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Classification</h1>
  <span class="post-date">21 Jan 2015</span>
  <h2 id="classification-problem">Classification Problem</h2>
<p>Given an input vector, assign it to one of K distinct classes. We can define 0-1 loss as the number of incorrect labels.</p>

<p>Approaches:</p>

<ul>
  <li>Nearest neighbor classifiers</li>
  <li>Discriminant functions (SVM, Perceptron, etc)</li>
  <li>Learn distributions
    <ul>
      <li>Discriminative models (Logistic Regression)</li>
      <li>Generative models (HMM, Naive Bayes)</li>
    </ul>
  </li>
</ul>

<p><strong>Exam Question:</strong> Why is the posterior the quantity of interest?</p>

<h2 id="logistic-regression">Logistic Regression</h2>
<p>Model class posterior using a sigmoid applied to a linear function of the feature vector:</p>

<script type="math/tex; mode=display"> p(C_1 \mid \phi) = y(\phi) = \sigma(\mathbf{x}^T \phi(x)) </script>

<p>Sigmoid is a smooth squashing function, logit function is the inverse of the sigmoid. This classifier generalizes to the <em>softmax</em> function, which allows many classes and not just two. See slides for usual tricks on finding <script type="math/tex"> P(t \mid w) </script> and on gradient descent derivation.</p>

<p><strong>Exam Question:</strong> Why is placing the decision boundary at 0.5 for the logistic classifier correct?</p>

<h2 id="generative-models">Generative Models</h2>
<p>Generative models learn the joint probabilities while discriminative models learn the conditional probabilities. In logistic regression, we make the assumption that the log odds are a linear function of x. </p>

<h2 id="gaussian-linear-discriminant-analysis">Gaussian (Linear) Discriminant Analysis</h2>
<p>Assume that the class conditional density is Gaussian. We make an assumption of what the Gaussians looks like, and then we estimate the parameters from the data. When we get a new data point, we sample from the distribution to determine the probability of each class. Fact: if we model the probabilities as Gaussians with the <strong>same covariance</strong> matrix, then the optimal decision boundry is linear. </p>

<p>Note to self: Look at derivation and do math yourself.</p>

<p>Logistic regression is more flexible about data distribution, but GDA works well when the distribution follows the Gaussian assumption. In contrast, logistic regression requries costly iterative training procedures, but requires less parameters.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
