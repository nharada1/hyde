<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Linear Regression &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Linear Regression</h1>
  <span class="post-date">12 Jan 2015</span>
  <h2 id="updated-grading-policy">Updated Grading Policy</h2>
<p>Now midterm and final, instead there’s a Kaggle based project. We can still do a project if we want. Talk to Kostas about this one to replace the Kaggle with research.</p>

<h2 id="linear-regression">Linear Regression</h2>
<p>We’ll use x for data, <script type="math/tex">\phi</script> for features, and y or t for targets. </p>

<p>General formulation, learn the function:</p>

<script type="math/tex; mode=display"> y(x,\mathbf{w}) = \sum_{j=0}^M w_jx^j = w_0 + \sum_{j=1}^{M-1}w_j \phi_j(\mathbf{x}) </script>

<p>This function is linear (why is this so?) Note that the basis functions do not need to be linear. The basis could be polynomia, or Guassian, or Sigmoidal, or whatever. We want to minimize some objective function, eg SSE. </p>

<p><strong>Batch gradient descent</strong>:</p>

<script type="math/tex; mode=display"> \nabla E(w) = \sum_{n=1}^N (w^T \phi(x_n) - t_n)\phi(x_n) </script>

<script type="math/tex; mode=display"> w := w - \eta \nabla_w E(w) </script>

<p>Also popular is <strong>stocastic gradient descent</strong>, which is online. We also have the option of using the <strong>closed form solution</strong>, which is only possible if we can do batch gradient descent. The closed form solution is:</p>

<script type="math/tex; mode=display"> \hat w = (X^T X)^{-1} X^T y </script>

<p>Note that X^T X is invertible. Also important is that we scale inversion on number of features, not number of datapoints.</p>

<h2 id="intro-to-regularization">Intro to Regularization</h2>
<p>How do we choose the degree of the polynomial for least squares? We can regularize, where we penalize for high amounts of “bend” or “magnitude” in the weights. Quick discussion on L2 vs L1 regularization and the sparsity that L1 provides.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
