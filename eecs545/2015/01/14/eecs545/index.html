<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Linear Regression and Classification &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Linear Regression and Classification</h1>
  <span class="post-date">14 Jan 2015</span>
  <h2 id="probability-review">Probability Review</h2>
<p>Not many notes here. Review axioms. Review additional properties such as unions, law of total prob, Bayes’ rule. Likelihood functions, applications of Bayes’ rule:</p>

<p>Posterior <script type="math/tex">\propto</script> likelihood x prior</p>

<p>Remember that it’s easier to find the likelihood than it is to find the posterior. </p>

<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<p>Choose a parameter setting w that maximizes the likelihood function p(D|w). We’ll use log-likelihood so we can minimize the log (which has nice properties that let us sum instead of multiply.) There is also the Maximum a posteriori estimation, where we maximize p(w|D) instead of p(D|w). In this case the prior is important. These two different estimates will be the same when the prior is uniform.</p>

<h2 id="maximum-likelihood-interpretation-of-linear-least-squares">Maximum Likelihood interpretation of Linear Least Squares</h2>
<p>Assume a stochastic model:</p>

<p><script type="math/tex"> t = y(x, w) + \epsilon </script> where <script type="math/tex"> \epsilon \sim \mathcal{N}(0, \beta^{-1}) </script></p>

<p>this gives likelihood function:</p>

<script type="math/tex; mode=display"> p(t \shortmid x,w,\beta) = \mathcal{N}(t \shortmid y(x,w),\beta^{-1}) </script>

<p>Minimizing the log likelihood will be exactly minimizing the sum of squares error function.</p>

<p><strong>Exam question:</strong> Given a different stochastic model, derive the maximum likelihood solution.</p>

<p><strong>Exam question:</strong> L2 regularized least squares is the MAP estimate of w with a prior of <script type="math/tex"> p(w) \propto e^{\frac{\lambda}{2} \parallel w \parallel^2} </script></p>

<h2 id="locally-weighted-linear-regression">Locally Weighted Linear Regression</h2>
<p>Weighted regression where the weights are dependent on x (query point), and you solve linear regression for each query point x. The wider the Gaussian, the less complex the model. Note that you’ll need to refit the regression for every single query that comes in.</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
