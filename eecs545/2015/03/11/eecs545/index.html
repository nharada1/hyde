<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Expectation Maximization &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Expectation Maximization</h1>
  <span class="post-date">11 Mar 2015</span>
  <h2 id="mixture-model-em">Mixture model EM</h2>
<p>Given observation X and latent variable Z, we can say:</p>

<script type="math/tex; mode=display"> \log P_{\Theta}(X) = \log \sum_Z P_{\Theta}(x,z) </script>

<p>There is a full derivation on CTools. We’ll use (Carly Rae) Jensen’s inequality and introduce a Q(z) that is the expected value of the log likelihood function, with respect to the conditional distribution of Z given X, under the <strong>current parameter estimates</strong>. </p>

<p>The final algorithm has two steps:</p>

<ul>
  <li>Expectation: Find Q, or the expected value with the current parameters</li>
  <li>Maximization: Fix z, and find the parameter that maximizes Q</li>
</ul>

<p>We are maximizing the lower bound! Claim: we have guarenteed to have improved the log likelihood. </p>

<p><strong>Exam Question</strong>: Given a simple variation of GMM, give the E and M steps of the model</p>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>
<p>PCA maximizes the variance of the projection of data into a lower dimensional space. Basically we choose the direction of projection so that the resulting projections have the highest variance. We can find principal components one at a time. We want to find the vector u1 that maximizes the projection variance, using lagrange multipliers. This ends up being the same as finding the eigenvector with the largest eigenvalue of the covariance matricies. BOOM THIS IS MATH.</p>


</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
