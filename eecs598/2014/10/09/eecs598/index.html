<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Data Analysis II &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Data Analysis II</h1>
  <span class="post-date">09 Oct 2014</span>
  <h2 id="clustering">Clustering</h2>

<h3 id="k-means">K-Means</h3>
<p>We’ll randomly assign some centroids to data, assign points to them, and iteratively cluster. Good because it’s simple, but choosing the starting point can be an issue. Density must be “bloblike” and we have to choose starting parameters to begin with. The process is stochastic and so not reproducible. </p>

<h3 id="hierarchical-clustering">Hierarchical Clustering</h3>
<p>We start with each point as a cluster and we merge clusters repeatedly. If we merge the closest clusters repeatedly we’ll eventually end up with a tree of clusters. Once we have a cluster, how do we decide which method to compare the distances between them? We can use minimum, maximum, avg, or centroid.</p>

<h3 id="fuzzy-clustering">Fuzzy Clustering</h3>
<p>Basically K-means but we assign membership to multiple sets and allow each point to “maybe” be in a cluster.</p>

<h2 id="feature-selection">Feature Selection</h2>
<p>We want to rank features for how well they do in our algorithms. How do we choose the features? We can use correlation, try each one, etc. Correlation only lets us look at one feature at a time but in multiple dimensions we might be able to classify. In most cases we’ll use forward or backward feature selection.</p>

<h3 id="principal-component-analysis">Principal Component Analysis</h3>
<p>We try and extract uncorrelated features by creating a set of basis functions in the original feature space. We want to maximize the variance for each component with respect to the original data.</p>

<h3 id="multidimensional-scaling">Multidimensional Scaling</h3>
<p>We want to capture the distance between points in our lower dimensional representation. In MDS we only require a notion of how different points are. This is considered a type of embedding.</p>

<h3 id="manifolds">Manifolds</h3>
<p>What do we do when the distribution of values follows a structured layout, aka a manifold. In this case the Euclidian distance no longer applies. Many times we’ll just embed a manifold into Euclidian space. A common way to do this is via isomap, where we look at the distance over a graph of the points in manifold space.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
