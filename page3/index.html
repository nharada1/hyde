<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/04/15/eecs586/">
        General NP and Parallelism
      </a>
    </h1>

    <span class="post-date">15 Apr 2015</span>

    <h2 id="general-stuff">General Stuff</h2>
<p>Talked about wall-time NP-complete solvers. Discussed exotic solutions such as Moore’s Law and relativistic effects. Most prove impossible. Quick discussion about bitonic sort.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/15/eecs545/">
        Sparse Sampling and POMDPs
      </a>
    </h1>

    <span class="post-date">15 Apr 2015</span>

    <h2 id="what-should-we-know-about-optimal-control">What Should We Know About Optimal Control?</h2>
<ul>
  <li>How does value iteration work</li>
  <li>How does policy iteration work</li>
  <li>Why does value iteration converge</li>
  <li>Why does policy iteration converge</li>
  <li>Describe Q-learning and why/how it works</li>
  <li>Describe SARSA and the difference from Q-learning</li>
</ul>

<h2 id="sparse-sampling">Sparse Sampling</h2>
<p>Creates a generative model to allow us to look ahead in an MDP and choose an optimal action instead of running through the entire MDP. This is basically a greedy algorithm but instead of looking only at the best it looks ahead a certain number. We can do backwards induction to determine our best action. We can relate the search depth to gamma by looking farther ahead the closer our gamma is to one. </p>

<h2 id="partially-observable-markov-decision-process">Partially Observable Markov Decision Process</h2>
<p>Satinder currently giving a fucked up version of the Monty Hall Gameshow problem where you get mauled by a tiger if you lose. In this game there are two possible world states, True State=R or S=L door. We can either open a door or listen. Listening gives us a probability of the true state, but we still don’t know for sure. This is a POMDP. </p>

<p>In a POMDP, we add a sensing model to the MDP, so now some things are not as clear-cut. We can’t see the state directly, but instead have a sensor that tells us with some probability what the actual state is. A POMDP is an HMM with actions! </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/13/eecs545/">
        Partially Observable and Q-Learning
      </a>
    </h1>

    <span class="post-date">13 Apr 2015</span>

    <h2 id="optimal-control">Optimal Control</h2>
<p>Remember that in the planning setting we did value iteration, where we knew everything except the optimal values. This includes the reward function as well as the transition probabilities. Value iteration just iterates, updating the policy as we reevaluate the value function. Can we prove that this algorithm converges? Yes</p>

<p>Similar to previous proof, somehow has to do with contraction. See picture or result in book.</p>

<h2 id="policy-iteration">Policy Iteration</h2>
<p>Start with an arbitrary policy. Do policy evaluation to get the value function for this policy. Then update the policy using the greedy update procedure and repeat. Stop when the value function stops changing. Why is this guaranteed to improve?</p>

<h2 id="q-learning">Q-Learning</h2>
<p>Let’s say that Q* is the best you can do in terms of rewards at state S taking action A. We can do Q-value iteration, which is the Q value version of value iteration. Again, we assume we know the reward function and transition probabilities. To update we do something similar to temporal difference. We mix the old value plus a new guess to get the next iteration. </p>

<p><strong>Exam Question</strong>: stochastic approximation conditions</p>

<p>In Q-Learning, the non-linear max operator is inside the expectation, so we can get a non-biased estimate. How do we initialize? Optimistic initialization. If we do this then we explore early. </p>

<h2 id="sarsa">SARSA</h2>
<p>Very similar to Q learning. Q learning considers actions you could take in the next state, while SARSA considers actions you actually take. Basically just no max. This algorithm converges to the expected value of the discounted rewards you’d get if you started in state s and took action a then followed policy pi thereafter. </p>

<p>How can we pick actions to get Q*? We could pick actions greedily, but this would get us stuck. Behave with an epsilon-greedy policy, shrinking epsilon over time. SARSA is <em>on-policy</em> while Q-learning is <em>off-policy</em>, meaning that SARSA converges to the value of the policy you are behaving on.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/04/08/eecs586/">
        NP-Complete
      </a>
    </h1>

    <span class="post-date">08 Apr 2015</span>

    <h2 id="hardness">Hardness</h2>
<p>A problem is NP-Hard if giving a polynomial time algorithm for it shows that P=NP. Say you wanna solve an NP-hard problem? Now what?</p>

<ol>
  <li>Maybe you have a small number of instances</li>
  <li>Maybe some parameters are fixed</li>
  <li>Maybe it’s easy for special cases of interest</li>
  <li>Maybe you only care about the expected case</li>
  <li>Maybe you only need a close approximation</li>
</ol>

<p>How do we know when special cases are easy? Who knows?</p>

<h2 id="expected-case-analysis">Expected Case Analysis</h2>
<p>For expected case we need to know the probability distribution. </p>

<p>Hamiltonian cycles: recall that it’s a cycle through all verticies where we don’t repeat any of them. Problem: for every pair of vertices, we add an edge with probability p. As n verticies goes to infinity, the probability of having a Hamiltonian cycle goes to 1. In 1987, Gurerich and Shiloch proved that you can find a Hamiltonian cycle or prove that non exists in expected O(n) time in this specific case.</p>

<p>In many cases the NP part of the problem won’t be reached in the expected case if the distribution you’re working with is right.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/08/eecs545/">
        Monte Carlo and POMDP
      </a>
    </h1>

    <span class="post-date">08 Apr 2015</span>

    <h2 id="monte-carlo">Monte Carlo</h2>
<p>Use first-visit Monte Carlo for MDP evaluation, or we can use the linear relationship between value functions. For the equation, <script type="math/tex">R_i^{\pi}</script> represents the you get for a state pi at an immediate action state.</p>

<p><strong>Exam Question:</strong> Prove that the iterative policy evaluation converges, and the error of Vk+1 is no less than gamma.</p>

<p>This is a contraction mapping. No idea what that is.</p>

<p><strong>Exam Question:</strong> How many iterations of value iteration are required to get an epsilon answer?</p>

<p>What about for Monte Carlo? To get an epsilon answer, the number of MC simulations required is independent of the size of the state space! The variance matters, but the size of the state space does not. How many random samples do we need to draw from an arbitrary distribution so that the empirical mean is within epsilon of the true mean with probability of 1-alpha? </p>

<h2 id="temporal-difference-0">Temporal Difference 0</h2>
<p>The idea is that want to update the value of the state at time t, so you look ahead one step. Take the difference of the lookahead estimate and the current estimate. You update the value of your state proportional to the difference.</p>

<h2 id="optimal-policy">Optimal Policy</h2>
<p>Finds the maximum policy via iteration. We can’t solve in closed form, but we can use dynamic programming.</p>

<h2 id="value-iteration">Value Iteration</h2>
<p>A dynamic programming method to compute optimal policies. Next time is Q learning.</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page4">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page2">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
