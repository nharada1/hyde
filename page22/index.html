<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/18/eecs492/">
        Local Search
      </a>
    </h1>

    <span class="post-date">18 Sep 2014</span>

    <h2 id="recap">Recap</h2>
<p>We’ll do a short recap. Maybe, but not likely short.</p>

<h3 id="informed-search">Informed Search</h3>
<p>Informed searches use data about the goal that we have that’s not part of the problem definition. For example, <strong>A-star</strong> search uses a heuristic function to work towards a goal, and applies the node cost as <em>path cost + heuristic cost</em>.</p>

<p>Quick proof: Prove A-star is optimal. This will likely be on the exam.</p>

<p>First prove A-star is non-decreasing cost
Then argue about the exploration of the path cost</p>

<p>There are also memory bound heuristic searchs. <strong>Iterative deepening</strong> A-star is like iterative deepening but cutoff is now f_cost and not the depth. We can also use <strong>recursive best first search</strong> to mimic best first search but cutting memory by only storing one child.</p>

<p>Heuristics can be designed in several ways, one way is to relax the problem. For example, we can solve only part of the problem and create a pattern database with solutions to all the possible subproblems.</p>

<h2 id="local-search">Local Search</h2>
<p>Previously we explored the search space in a systematic way. When a goal is found the path is the solution. In many cases we actually don’t care about the path, the final product is important. During a local search, we evaluate and modify the current state.</p>

<p>Example - 8 queen problem. We only care that this is solved, not the path to the solution. </p>

<p>In local search, we only maintain the current node, and we don’t retain path. This allows us to solve both massive search space problems as well as pure optimization problems. Complete if we reach the goal always, optimal if we always in the local maximum.</p>

<h3 id="hill-climbing">Hill Climbing</h3>
<p>We just move in the direction of the gradient and try and find the nearest peak. This search is not globally optimal unless the objective function is convex. How can we hill climb with the 8 queen problem?</p>

<p>We’ll define a heuristic that keeps track of how many queens conflict, and just go until we’ve hit zero, headed towards the minimum. In some cases we hit the local maximum though, and this isn’t the solution. Classic optimization problem, but how to solve? We can offer stochastic solutions to hill climbing, which generally require many runs. Unfortunately this doesn’t offer a worst case guarantee.</p>

<h3 id="simulated-annealing">Simulated Annealing</h3>
<p>Allows a transition from one non-local state to another. Probability of this transition falls over time. We pick a random move, if it improves we accept it. If not, we either ignore it, or accept it with probability P. </p>

<h3 id="local-beam-search">Local Beam Search</h3>
<p>We track k states instead of only one. Generate k initial states and generate successors for each state, saving only the best. This is basically a best first search that occurs in local space instead of global.</p>

<h3 id="genetic-algorithm">Genetic Algorithm</h3>
<p>We start with randomly generated states and rate each. Then we combine parents and randomly mutate some of them. The process repeats as we proceed, discarding agents with low fitness and keeping better ones. In genetic algorithms, the fitness function is quite important, similar to the heuristic in informed search. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/17/eecs584/">
        Paper Review - The Design of an Acquisitional Query Processor For Sensor Networks
      </a>
    </h1>

    <span class="post-date">17 Sep 2014</span>

    <p>Sensors for long term, low power monitoring present different challenges than traditional database workloads. This paper addresses common problems encountered when creating and deploying these networks, and offers solutions by the way of simple SQL extensions as well as low level query processing. Written in 2003, the issues discussed in this work are even more relevant nowadays - sensors are becoming even cheaper and as the “Internet of Things” looms on the horizon we must confront the reality of large scale and low power ad-hoc networks becoming commonplace. While the techniques in this document pertain mainly to scientific monitoring, they can be applied to home automation, building management, commercial operations, and an endless variety of other uses.</p>

<p>From the writing, this paper appears to consider itself among the first to truly consider a sensor network as a separate entity. Previous works attempt to offer more general solutions such as saving power on transmitting or processing. The novel idea explored in this paper is the idea that a sensor network can control its rate of sampling, and that many times the sampling rate is flexible with respect to other, more important goals (such as network longevity). The authors introduce the query language for their acquisitional query processor, which they call acquisitional query language. The AQL is basically an extension of SQL that allows specification of sampling rates, as well as windowing and view support (which the paper calls materialization points”. The database in the paper, TinyDB, also supports event based queries that trigger based on external events. This is critical to maintaining long battery life (because polling is so expensive) and already supported via interrupts in most hardware. The query processor has a general idea of the power used for each query, and can optimize based on expected power usage both for queries and to ensure longevity of the system (ie set a sample rate based on lifetime).</p>

<p>The paper also explores ways to connect the sensor network, which is done via semantic routing tree. By storing metadata and knowing their parents properties, nodes can conserve power by avoiding unnecessary processing. I really like that this paper addresses these issues because it shows the authors are working on a very specific problem and not just trying to conserve power in a general network system. The sensor network can also adapt to heavy load by changing its sampling rate or choosing which tuples to drop in a congested network. While a short section, this part of the paper is a novel solution to network congestion and power reduction. By reducing or eliminating less useful data, the network can eliminate backlog and congestion while maintaining a good picture of the final data. </p>

<p>While this paper did a good job of addressing the requirements of an ad-hoc sensor network, I think it could do better at implementation in some cases. Dropping data on a packet by packet basis is done crudely - data compression could make a big impact on the quality of the data the network receives. The authors do mention this fact and concede that the low processing power of the network prohibits expensive operations such as Fourier analysis. The authors also make assumptions about the data sent via sensor networks, assuming most of it is real valued and univariate. This paper works very well in scientific monitoring domains, but fails to extend to cases where higher dimensional data or textual data is required. While shortsighted, this is most likely because the authors had a specific application in mind and possibly failed to see the explosion of sensors and low energy devices that would occur in the future.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/16/eecs492/">
        Informed Search
      </a>
    </h1>

    <span class="post-date">16 Sep 2014</span>

    <h2 id="review-from-last-time">Review from Last Time</h2>
<p>Last time we looked at breadth first search and uniform cost search, searches that are generally complete and optimal (UCS is optimal), but are quite time and especially space complex. Important difference to remember:</p>

<p><strong>Breadth First</strong> - Goal test at generation</p>

<p><strong>Uniform Cost</strong> - Goal test at expansion</p>

<p>We also looked at depth first search, which is not optimal but trades off for decreased space complexity. The DFS is complete only if used as a graph search, because in a tree search we could get stuck in a loop. Also not complete in infinite search spaces, obviously. If we limit the depth of DFS we can fix the problem with unbounded trees.</p>

<p><strong>DFS-limited</strong> - Not complete, not optimal, but time complexity, while still exponential, is now <script type="math/tex">O(b^L)</script> where L is the max depth. Same thing with space complexity, now <script type="math/tex">O(bL)</script>.</p>

<p><strong>Iterative Deepening</strong> - Use DFS-limited but instead of fully exploring, expand the depth limit.</p>

<p><strong>Bidirectional Search</strong> - Search from both the goal and the start, saves space by searching from the goal.</p>

<h2 id="informed-search">Informed Search</h2>
<p>We consider “best first search”, where we evaluate nodes to expand based on an evaluation function. We exploit an f(n) to hopefully find the goal faster. </p>

<p><strong>Heuristics</strong> - Uses information not in the problem definition, where h(goal) = 0. This function is critical to a good search. </p>

<p><strong>Greedy Best-First Search</strong> - Expands the node closest to goal. Not necessarily optimal.</p>

<h2 id="a-star-search">A-star Search</h2>
<p>Combines the path-cost as well as a heuristic that looks at straight-line distance to the goal.</p>

<p>Short time out for admissibility. An admissible heuristic never overestimates the cost to reach the goal, eg <script type="math/tex">h(n)</script> &lt; minimum achievable cost from n to goal. Consistency requires monotonic heuristic; it’s a stronger version of admissibility that requires consistency from each next node also.</p>

<p>Tree-search is optimal if heuristic is admissible, and graph-search is optimal if consistent. The proof is important but this lecture was piss poor at explaining it.</p>

<p>If the heuristic is admissible, A-star is complete, optimally, and optimally efficient.</p>

<p>If we are <em>memory bound</em>, we can use either iterative deepening for A-star, or recursive best first search. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/14/eecs584/">
        Paper Review - Automated Selection of Materialized Views and Indexes for SQL Databases
      </a>
    </h1>

    <span class="post-date">14 Sep 2014</span>

    <p>Modern relational and object-relational database systems support materialized views, which provide a more flexible way to encapsulate database structure. These views can offer significant performance gains when properly configured, because unlike indexes they can span multiple tables and have selections or GROUP BY over multiple columns. This paper, written by the Microsoft SQL research team, introduces a solution to automatically select both views and indexes based on “training data” provided by a database administrator. Unlike previous work, this paper focuses on combining index and view selection, as well as providing novel work at selecting materialized views.</p>

<p>The architecture of index and view selection is based on the Microsoft SQL server, and at times reuses portions of that codebase, specifically the query cost estimator and more generally the query optimizer. Because the search space of possible materialized view candidates is quite large, the authors seek to find interesting table subsets via a cost metric that considers the total cost of all queries in the workload and attempts to normalize that by the size of the tables in Q and T, where Q are tables referenced in the query and T is the table-subset. This is a requirement because some tables may be so small that building a view on them would be a waste of time - queries on the table are fast already. The authors also use the Microsoft SQL optimizer to prune the possible views, and then merge the views to try and include views that will answer multiple queries and thus be the most space efficient representations.</p>

<p>Unlike previous works, this paper examines both indexes and materialized views jointly when considering the retained options. This allows the optimizer to choose combinations of views and indexes that may be pruned by examining only one type of structure.</p>

<p>In contrast to many academic research papers, the techniques in this work have already been incorporated into a professional database tool. This allows the authors to create an impressive set of benchmarks and performance statistics. The authors provide convincing evidence that their methods are superior and robust, testing their finished product on several synthetic and real world datasets. An argument could be made that their datasets were not large enough for real world data, but because the competing techniques are so inefficient, the tuning wizard created in this paper is sometimes the only possible option. I also like that the authors offer not only possible future directions, but also point to specific papers when discussing open problems remaining in their area.</p>

<p>While a strong paper, I would have liked to see an overview of the Greedy(m,k) algorithm referenced several times throughout the paper. The authors did provide reference material, but based on the importance of the algorithm it would be beneficial to provide at least a superficial overview. I also think it’s important to remember that because this is a paper that relates directly to a product, the authors have implicit bias towards their own solution. There is little discussion of competing commercial solutions such as Oracle or DB/2; although the paper mentions briefly that the competition has automated tuning available, it never compares its techniques directly.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/13/eecs584/">
        Paper Review - An Overview of Query Optimization in Relational Systems
      </a>
    </h1>

    <span class="post-date">13 Sep 2014</span>

    <p>The declarative SQL language is currently the lingua franca of modern relational, and occasionally non-relational, database systems. Converting such a high level syntax to commands in the DBMS is a difficult task as there are generally multiple representations for a given high level query, and each representation has different and sometimes highly varying performance. This paper seeks to present the basics of query optimization in relation databases. As the paper was written and published by Microsoft, we can assume there are implicit biases towards Microsoft SQL server, but the paper does a good job of addressing historical precedents.</p>

<p>The paper begins by providing a discussion of the System-R optimization framework, which could be considered the seminal work in query optimization, as well as SQL in general. System-R was developed by IBM as the first implementation of SQL, and thus their dynamic programming based query optimizer acted as an example for subsequent query optimization attempts. System-R provides powerful but limited query optimization for join ordering, and most implementations of SQL today use a variant of the dynamic programming idea for join ordering optimization. Unfortunately, current databases support optimization for other operators. For example, select-project-join queries may be flattened or outer joins may be reordered. The optimizer may attempt to collapse multi-block queries into a single block via multiple different methods. </p>

<p>The author provides an overview of cost estimation, and explains how most cost optimization relies on database statistics collected over the life the system, such as histograms or correlation tables. This section is hand waved over, and very little actual information about cost estimation is given. This is likely because cost estimation is worth multiple papers alone, and the author felt he could not adequately present most of the material. The paper presents short looks at two query optimization projects - the query graph model based Starburst, and the goal driven Volcano/Cascades system. Finally, there is a short look at modern problems such as parallelism and user-defined functionality (like, for example, PostgreSQL). I really appreciated that the author included these looks forward as it allows the paper to stay fairly modern even years later. I also felt the paper was strong in examples, although the way these examples were presented had room for improvement. The examples were all in technical terms and understanding them took a significant amount of time. However, for the example with a visual aid I was able to understand the core concept quickly. More graphs and visuals would have greatly improved the accessibility of this paper, which is important in a survey paper like this.</p>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page23">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page21">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
