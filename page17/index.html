<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/10/20/eecs584/">
        Paper Review - A Comparison of Approaches to Large-Scale Data Analysis
      </a>
    </h1>

    <span class="post-date">20 Oct 2014</span>

    <p>When Google announced MapReduce in 2004, it was widely viewed as a paradigm shift in distributed data analysis. The architecture was quickly developed into an open source deriviative, designed to run atop the HDFS, itself modeled after Google’s GFS. This open source combination of software, along with other packages, became Hadoop and positioned itself as the spearhead in the data science and big data movement. In this paper (somehow not surprisingly featuring Michael Stonebreaker), the authors argue that MapReduce is not the silver bullet to data warehouse analysis and that parallel DBMSs are more established and outperform MapReduce in most tasks, while offering ease of use that MapReduce lacks.</p>

<p>MapReduce works by running a set of mapper and reducer tasks over a large set of data, distributing the computation in such a way to make restarts easy in the case of hardware failure. This paper highlights the weaknesses of MapReduce, including but not limited to its runtime performance. The authors equate MapReduce’s requirement to program in Java to the domain specific and hardware specific language of the failed CODASYL, and make a similar argument against the system that several of the authors did years ago against graph based models. This is a reasonable complaint, and at the time this was a strength of the paper. Since publication, it has been accepted that a higher level of abstraction would be required, and SQL to MapReduce translators such as Hive were developed. Additionally, the authors argue against the requirement of placing intermediate data on disk, and point out that parallel DBMSs don’t have this requirement. Again, modern development prevails, and Spark/Shark are designed to do exactly this. It’s clear that this paper had some influence on those systems, as the authors have direct influence on the Berkeley database group that developed Spark/Shark. </p>

<p>The paper also presents several comparisons of MapReduce performance against DBMS-X (almost certainly Teradata), and Vertica (another Stonebraker enterprise). Across all benchmarks, the parallel DBMSs appear to be superior. The authors do note that MapReduce has better failsafe mechanisms than the databases tested, but assert that the performance penalty is not worthwhile. I like that the authors make claims as to why the parallel RDBMSs outperform MapReduce. However, many of the performance reasons are the result of such a mature field. Indexing and internal optimizations developed over years and years of industry have made the RDBMS very powerful at extracting as much performance as possible out of the machines it is installed on. While the authors do make valid claims, it’s important to remember how young this new architecture is. I think one weakness of this paper is that it fails to recognize how important MapReduce’s “many low power machines” architecture is to it’s design. The other database systems cannot even be installed on thousands of machines, and in some cases it may be significantly cheaper to buy many commodity machines (especially for large companies like Google who do so at massive scale). This is overlooked, however, and should be addressed by the authors. Overall this paper is a good reality check for developers who get easily excited over shiny new technology - parallel DBMSs have existed since the Gamma project and for mission critical and time critical applications it is likely worth paying the extra for the much faster system.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/10/18/eecs584/">
        Paper Review - MapReduce: Simplified Data Processing on Large Clusters
      </a>
    </h1>

    <span class="post-date">18 Oct 2014</span>

    <p>Distributed data processing has gained heavy traction as a result of internet scale data warehouses. In 2004, Google introduced MapReduce, a programming model and associated implementation for processing and generating large data sets. MapReduce has been extremely influential in the data processing space, and has become a de facto standard for distributed data analysis, running on Hadoop or similar system. MapReduce earns its name from its programming model, which draws from the functional programming paradigm of mapping a function to data and reducing that data to an aggregate value. Because these operations are atomic, and in most cases referentially transparent, the analysis can easily be parallelized and made fault tolerant.</p>

<p>This paper outlines the details of Google’s MapReduce system and provides examples of when the application is appropriate. MapReduce requires a user to write two functions, a mapping function that applies a function to data and emits key value pairs, and a reducing function that takes key value pairs and reduces them to one output. The intermediate result is stored on the machine, allowing machines to be either a mapper or reducer. This algorithm also allows for machine failure - in the case that a node fails to respond to the master, the node’s jobs are simply placed on other machines. </p>

<p>This paper’s strength lies in the simplicity of the system. I suspect that many people who use MapReduce on a daily basis have never read the paper, and likely do not have significant background with parallel computation. By applying a simple API to a complex problem, Google has allowed data computation on a large scale. Thus, I claim the primary contribution is the idea of using the map-reduce paradigm itself. There are other strengths in this paper, including the idea of backup tasks to reduce stragglers and the methods of splitting data to be used by the workers. I also liked that the paper includes an example program in appendix A, as this highlights the practicality and simplicity of the system. </p>

<p>To achieve such power, the designers of MapReduce had to make several significant tradeoffs. The largest weakness of this system is that the programmer must be able to express their idea in the form of a map-reduce query. This is difficult for tasks that cannot run in isolation (for example training an SVM). Additionally, because MapReduce is a large system, real time processing is impossible. The system is best used for batch processes, and has trouble handling streaming data. One problem that has been addressed since the introduction of MapReduce is that the user must program the system themselves. This is analogous to IMS or CODASYL where the user had to define functions to query the database. Systems to translate SQL or other domain specific languages into MapReduce jobs are now quite popular, for example Hive or Pig. Finally, while MapReduce allows large data sets to be easily processed by storing intermediate results on disk, this is a weakness because each part of the job requires additional IO writes. By placing the results in memory significant speedups could be achieved. This has been explored in Apache’s Spark and in-memory Hadoop.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs598/2014/10/16/eecs598/">
        Biostatistics
      </a>
    </h1>

    <span class="post-date">16 Oct 2014</span>

    <p>How do we model expected outcomes? We can predict either binary or continuously.</p>

<h2 id="binary-methods">Binary Methods</h2>
<p>A - True Positive
B - False Negative
C - False Negative
D - True Negative</p>

<h4 id="accuracy">Accuracy</h4>
<p>Proportion of cases predicted correctly. Rarely used in real systems.
<script type="math/tex"> \frac{A+D}{A+B+C+D} </script></p>

<h4 id="recall">Recall</h4>
<p>Sensitivity, or how well we predict true positives compared to false negatives
<script type="math/tex"> \frac{A}{A+B} </script></p>

<h4 id="specificity">Specificity</h4>
<p>How well we distingish between negatives and false positives
<script type="math/tex"> \frac{D}{C+D} </script></p>

<h4 id="precision">Precision</h4>
<p>Positive predictive value
<script type="math/tex"> \frac{A}{A+C} </script></p>

<h4 id="negative-predictive-power">Negative predictive power</h4>
<p><script type="math/tex"> \frac{D}{D+B} </script></p>

<h4 id="relative-risk">Relative Risk</h4>
<p>How likely an outcome is relative to the alternative
<script type="math/tex"> \frac{A/(A+C)}{B/(B+D)} </script></p>

<h4 id="odds-ratio">Odds ratio</h4>
<p>Odds of actual predictions, does not change based on positive or negative outcomes
<script type="math/tex"> \frac{A/C}{B/D} </script></p>

<h2 id="continuous-methods">Continuous Methods</h2>
<p>Gives a somewhat better model</p>

<h4 id="auc---operating-curve">AUC - Operating Curve</h4>
<p>Lets us see the values at different decision thresholds, generally widely used.</p>

<h4 id="hosmer-lemeshow-test">Hosmer-Lemeshow Test</h4>
<p>Check to see if observed outcomes and predicted outcomes match, but can be fooled by larger samples.
<script type="math/tex"> \sum\limits_{i=1}^n \frac{(O_i - E_i)^2}{E_i(1-E_i/n_i)} </script></p>

<h2 id="comparing-two-systems">Comparing Two Systems</h2>
<p>How can we compare two systems to each other? Well we could just use the metrics we’ve already discussed. We have other systems though:</p>

<h4 id="net-reclassification-improvement-nri">Net Reclassification Improvement (NRI)</h4>
<p>Focuses on binary predictors, and looks at how system B treats observations differently. This gives us a sense of how many patients will be affected by improvements.</p>

<h4 id="integrated-discrimination-improvement-idi">Integrated Discrimination Improvement (IDI)</h4>
<p>Compares differences in discrimination slopes, aka differences of mean of probabilities for outcome vs non-outcome.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/10/16/eecs492/">
        Planning
      </a>
    </h1>

    <span class="post-date">16 Oct 2014</span>

    <p>This lecture is actually more about first order logic.</p>

<h2 id="inference-in-fol">Inference in FOL</h2>
<p><strong>Universal instantiation</strong> is the idea that we can infer any sentence obtained by substituting a <em>ground term</em> for the variable. If we replace a variable by a single new constant it’s called <strong>existential instantiation</strong> and we replace a variable with a single constant. </p>

<p><strong>First order inference</strong> allows us to substitute names for variables, and we can apply this many times over many sentences. This brings us to <strong>unification</strong>, which allows us to find the values of variables to make a sentence logically consistent. </p>

<p>How do we resolve first order logical statements? We can look at chaining again. Let’s look first at <strong>forward chaining</strong>:</p>

<p>In forward chaining we want to apply logic iteratively until we reach a conclusion. However, this is quite inefficient, if intuitive. We recheck each rule each iteration which is a huge waste. There’s a version that’s efficient, but we won’t discuss it.</p>

<p>Obviously the other option of this is <strong>backward chaining</strong>, which is basically just using depth first search. </p>

<h2 id="resolution">Resolution</h2>
<p>Resolution requires conjunctive normal form! </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/10/12/eecs584/">
        Paper Review - Dremel: Interactive Analysis of Web-Scale Datasets
      </a>
    </h1>

    <span class="post-date">12 Oct 2014</span>

    <p>Web scale data analysis is time consuming even with modern technologies. While Map-Reduce offers a flexible model for analyzing large scale data, it is still generally not near interactive speed and requires writing complex Java procedures to run. Dremel is a new take on petabyte-scale data analysis, offering a column storage based, interactive SQL approach to read only data. This new system is quite a bit different from well known SQL based interfaces to Map-Reduce such as Hive or Pig, because instead of simply translating SQL to MapReduce jobs it creates a new architecture in order to deliver results faster.</p>

<p>The primary contribution of this paper is the introduction of a columnar based storage system for nested data, aka data with repeatable and optional fields. Dremel does this by assigning two values to the column data - a repeated field that specifies which reptition the value represents, and a definition level to specify how many fields are actually defined. This layout is quite clever, and allows Dremel to store data in a column format and reap the benefits that come with columns for aggregate queries. The paper also introduces a new query language for the Dremel system, as well as providing a finite state method of reassembling the data structures from columns. Because Dremel is a shared nothing system, it uses a tree architecture to execute queries across many machines, with each eventually moving through the root server. Dremel’s performance seems incredible from the data the paper presents, processing queries an order of magnitude or two faster than MapReduce alone. </p>

<p>With any high performance system comes trade offs, and Dremel is no exception. Data in Dremel is read only, which is a massive limitation for any system that must fufill both storage and OLAP requirements. In order to actually execute data queries so quickly, the user must load up a separate cluster just to do analysis. I like that the paper provides algorithms for each of its components, but it does a poor job of explaining exactly how the underlying column storage works. Because we are never writing to the system, we alleviate most of the traditional database concerns. However, this aspect of the system still likely has pitfalls and gotchas, and an overview would have been nice. I would also be interested to see how data is actually stored on the system and split between nodes, as the actual process of loading data onto the cluster is completely skipped. This is likely because the Dremel team feels no need to introduce a proprietary piece of technology into the paper. Once again, papers from a commercial system lack details that would likely be required for implementation.</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page18">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page16">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
