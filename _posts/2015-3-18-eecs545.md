---
layout: post
title: Decision Trees 
category: eecs545
---
## Information Theory
Decision trees need a way to quantify uncertainty. We'll use entropy, which is "how many bits does it take to transmit this information". In the sense that if the data is completely random you'll need one bit for two symbols, but if it's not you need fewer bits. If it's only one, you need zero bits (since the data is always the same).

We can also define joint entropy and conditional entropy.

Information gain describes how many bits on average it would save me if both ends of the channel already *knew* X. At each node, we want to compute the information gain and that's how we will construct our tree.

## Decision Trees
Decision trees are easy ways to make decisions --- they don't use much computational power and they're interpretable. Note that a decision tree can perfectly fit data as long as there isn't noise. Thus, we either need to stop early or prune the tree in order to prevent overfitting. 
