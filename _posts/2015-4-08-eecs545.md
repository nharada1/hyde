---
layout: post
title: Monte Carlo and POMDP 
category: eecs545
---
## Monte Carlo
Use first-visit Monte Carlo for MDP evaluation, or we can use the linear relationship between value functions. For the equation, $$R_i^{\pi}$$ represents the you get for a state pi at an immediate action state.

**Exam Question:** Prove that the iterative policy evaluation converges, and the error of Vk+1 is no less than gamma.

This is a contraction mapping. No idea what that is.

**Exam Question:** How many iterations of value iteration are required to get an epsilon answer?

What about for Monte Carlo? To get an epsilon answer, the number of MC simulations required is independent of the size of the state space! The variance matters, but the size of the state space does not. How many random samples do we need to draw from an arbitrary distribution so that the empirical mean is within epsilon of the true mean with probability of 1-alpha? 

## Temporal Difference 0
The idea is that want to update the value of the state at time t, so you look ahead one step. Take the difference of the lookahead estimate and the current estimate. You update the value of your state proportional to the difference.

## Optimal Policy
Finds the maximum policy via iteration. We can't solve in closed form, but we can use dynamic programming.

## Value Iteration
A dynamic programming method to compute optimal policies. Next time is Q learning.
