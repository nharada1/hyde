---
layout: post
title: Cross Validation 
category: eecs545
---
## Applications of ML
Two approaches: you can go bottom up or top down. Bottom up is for research and new methods while top down is more for applications. Which way you go is up to you.

## Bias Variance Tradeoff
Variance is how much does the model fit, bias is how well a model fits the data on average. We can view bias as "accuracy" and variance as "precision". Our expected loss is bias^2 + variance + noise. This is a brilliant decomposition: the expected error is **exactly** these three terms and we can prove it. 

This nature of bias and variance means that we generally must choose a tradeoff between the two. Because noise is always present, we have to pick between these parameters. Usually we can find a minima of these terms (especially because bias is squared). In these cases, we usually find that the minimum of the test squared error occurs at this minimum. We find a good balance between the two via cross-validation.

## Cross Validation
Generally the best way to cross validate is to use a train, validation, and test set to avoid overfitting the hyperparameters to the dataset you have. Cross validation is fairly basic - k-Fold or however many folds you want. Leave one out is the same as k-fold if k is n. 
