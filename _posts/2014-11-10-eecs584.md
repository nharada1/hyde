---
layout: post
title: 'Paper Review - Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers' 
category: eecs584
---
While crowdsourcing is a powerful tool for researchers who need a human intelligence for their data, it can be quite expensive to hire individuals for a large data set. This paper explores the cost model of data labeling, and looks at the most effective way to partition funding when accuracy is required. In cases of unlabeled or partially labeled data, gaining a few key labels is quite important. However, because of the cost of those labels, most systems attempt to label only the most important points. This compounds problems when a label coming in is wrong - falsely labeling support vectors in an SVM will reduce accuracy dramatically.

The paper first looks at the idea of repeated labeling. By asking many people to label one data point and taking a vote, most inaccuracy can be removed from the new points. The paper asserts that the probability of a label appearing in the system changes how well the system responds to hand labeled data. In cases where the label is more uncertain, labeling data multiple times improves accuracy dramatically. The paper also discusses the idea of preserving uncertainty in labeling via majority vote. Because some classifiers can use the information about label uncertainty in their decision or training functions, by saving the results of the vote we can increase accuracy (ie a label where all voters agree is a more powerful label than one where one side wins by a few votes). 

I like the paper's discussion of selective repeated-labeling. The intuition is that while repeated labeling can improve accuracy, if we can allocated our total label requests towards less confident samples, we can improve quality without requiring more human requests. That is, we will take some of the repeated labels away from points we're certain about, and move them towards points we aren't. The paper presents a method of estimating label uncertainty that avoids natural purity measures such as entropy. The paper concludes with a fairly comprehensive results section where the authors use mostly synthetic data to test their hypothesis. I wish the authors had used real data instead of introducing noise to existing data benchmarks, as the practically of the system is hard to assess under those conditions.

One weakness of the paper was the assumption of a homogenous environment. Although the authors do relax some assumptions throughout the paper, they fail to account for the possibilities of a real world crowd. The authors mention the failings, specifically that a higher reward may correspond to a better label (so paying more might be a better option than simply repeating many times) and that label difficulty is not static. This paper does provide a strong framework for further exploration into crowd-based reinforcement learning. 
