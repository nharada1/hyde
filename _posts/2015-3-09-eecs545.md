---
layout: post
title: Expectation Maximization
category: eecs545
---
## Learning in Bayes Nets
If we know the structure, there are two methods of learning: full observation (maximum likelihood) and partial observation (EM). 

## Maximum Likelihood for Bayes Nets
Given a simple Bayes net X->Y, what are the parameters?

* $$P(X)$$
* $$P(Y \mid X)$$

We can take derivatives, set them to zero, and find the maximum likelihood. In Bayesian nets, the likelihood term decomposes with respect to local conditional probability tables. If we know all of the values of each parameter, it's very easy to find the correct values. The complexity depends on the size of the parent sets, which is good for scaling as long as parent sets aren't massive. This is equivalent to finding weights in linear regression. 

In a Markov network, have a probability distribution and a structure. We want to take derivative with respect to the parameters. We have representation of each clique. The solution isn't closed form, but we can use derivatives for an interative approach. 

## Expectation Maximization
For parameter learning when the data is not fully observed. Realistically, this is what we're going to get. Suppose we have a set of variables X and the remaining set of variables are hidden Z. We treat Q=P(Z|X) and then we update the observations by treating Q as an observation. 

We'll start with K-means: K-means is iterative and we try to minimize distortion measure J. The K-Means algorithm can be viewed as an EM sequence! In the E step we assign each point to its closest center. In the M step we update the centers based on distance. 

Mixture of Gaussians allows soft clustering, where K means requires that each point is in one cluster. In the GMM, each point is generated by a Gaussian, but they overlap so we don't know for sure which point comes from where. E step is to compute the probability that point was generated by Gaussian K. The M step is that given the responsibilities of each Gaussian, we reevaluate the coefficients. Derivation of EM is available online, but suggest [Stanford 229 notes](http://cs229.stanford.edu/notes/cs229-notes8.pdf). 
