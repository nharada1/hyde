---
layout: post
title: 'Paper Review - The Gamma Database Project'
category: eecs584
---
This paper details the Gamma Database, a groundbreaking DBMS designed for shared nothing machines. Unlike the other attempts at shared nothing databases at the time of writing (which was the late 80s), the Gamma project was designed to scale linearly with the number of nodes added. This was a revolutionary achievement, as previous shared nothing machines were not entirely decentralized and still ran into single-node bottlenecks if the number of machines was too high. Gamma did this by using novel algorithms for horizontal partitioning and parallel hash algorithms.

The Gamma paper is lengthy and so in the interests of brevity I'll gloss over details. By the time this paper is released, the Gamma database project is actually on version 2.0. The new architecture uses a 32 processor hypercube from Intel with enough RAM to provide adequate buffering. The authors had to develop their own operating system to support their database, an impressive feat in itself. Gamma stores its data in horizontal partitions, which allows the system to exploit all the bandwidth of the disk subsystems. One thing I find interesting is that the authors were willing to admit their mistake in declustering all relations across all nodes. They advocate another system to decluster, based on "heat" of the relation (in COPE88). Generally authors do not like admitting that they are wrong in academic writings, and I think it's commendable these authors are willing to do so.

One strength of this paper is the authors proposal to use hash-based parallel algorithms to scale up query processing. By decentralizing data page processing throughout the system, Gamma avoids a potential bottleneck that its predecessor would frequently encounter. The paper outlines their multiprocessor join (and select) algorithms later in the document. The intuition behind the join operators attempts to combine relations into disjoint buckets via hashing. These buckets hold all tuples with the same value attribute. The authors outline sort-merge, Grace, Simple, and Hybrid algorithms for parallel joins. Each of these algorithms has already been published in a paper, so this proposal simple acted to aggregate the results into a real system. While on the surface this seems like a weakness of the paper, without an architecture these ideas would possibly have languished in obscurity. 

The paper also outlines the locking mechanism used by Gamma, which in my eyes is the weakest and most outdated part of the paper. The centralized deadlock detection algorithm proposed introduces a bottleneck into the system, and seems quite heavy duty to me. I think part of the reason for this is the time the paper was written in - seek and write times were much longer and conflicts were likely to occur. The log algorithm also appears slow on recovery, as all data must be forwarded to each recovering node. 

I do like that the system employs "chained declustering" to ensure availability in the event of a node failure. This thought is a predecessor of K-safety and is a requirement in a modern system. The authors haven't actually implemented the chained declustering at the time of publication, however, so any benefit in real systems is speculation (as compared to interleaved declustering). The paper does a thorough evaluation of the Gamma project, although there is no comparison to a commercial DMBS such as Teradata, possibly because the new database is so different the authors feel comparison would not make sense.
