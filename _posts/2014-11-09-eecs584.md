---
layout: post
title: 'Paper Review - CrowdDb: Answering Queries with Crowdsourcing'
category: eecs584
---
While strides in machine learning have vastly improved the ability of machines to emulate human cognition, there are still many tasks which humans excel or where building the appropriate system would be far too costly. With the connectivity of the internet, new models for using human intelligence have become popular, now called "crowdsourcing". Crowdsourcing refers to breaking a task into many smaller subproblems, each of which are easily solvable by a human but not a machine. For example, recognizing the object in a photo or the numbers on a house are both easy for humans and generally hard for machines. Amazon's Mechanical Turk has become a popular platform for researchers and companies who require human intelligence in otherwise automated systems. CrowdDB attempts to provide a database abstraction over mechanical turk to allow otherwise difficult tasks to be automated without a programmer to create the tasks in question.

CrowdDB introduces an extension to SQL (thankfully not named CrowdQL) which is designed to allow programmers to specify which tasks in the database should be allocated to humans. The paper introduces several tasks that are dispatchable to Amazon, specifically comparison, ordering, and field completion. I think that this is a smart and interesting way to provide interaction with the crowd, as the language separates the crowd sourcing from the database itself. In theory any other crowdsourcing method could be used, including one that is done mainly by machine. The paper also introduces a generic user interface building utility that can automatically generate user interfaces for the crowd based on schema from queries. The authors herald their UI builder as a major contribution, but I think that this is less significant than the other parts of the paper. Automatic UI development has been done before and the user interface itself provides nothing new. The interfaces built are simply form versions of the queries presented. 

This paper shows major weakness by not properly taking accuracy of the crowd into account. The crowd can be made up of people who deliberately want to undermine the task at hand, and the database takes crowd responses at face value. While the authors do address this as a weakness, I think that for any real production system this needs to be addressed, likely in a whole other paper. It was interesting to see the human variability on trust that must be accounted for. The workers can choose who to work for, and undermining their trust will result in poorer results. The need to respond to this automatically would be fantastic.

While I like the idea of a crowd sourced database, it seems to me that the idea of crowdsourcing has been shoehorned into SQL more than it probably needs to be. I can appreciate the requirement for an abstraction of dispatching tasks to humans, but it almost seems as if SQL is a bad choice of first class citizen for the task. The rigid schema of databases clashes with the vast flexibility of the crowd, and I feel it would be better to see a more general model of crowd dispatch that would then be accessed via SQL, in addition to any other languages that could allow more expression.
