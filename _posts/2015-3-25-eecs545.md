---
layout: post
title: Hidden Markov Models 
category: eecs545
---
## Sequential Data
In many cases you don't want to model things as IID, generally because they have dependencies over time (or sequence, whatever). 

## Markov Chain
A Markov chain is a series of random variables where future is independent of the past, given the present. Basically, we know the future state from today's state. This is still simplified, but isn't IID (where tomorrow is independent of today). We can also create higher order Markov models, where the dependencies are extended going further back.

## Hidden Markov Model
An HMM is a Markov chain that emits observations in every time step. We can't see the hidden state, but we can assume it's there and we can model it. In HMM, the latent variable is discrete. You can also imagine that the Z's are not discrete, which creates a linear dynamical system.

In many cases, we will need to add constraints to the transitions. 

## Maximum Likelihood for EM
In this case, gamma is the probability that a particular state is created by a given point. 

## Backwards-Forwards Algorithm
Message passing algorithm that sends alpha and beta backwards and forwards through the model. 
