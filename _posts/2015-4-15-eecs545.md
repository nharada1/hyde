---
layout: post
title: Sparse Sampling and POMDPs
category: eecs545
---
## What Should We Know About Optimal Control?
* How does value iteration work
* How does policy iteration work
* Why does value iteration converge
* Why does policy iteration converge
* Describe Q-learning and why/how it works
* Describe SARSA and the difference from Q-learning

## Sparse Sampling
Creates a generative model to allow us to look ahead in an MDP and choose an optimal action instead of running through the entire MDP. This is basically a greedy algorithm but instead of looking only at the best it looks ahead a certain number. We can do backwards induction to determine our best action. We can relate the search depth to gamma by looking farther ahead the closer our gamma is to one. 

## Partially Observable Markov Decision Process
Satinder currently giving a fucked up version of the Monty Hall Gameshow problem where you get mauled by a tiger if you lose. In this game there are two possible world states, True State=R or S=L door. We can either open a door or listen. Listening gives us a probability of the true state, but we still don't know for sure. This is a POMDP. 

In a POMDP, we add a sensing model to the MDP, so now some things are not as clear-cut. We can't see the state directly, but instead have a sensor that tells us with some probability what the actual state is. A POMDP is an HMM with actions! 
