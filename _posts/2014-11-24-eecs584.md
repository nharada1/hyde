---
layout: post
title: 'Paper Review - Efficient Query Evaluation on Probabilistic Databases' 
category: eecs584
---
Databases, especially for OLAP and business intelligence workloads, are well established but not very flexible for users. They rely on rigid schema and precise queries to retrieve results. On the other hand, information retrieval services such as Google are good at retrieving relevant information, but lack the structure of a database. For example, Google can correct spelling errors, allow for inaccurate queries, and can generally intuit what the user is asking for. This paper seeks to unify those opposing ideas by creating a probabilistic database that can rank the results in order of most relevant instead. This new paradigm would allow a user to query a database by asking less precisely what they want - for example a user could ask for "all good mystery novels from the 1990s" and receive a ranking of the results. By enabling these queries, an analyst could more easily analyze data from a large OLAP database, freed from the usual constraints of formulating just the perfect query to get the results he or she wants.

The database described in this paper uses the idea of "possible worlds semantics", which represents each possible database state along with its probabilities. The database assigns a probability to each tuple and uses these values to compute the probability of the tuples in the answer. Unfortunately, the probabilities computed in the naive way are wrong - the events are conditional on each other in unknown ways. Thus, the databases rewrites the query plan, searching until it finds one that is correct. This process is extremely powerful because it works on any SQL query with approximate predicates, and includes joins, nested sub-queries, aggregates, etc. The paper is quite mathematically rigorous, and provides a relational algebra for approximate operators, as well as formal proofs and theorems for much of the author's work. One strength within this work is the section of query optimization, which provides transformation rules for this probabilistic algebra within a database. As a foundation for future projects, this work would appear quite influential.

The paper also includes methods for dealing with the corner cases where a query has Sharp-P-complete complexity and cannot be solved in polynomial time. In these cases, the paper introduces probabilistic workarounds in the form of either error minimizing heuristic or Monte-Carlo simulation. The paper also (poorly) describes the prototype database that the authors implemented. The proposed database operates as a middleware to the RDBMS (in this case Microsoft SQL Server) and reformulates probabilistic queries into "extensional" SQL queries. This section seems to be a weakness of the paper, as both the experimental setup and results are fairly lacking compared to the rest of the journal article. Additionally, if we look at the results of runtime, the jump in time required to utilize this database is high - in some cases 1000 fold from safe plan to bare query. I would be interested to see a proper commercial implementation of this database to compare this paradigm to traditional RDBMS and information retrieval solutions. Obviously this is far outside the scope of the paper.

One interesting idea would be extending the probabilistic database to fit other database designs. For example, supporting an object-relational model such as Postgres. Doing so would be enable approximate queries on a larger variety of data, such as geospatial or coordinate information.
