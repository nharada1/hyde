---
layout: post
title: 'Paper Review - The PageRank Citation Ranking: Bringing Order to the Web' 
category: eecs584
---
In the early days of the web, search engines simply indexed pages by how much of your text matched the page. Results were inconsistent, and website designers could fool the systems by simply placing a bunch of hidden text somewhere on the page. In addition, results were never listed by what the user was truly looking for - searching a common word could bring results from any number of random webpages. The PageRank algorithm attempts to provide a ranking algorithm based on authority instead of frequency. The intuition is that important web pages are likely linked to by (a) lots of webpages and (b) other important webpages. This realization is the underlying of Google's first search algorithm, and spawned the multinational monopoly that exists today.

The PageRank algorithm uses the underlying structure of hyperlinks to determine page rank instead of page content. The more pages that link to your page, the higher your rank will be. Additionally, the authors chose to model the system as a random internet surfer, and introduced a damping factor (matrix E) to make the algorithm more resilient to infinite loops and cyclic graphs. While many details of the algorithm are excluded, the final form lends itself to matrix operations, and thus is computable in a reasonable amount of time. Even at the time of this papers release (1998), the algorithm could index millions of pages effectively. The authors provide some implementation details such as how to allocate memory for each page and how to remove dangling links to reduce memory consumption.

This paper is very strong in explaining its major points - the authors offer lots of simple explanations for how their system works, while also including some mathematical background. It's worth noting, however, that the final implementation of the algorithm ends up being full of warts - the non-conformity of the web (especially in an era before stronger compliance and open source parsers) means that the authors have to introduce ugly heuristics to create a working system. For example, the authors exclude everything from a cgi-bin folder, which is a sloppy way of excluding dynamic pages. One thing I really like about this paper is the discussion of the personalized E matrix, and how personalized search can be created very easily. I tend to think of personalized results as a recent development, but even during the genesis of Google the authors had created a model for it. From a historical perspective this paper is interesting and I think it's good to read as a history lesson in the field. At the time, it introduced a novel concept of page ranking, and has become so influential that most people who were unaware of early search are surprised that the systems in place were ever **not** authority based.

I would say this paper is fairly weak in its actual mathematical background, and reads like a paper written by engineers and not scientists (which isn't a bad thing). While it's quite readable, the paper provides few theoretical guarantees or proofs, simply giving us a few equations. Part of this is the simplicity of the algorithm, but I would have liked to see a discussion provide more mathematical background. Even the wikipedia article for PageRank handles the math at a higher level than this paper.
