---
layout: post
title: Discriminant Functions and Kernel Methods
category: eecs545
---
## Linear Discriminant Functions (again)
In discriminant functions we directly represent the decision boundary as opposed to learning the distribution. Why is the decision region convex and linear?

**Exam question:** Show that the decision region in a linear discriminant function is linear and convex.

## Perceptron
A generalized linear function of the form

$$y(x) = f(w^T \phi(x))$$

where f(a) is the sgn function. We claim that the perceptron will *always* converge if the data is linearly separable. This is unaffected by learning rate.

## Kernel Methods
Remember that linear regression is not too computationally expensive because it doesn't scale with data but with features. We can use the kernel trick to work in high dimensions and only use inner products. 

$$k(x,x') = \phi(x)^T\phi(x')$$

## Dual Representations
See slides for the math. We'll work through, substituting $$w = \phi^T a$$ in the objective function (for linear regression) and eventually we end up with:

$$y(x) = k(x)^T(K+\lambdaI_N)^{-1}t$$

Kernel method scales quadratically with input datapoints, so when to use this vs non-kernel methods. We can use kernel methods for any algorithm that requires a distance between points. However, we can't find the mean. Distances to the mean is totally fine though.

Can we do stochastic gradient descent with just kernels? Who the fuck knows, but try and figure it out
