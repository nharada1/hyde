---
layout: post
title: Bayesian Networks 
category: eecs545
---
## Network Parameterization
Explaining away: when you have two causes for a symptom and one cause is true, the belief that the other cause is true goes down. This is the heart of inference and is intuitive. 

D-separation: If all paths from A to B are blocked, A is d-separated from B by C. If A is d-separated from B to C, the joint distribution over all variables in the graph are A dep B given C.

Markov Blanket: A set S is a Markov blanket if if X is conditionally independent of all other vars given S. We must include all children, parents, and parents of children. We need to include parents of children because otherwise we could feed through head to head knowledge by changing the co-parent.

Noisy-OR Parameterization: In context of diseases: for every disease you have the prob of you not having a symptom goes down by a multiplicative amount. We claim this is a compact version of the conditional prob tables.

## Hidden Markov Models
**Exam Question**: Given a diagram write down the joint probability (slide 55).

## Markov Random Fields
A clique is a set of nodes connected to its neighbors. A maximal clique means the nodes are fully connected. If our intuition is about causality, then we want a directed graph. If there isn't a causal intuition, then we can go for an undirected graph. To convert directed to undirected graphs we need to 'moralize' the graph by marrying the parents.
