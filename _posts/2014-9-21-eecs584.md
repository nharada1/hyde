---
layout: post
title: Paper Review - On Optimistic Methods of Concurrency Control 
category: eecs584
---
Traditional database systems require stringent concurrency controls the ensure that there is no data loss or corruption when multiple agents are accessing the system. In most modern DBMSes, this is managed through the use of locks, which provide exclusive access to some subset of the database. While this approach manages concurrency while preventing corruption, it is inherently a heavy overhead system, and locks introduce complexity at both runtime and during system design. 

This paper proposes a new approach to concurrency, which it refers to as optimistic methods of concurrency control. The new method is motivated by the observation that in most cases, locks aren't actually required to prevent corruption. Only in a small number of cases will locks actually be utilized, and the remainder of the time they simply create overhead and performance issues. Thus, the new system does away with locking entirely, writing and reading as if the database is being accessed only by a single system at a time and then using a strong validation algorithm to discard and retry operations that would result in an inconsistent state. The validation phase of the new method checks to see if the operations performed are serializable, and if they are not it assumes corruption occurred and aborts the transaction. It accomplishes this validation through the use of transaction numbers, which are assigned at the end of the read phase to maximize system throughput.

The authors make other considerations, such as what to do when a validation repeatedly fails. This represents the worst case scenario for the performance of the system, and after a set number of tries the transaction is given exclusive access to the entire database, forcing it to complete. The paper examines the "critical section" of transactions, and examines cases of parallel operations (such as multiple CPUs and parallel validation). Finally, the authors do a rigorous analysis of how their method works on B-trees, including calculating the probability that a calculation requires a restart. I really like that the authors include this calculation, although the authors find a good case for their calculations. The optimistic approach seems like a good idea when domain knowledge tells you that you will likely not have many conflicts, but clearly performance will degrade significantly if this assumption isn't true. As the authors assert, a technique combining locking and optimism would be ideal to support cases not on either extreme of the spectrum (which most real world cases would be). It would also be interesting to examine lock-free methods to supporting the other ANSI SQL isolation levels besides simply serializability, such as READ COMMITTED or UNCOMMITTED. It's unclear whether or not the authors implemented the idea they proposed, but if they did I would have liked to see real world comparisons of access performance on both real and synthetic data.
