---
layout: post
title: 'Paper Review - Why Your Data Wont Mix: Semantic Heterogeneity' 
category: eecs584
---
Data is ubiquitous in any company or organization, but in many cases incorporating data from different sources presents a problem. Even within one organization, different data sources are designed using different schemas. Trying to reconcile all those slightly different schemas is currently an arduous process that must be done by hand. A human user must use their own domain knowledge to match schemas and determine which fields match and how to deal with missing or incomplete fields. This paper, which is more of a survey paper than anything else, offers a look at the problems that data mixing presents, and explains why the problem is so hard. The author also examines potential approaches that could work in the future, as well as problems he forsees moving forward.

Schema matching is a hard problem, but an automated solution offers many benefits. The most general and largest benefit is that as data becomes more fluid, the amount of analysis and calculation that is possible in a fixed amount of development time grows. The author presents specific instances where data matching becomes useful, for example indexing the deep web. The deep web refers to any dynamically generated content behind search forms, and a search engine that could access and index these databases would provide magnitudes more information in some queries. To do this, the search engine would need a way to interpret the many different forms available from various websites, and this is not currently possible. The author argues that the domain knowledge required combined with technical expertise makes it hard to find people to match by hand. He then examines several state of the art data matching systems, and points out that each system has flaws and a single method is fragile and unlikely to succeed. While some systems attempt to combine solutions, the paper argues for machine learning based techniques that learn from a training set how to match schema. Such a system, the author argues, would be much more stable and likely to offer an automated solution that can handle real world data. 

One strength of this paper is the looking forward section where the author gives his outlook of the future and the problems facing semantic heterogeneity. He correctly predicts that the size of data in the near future will make many of the techniques in the state of the art intractable. He also points out that managing a *dataspace* as opposed to a database is going to be a problem. A dataspace is comprised of participants and relationships, and as technology becomes more and more ubiquitous in our lives we can expect this problem of dataspaces to grow just as the paper predicts. One can imagine that the data for a human of the future would contain boundless dimensions, ranging from health data to smart devices to internet connected appliances. 

The strength of this paper is also a weakness in some respects - the author doesn't really offer any ideas on how to face the problems he presents. The paper claims that the future roadblocks are just seeing research, but clearly did not expect the amount of heterogeneous data that would be available in only a few years. I also think the discussion about machine learning is lacking - it reads as a cursory glance but we're never enlightened with how exactly machine learning could be applied to the problem of schema matching. I assume that a thorough treatment is available in other resources.
