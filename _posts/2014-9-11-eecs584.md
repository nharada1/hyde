---
layout: post
title: Paper Review - RAID, High-Performance, Reliable Secondary Storage 
category: eecs584
---
Mechanical disk drives are by far the most common point of failure in modern computer systems. As a result, disk redundancy is of upmost importance in any database system. Even if the individual disks have an acceptable median time to failure (MTTF), the large data storage requirements of any current database requires additional drives, and thus decreases the MTTF by magnitudes of time. RAID, or Redundant Array of Inexpensive Disks, provides standard configurations to aggregate multiple disks into one logical volume. This paper provides an overview of the various RAID configuration options (referred to as *levels*), as well as performance and implementation considerations (although some of this was outside the scope of this reading).

The RAID standard specifies techniques used to partition data and store/restore data, but does not specify exact implementations. These configuration options are split into seven *levels*.

* Level 0 – This level simply uses multiple disks as one large volume, with no redundancy across disks.
* Level 1 – A basic data redundancy, level 1 creates two copies of data, one on each disk.
* Level 2 – Level 2 uses error correcting codes as used in main memory hardware. This reduces the disks required to store redundancy, but multiple recovery disks are required to identify failures.
* Level 3 – Uses bit interleaved parity and stores parity information on extra disks. This allows the volume to use only one parity disk, but for writes the volume must write to that additional disk and thus the volume can only service a single write request at once.
* Level 4 – Block interleaved parity is similar to level 3 but parity is written in blocks called striping units. Again the parity disk easily becomes a bottleneck.
* Level 5 – Block interleaved distributed parity is similar to level 4 but instead interleaves parity blocks throughout the regular data. This removes the parity disk as a bottleneck and allows fast small read, large read and write access. The distributed parity is a weakness, however, for small writes that require syncs across multiple volumes.
* Level 6 – The final level uses Reed-Soloman codes to allow for additional disk failure over level 5. This level offers improved redundancy at the expense of space, and compounds the small write problem of level 5 by using more volumes.

The paper further visits reliability of the different RAID levels, as well as offering a look at differing failure modes and adjusting calculations for more reasonable failure conditions. Besides double disk failure, the paper examines system crashes and uncorrectable bit error for both level 5 and level 6 disk arrays. As expected, level 6 provides strong protection against failure, with even the catastrophic system crash having a 10 year probability of less than 8%. The level 5 RAID is reasonably well protected against double disk failure and system crashes, but has an almost 25% chance to fail due to bit error within 10 years. The final section of our reading concerns physical layout concerns and practical issues in the RAID system.

I felt this paper did a good job of addressing the RAID specification in a technical but accessible manner. I especially liked that it offered more practical considerations, such as the orthogonal RAID section, while still offering theoretical conclusions. While the paper did briefly discuss and hint that only RAID level 5 and 6 were widely used in large systems, I felt it failed to offer much discussion about why this was the case, or offer counter examples of when lower levels would be used in real-world scenarios. It also would have been interesting to see tangible examples of when varying RAID levels would be used in industrial applications. Last, I would have been interested in a discussion about implementation, especially operating system support. This paper is old enough, however, that such standards may not have evolved yet.
