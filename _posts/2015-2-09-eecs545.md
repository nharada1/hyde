---
layout: post
title: SVM and Algebra
category: eecs545
---
## SVM Reviews
It's an optimization problem, we want to solve a dual optimization with a Lagrangian function. We introduce slack variables to allow misclassifications. The dual formulation just introduces more constraints. The higher the C value the stricter the classifier is. A value of C=inf will result in the hard boundary classifier.

## Multivariate Gaussians
Covariance matrix $$\Sigma$$ is the outer product of $$[x-E[x]]$$

Partitioned Gaussians: We can combine Gaussians via partitions xa and xb, which allows us to partition the covariance matrix into only four covariances! Also notational is that the inverse of the covariance is the precision matrix $$\Lambda$$. The partitioned Guassian can use the Woodbury Matrix Inversion Lemma to invert which is easier than computing it by hand. 

In general we can treat matricies that are partitioned as matricies themselves. That is, linear algebra we do with numbers can extend to matricies. Fact: given a linear combo of Gaussian random vars, its distribution is also Gaussian. Its marginal distribution is also Gaussian. Its conditional distribution is also Gaussian.

## Bayes' Theorem for Gaussians
We define our probabilities in terms of Guassians. Then we can find the posterior Gaussians just like with normal numbers but in symbolic form.

# Bayesian Linear Regression
Doing regression from a truly Bayesian perspective. We have a likelihood and a prior, which gives us a posterior. We also now have probabilities for the weights, which we might assume has zero mean and identity covariance. We get the variance of the weights fo' free, which allows us to do regularization using weight variance instead of L2 norm. 
