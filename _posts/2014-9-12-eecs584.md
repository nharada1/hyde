---
layout: post
title: Paper Review - The Google File System 
category: eecs584
---
This paper details the Google File System, a distributed, fault tolerant file system in use by Google to handle their massive amounts of data on commodity hardware. The GFS uses a master-node configuration, running many small “chunkservers” on commodity Linux machines and a master server to coordinate operations between those nodes. The file system runs in user space (ie is not a kernel process) and *expects* some nodes to be in a failure state at some time. This expectation allows for scalability and a highly fault tolerant system. 

Clients communicate first with the master server, which holds in memory a hash of each file path to the corresponding chunk location (or locations as the data is duplicated on multiple machines at all times). The master server returns the server(s) location(s) and the client does the majority of its communication with the chunkserver directly. This maximizes system throughput, as each node uses its own Ethernet connection and there is no bottleneck at the master server for many simultaneous clients. This high throughput generally comes at the expense of latency. The chunkservers also supports fast, simultaneous writes to a single file via “record appends”, which guarantee only that the data will be written, and not in what order.

The master servers themselves (plural because even though one master runs at a time there are duplicates to prevent data loss) manage the network and chunk layout. The master holds metadata about each chunk, and requires each chunkserver to periodically check in with a “heartbeat” message. The master allows “snapshots” to be made by creating a second pointer to a chunk and requiring the chunkserver to duplicate that chunk at next access. Additionally, the master manages garbage collection by simply marking items as “deleted” and occasionally running a garbage collector to lazily delete the items. 

This paper is an interesting read because this is the paper written for a commercial system that was designed with specific industry goals in mind. As a consequence of being designed by Google, the GFS has very specific performance goals in mind. Google knows that most of their data will be written or read as a stream by many producers and consumers, and thus made tradeoffs to support this. The filesystem is not optimized for many small writes, although it would have been interesting if the paper examined these possibilities in their conclusions. Additionally, Google requires high throughput and dependency at the expense of hardware. Thus, the high level design emphasizes that hardware is cheap and downtime is expensive. For a home or small business user, this design is totally impractical because it requires so many machines (even the small test clusters use 16 nodes and 3 masters). 

The paper briefly mentions diagnostic tools that the GFS uses, such as RPC logs and other diagnostic logs, but does not disclose them or provide details. It would have been nice if the paper went into further detail, as diagnostics are important in production ready systems. I suspect that these logs are partially industry secrets, and Google feels no need to disclose their proprietary diagnostic systems in the paper. This weakness is a general weakness of all industry papers – the authors cannot and will not tell us certain things about their system because doing so would compromise the company’s proprietary data. Even in this paper, we are given very little information about the benchmark system’s actual function, as the production server and especially research server details are likely shrouded in secrecy by the company. 

