---
layout: post
title: Agents and Rationality 
category: eecs492
---

## Agents
An agent is anything that acts on its environment and perceives its environment. Sensing is required, so a windup toy isn't an agent. Agents also must pursue a set of pre-specified goals.

**Percepts** - The perceptual inputs at any time

**Percept sequence** - The history of everything an agent has perceived

Agents actions can only depend on things it perceives. Note that this can include non-states such as a *lack* of a wall.

**Agent function** - Maps percepts to actions, what we'll focus on

**Agent program** - Implementation

We must somehow evaluate an Agent's behavior. The answer generally depends on the task environment. Since rationality depends on the agent's capabilities, we can say a rational agent tries to maximize a performance measure with respect to its capabilities and prior knowledge.

Learning is considered a part of rationality - you must learn to be rational.

Exam note: Proper definition of rationality will be on the exam.

Rationality - For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

## Task Environment
Task environments have seven different properties.

#### Fully vs Partially Observable
Can an agent see everything in the environment? If fully observable, there's no internal state needed because we can see everything.

#### Singe Agent vs Multiagent
Multiagent environments mean that an there is another agent that impacts you.

#### Deterministic vs Stochastic
Deterministic means we can predict perfectly the state of the world. In practice almost every environment is stochastic.

#### Episodic vs Sequential
Episodic results in independent events - previous action doesn't impact the now.

#### Static vs Dynamic
Dynamic means that the environment changes *while* the agent is thinking. 

#### Discrete vs Continuous
Refers to the way time is handled. Straightforward, same idea as DSP or control theory. Interesting idea, chess is discrete, but chess with a clock is continuous.

#### Known vs Unknown
A known environment means that the agent knows the rules of the environment going in.

## Agent Structure
An agent consists of its architecture and program. 

A few model types of agents:

**Stateless** - Effects of time aren't modeled

**Fixed Model** - Designer provides model, contrast to **learning model** where the agent can adapt.

A few planning types of agents:

**Reflexive** - Actions based on preprogrammed conditions

**Predictive** - Actions based on potential effects

#### Table-Driven Agent
Agents choose actions based on states, explicit structure. This is based on percept sequences, which grows large quickly. Table driven *must* look at all states, including all past states.

#### Simple-Reflex Agent
Similar to table driven, but based on what we see *right now*. This is a quite simple structure, and in partially observable worlds we can't make correct decisions.
