---
layout: post
title: Partially Observable and Q-Learning 
category: eecs545
---
## Optimal Control
Remember that in the planning setting we did value iteration, where we knew everything except the optimal values. This includes the reward function as well as the transition probabilities. Value iteration just iterates, updating the policy as we reevaluate the value function. Can we prove that this algorithm converges? Yes

Similar to previous proof, somehow has to do with contraction. See picture or result in book.

## Policy Iteration
Start with an arbitrary policy. Do policy evaluation to get the value function for this policy. Then update the policy using the greedy update procedure and repeat. Stop when the value function stops changing. Why is this guaranteed to improve?

## Q-Learning
Let's say that Q\* is the best you can do in terms of rewards at state S taking action A. We can do Q-value iteration, which is the Q value version of value iteration. Again, we assume we know the reward function and transition probabilities. To update we do something similar to temporal difference. We mix the old value plus a new guess to get the next iteration. 

**Exam Question**: stochastic approximation conditions

In Q-Learning, the non-linear max operator is inside the expectation, so we can get a non-biased estimate. How do we initialize? Optimistic initialization. If we do this then we explore early. 

## SARSA
Very similar to Q learning. Q learning considers actions you could take in the next state, while SARSA considers actions you actually take. Basically just no max. This algorithm converges to the expected value of the discounted rewards you'd get if you started in state s and took action a then followed policy pi thereafter. 

How can we pick actions to get Q\*? We could pick actions greedily, but this would get us stuck. Behave with an epsilon-greedy policy, shrinking epsilon over time. SARSA is *on-policy* while Q-learning is *off-policy*, meaning that SARSA converges to the value of the policy you are behaving on.
