---
layout: post
title: Linear Regression and Classification 
category: eecs545
---
## Probability Review
Not many notes here. Review axioms. Review additional properties such as unions, law of total prob, Bayes' rule. Likelihood functions, applications of Bayes' rule:

Posterior $$\propto$$ likelihood x prior

Remember that it's easier to find the likelihood than it is to find the posterior. 

## Maximum Likelihood
Choose a parameter setting w that maximizes the likelihood function p(D|w). We'll use log-likelihood so we can minimize the log (which has nice properties that let us sum instead of multiply.) There is also the Maximum a posteriori estimation, where we maximize p(w|D) instead of p(D|w). In this case the prior is important. These two different estimates will be the same when the prior is uniform.

## Maximum Likelihood interpretation of Linear Least Squares
Assume a stochastic model:

$$ t = y(x, w) + \epsilon $$ where $$ \epsilon \sim \mathcal{N}(0, \beta^{-1}) $$

this gives likelihood function:

$$ p(t \shortmid x,w,\beta) = \mathcal{N}(t \shortmid y(x,w),\beta^{-1}) $$

Minimizing the log likelihood will be exactly minimizing the sum of squares error function.

**Exam question:** Given a different stochastic model, derive the maximum likelihood solution.

**Exam question:** L2 regularized least squares is the MAP estimate of w with a prior of $$ p(w) \propto e^{\frac{\lambda}{2} \parallel w \parallel^2} $$

## Locally Weighted Linear Regression
Weighted regression where the weights are dependent on x (query point), and you solve linear regression for each query point x. The wider the Gaussian, the less complex the model. Note that you'll need to refit the regression for every single query that comes in.

