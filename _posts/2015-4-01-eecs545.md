---
layout: post
title: Reinforcement Learning 
category: eecs545
---
## Bandits
Recall the bandit problem. We don't know what the distribution of each bandit is, but we want to maximize our reward. This will transition into MDPs. First let's talk about notation:

* Action values (Q) is the average reward for a specific action.

Epsilon greedy algorithm for playing bandits:

We select a greedy arm most of the time, and a random arm some other percentage of the time. We can also start with a high epsilon and decrease it. With a very small epsilon we'll find almost the optimal. How fast can we shrink epsilon though? We need to explore but if we shrink too slow we'll perform worse. We'll call this change in the parameter the **schedule**. 

For episilon-decreasing, we can bound our sum on epsilon by a few things. One, we want the sum to be infinity. Two, we want the sum of epsilon to be finite. A harmonic series satisfies this property. In practice we have bigger steps and we reduce the learning rate, because in practice 1/t is too slow. 

Softmax Procedure:

Another version is to set the probability of choosing a lever as the softmax of the lever. Temperature is a parameter of the softmax, and with a higher temperature you'll have a more even probability. 

Bandits are a pure exploration-exploitation problem. They have no generalization problems.

## Final and logistics

No project. Two more homeworks. LOL. Remaining 20 percent will go to the better of the exams. 

## Contextual-bandits
We can choose levers based on context, and then we can generalize the actions based on new contexts. 

## Markov Decision Process
Notation

* Ot is the observation at time t
* Rt is the reward at time t
* At is the action at time t

The agent wants to maximize a reward given observations by taking actions. There is some discount factor because doing something now is better than later. 

How does a world satisfy a Markov property? If we have the last observation and action, then in a Markov world this is all we need to characterize the world. If observations are perfect, then we call the observation the state.

An MDP is defined by:

* A set of states
* A set of actions
* Transition probabilities
* A reward function
* A discount factor



