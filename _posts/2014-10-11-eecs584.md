---
layout: post
title: 'Paper Review - Big Table: A Distributed Storage System for Structured Data'
category: eecs584
---
With the rise of monopoly internet companies, a few behemoths generate data on scales unexpected by researchers. BigTable, the brainchild of the Google research team, is an example of a technology developed purely for application. With a large research budget and petabyte scale data, Google designed BigTable to work across hundreds to thousands of commodity machines. BigTable is a non-relational database that uses a three dimensional indexing system to store data - two dimensions are string keys, and the third is a time based index. This structure allows BigTable to naturally fit the data usage patterns of many products in the Google domain, where data changes across time on a massive scale (ie Street View, Google Earth, Web Indexing, etc). 

The largest contribution of this paper is the data model, which introduces time indexing as a first class citizen of the database. It's important for the reader to note that BigTable is *not* a database in the conventional sense, and is instead a sparse, multidimensional, sorted map. In most cases, the client is designed in such a way that commonly accessed information is stored nearby. For example, webpages can be stored as com.domain.subdomain, and by doing so will keep subdomains nearby on disk. BigTable is built upon several other Google services, specifically the Google File System, the Chubby distributed lock system, and the Google SSTable file format. The system consists of thousands of "tablets", hundreds of which are stored on each tablet server. Like the Google File System, in most cases the client will communicate directly with these tablet servers, talking to the master server only when it needs an update on cached tablet server information. Tablets distribute disk data across machines, and are stored via the underlying SSTable file format. Google also introduces performance enhancements, such as bloom filters for tablet membership and compression inside of SSTables using a novel compression algorithm designed to trade CPU time for compression efficiency. 

BigTable shares many similarities with C-Store, and the authors mention this directly. One important inspiration from C-store is the use of a hybrid read/write system to write data to disk. BigTable uses a "memtable" to write data to directly, and then transfers that memory based table to disk over time. One major strength of BigTable is its robustness. Servers of any type can fail and the system will recover quickly. By using many small tablets, the remaining machines can easily take a small part of one machine's work. Additionally, the commit-log implementation is quite smart - the system uses one massive commit log per tablet server, but on recovery sorts the log to allow sequential read by each machine. 

The commit-log is likely a major weakness of the system as well - the tablet server writes its own commit log, and it seems as though if a system crashed and was unrecoverable that its data would be unrecoverable as well. The GFS provides K-safety for the data itself, so this situation will not completely lose data. Additionally, the method of using multiple threads to write the log seems like it wouldn't fix write issues in many cases. If one thread is writing slowly, there is likely an underlying cause and switching to another thread for writes will only delay the onset of the larger problem. One final weakness of this system is that it relies on the multidimensional structure it provides, as well as the client to ensure that data access is efficient. For example, if the user had decided to store subdomain.domain.com instead of the other way around, access would be slow for nearby subdomains. Thus, this system is only truly useful for someone with technical knowledge, and would not be as useful for non-technical business analysts. 
