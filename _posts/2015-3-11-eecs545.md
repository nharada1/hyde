---
layout: post
title: Expectation Maximization 
category: eecs545
---
## Mixture model EM
Given observation X and latent variable Z, we can say:

$$ \log P_{\Theta}(X) = \log \sum_Z P_{\Theta}(x,z) $$

There is a full derivation on CTools. We'll use (Carly Rae) Jensen's inequality and introduce a Q(z) that is the expected value of the log likelihood function, with respect to the conditional distribution of Z given X, under the **current parameter estimates**. 

The final algorithm has two steps:

* Expectation: Find Q, or the expected value with the current parameters
* Maximization: Fix z, and find the parameter that maximizes Q

We are maximizing the lower bound! Claim: we have guarenteed to have improved the log likelihood. 

**Exam Question**: Given a simple variation of GMM, give the E and M steps of the model

## Principal Component Analysis
PCA maximizes the variance of the projection of data into a lower dimensional space. Basically we choose the direction of projection so that the resulting projections have the highest variance. We can find principal components one at a time. We want to find the vector u1 that maximizes the projection variance, using lagrange multipliers. This ends up being the same as finding the eigenvector with the largest eigenvalue of the covariance matricies. BOOM THIS IS MATH.


