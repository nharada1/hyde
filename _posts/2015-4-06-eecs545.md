---
layout: post
title: Markov Decision Processes 
category: eecs545
---
## Markov Process
At each time step we get a reward from our action. Consider the problem in the book found [here](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node33.html). Transitions are random, so even without noise in the reward there is an expected value. The optimal policy is simultaneously optimal for every state, although the optimal policy may not be unique. As a Markov problem, the probabilities don't change over time, so the value function *is a function of just state*. 

## Problems
We start with a given policy and want to compute how good it is. Optimal control problem is find the optimal policy. Today will cover policy evaluation.

## Policy Evaluation
Implementation note: to construct a policy matrix, take a row from each matrix of actions corresponding to the policy we are given. Given a pi, how do we find a Vpi? 

First option: Monte Carlo approach. Just sample a ton of times. We'll stop early because eventually the discount factor creates a vanishing function.
Second option: Matrix inversion method. Requires a model. Uses the value equation and solves.
Third option: Iterative approach. Same as previous, but we solve recursively.

Why Monte Carlo?
