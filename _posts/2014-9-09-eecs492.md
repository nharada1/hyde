---
layout: post
title: Problem Solving and Search 
category: eecs492
---

## Agents
Simple reflex agents based only on world at this moment.

### Types of Agents

#### Model based reflex agent
Now we want to use a model to estimate state from the percepts. We don't need to see the whole world state. Agents in this model will update their state according to a model (make a best guess at true world state) and then match a rule to execute an action.

#### Goal based agent
The agent wants to reach a goal, so it looks for desirable states. Now we need to consider what will happen if we take an action and if we want the result.

#### Utility based agent
Uses a performance measure to differentiate world states. This uses a utility function, which is an internalization of the agent's performance. An agent like this can deal with conflicting goals.

#### Learning agent
An agent that learns over time and becomes better as it explores an unknown environment. This model contains a critic to pass information back to the learning element.

### Evaluating Agent Designs
We need to somehow find optimality with respect to PEAS characterization. This can be either via trials or proof.

## Search
We want to represent a path to our goals. In many cases we can represent all possible states as a space, eg the search space. For example, we can represent the search state as a physical map. Right now we're finding our path through the world, and assuming the world won't change. How do we do search?

* Characterize the World
* Formulate a Goal
* Formulate the Problem

**Search**: The process of looking for a sequence of actions that records a goal.

Even for seemingly non-graph problems we can define a state space graph for each state. 
