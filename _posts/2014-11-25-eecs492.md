---
layout: post
title: Complex Decisions and Learning 
category: eecs492
---
## Partially Observable Markov Decision Processes
Aspects of a POMPDP:

* Transition model
* Actions
* Reward function
* Sensor model (new to POMDP)

In a POMDP, we don't know the state of the work or the agent. In this case, we can make the gridworld a POMDP simply by removing the knowledge of our location. In a POMDP, a **belief state** is a probability distribution over possible states. We want to determine the optimal action by mapping belief states to actions. Notice that this doesn't rely on the actual state.

How can we update our belief states? The probability of perceiving e, given action a is performed in belief state b:

$$ P(e \mid a,b) = \sum\limits_{s'} P(e \mid a,s',b)P(s' \mid a,b) $$

## Decisions with Multiple Agents
Draws from game theory in many cases. For example: prisoner's dilemma. 

* Dominant strategy: We always testify because it minimizes our losses deterministically, but this assumes that every outcome is equal.
    * Strongly or weakly dominates
    * Pareto optimal, pareto dominated

Equalibriums exist in games, in this case if both testify that's an equalibrium. John Nash proved that all games have at least *one* equalibrium.


