---
layout: post
title: 'Paper Review - MapReduce: Simplified Data Processing on Large Clusters' 
category: eecs584
---
Distributed data processing has gained heavy traction as a result of internet scale data warehouses. In 2004, Google introduced MapReduce, a programming model and associated implementation for processing and generating large data sets. MapReduce has been extremely influential in the data processing space, and has become a de facto standard for distributed data analysis, running on Hadoop or similar system. MapReduce earns its name from its programming model, which draws from the functional programming paradigm of mapping a function to data and reducing that data to an aggregate value. Because these operations are atomic, and in most cases referentially transparent, the analysis can easily be parallelized and made fault tolerant.

This paper outlines the details of Google's MapReduce system and provides examples of when the application is appropriate. MapReduce requires a user to write two functions, a mapping function that applies a function to data and emits key value pairs, and a reducing function that takes key value pairs and reduces them to one output. The intermediate result is stored on the machine, allowing machines to be either a mapper or reducer. This algorithm also allows for machine failure - in the case that a node fails to respond to the master, the node's jobs are simply placed on other machines. 

This paper's strength lies in the simplicity of the system. I suspect that many people who use MapReduce on a daily basis have never read the paper, and likely do not have significant background with parallel computation. By applying a simple API to a complex problem, Google has allowed data computation on a large scale. Thus, I claim the primary contribution is the idea of using the map-reduce paradigm itself. There are other strengths in this paper, including the idea of backup tasks to reduce stragglers and the methods of splitting data to be used by the workers. I also liked that the paper includes an example program in appendix A, as this highlights the practicality and simplicity of the system. 

To achieve such power, the designers of MapReduce had to make several significant tradeoffs. The largest weakness of this system is that the programmer must be able to express their idea in the form of a map-reduce query. This is difficult for tasks that cannot run in isolation (for example training an SVM). Additionally, because MapReduce is a large system, real time processing is impossible. The system is best used for batch processes, and has trouble handling streaming data. One problem that has been addressed since the introduction of MapReduce is that the user must program the system themselves. This is analogous to IMS or CODASYL where the user had to define functions to query the database. Systems to translate SQL or other domain specific languages into MapReduce jobs are now quite popular, for example Hive or Pig. Finally, while MapReduce allows large data sets to be easily processed by storing intermediate results on disk, this is a weakness because each part of the job requires additional IO writes. By placing the results in memory significant speedups could be achieved. This has been explored in Apache's Spark and in-memory Hadoop.
