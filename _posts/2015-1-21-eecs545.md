---
layout: post
title: Classification 
category: eecs545
---
## Classification Problem
Given an input vector, assign it to one of K distinct classes. We can define 0-1 loss as the number of incorrect labels.

Approaches:

* Nearest neighbor classifiers
* Discriminant functions (SVM, Perceptron, etc)
* Learn distributions
    * Discriminative models (Logistic Regression)
    * Generative models (HMM, Naive Bayes)

**Exam Question:** Why is the posterior the quantity of interest?

## Logistic Regression
Model class posterior using a sigmoid applied to a linear function of the feature vector:

$$ p(C_1 \mid \phi) = y(\phi) = \sigma(\mathbf{x}^T \phi(x)) $$

Sigmoid is a smooth squashing function, logit function is the inverse of the sigmoid. This classifier generalizes to the *softmax* function, which allows many classes and not just two. See slides for usual tricks on finding $$ P(t \mid w) $$ and on gradient descent derivation.

**Exam Question:** Why is placing the decision boundary at 0.5 for the logistic classifier correct?

## Generative Models
Generative models learn the joint probabilities while discriminative models learn the conditional probabilities. In logistic regression, we make the assumption that the log odds are a linear function of x. 

## Gaussian (Linear) Discriminant Analysis
Assume that the class conditional density is Gaussian. We make an assumption of what the Gaussians looks like, and then we estimate the parameters from the data. When we get a new data point, we sample from the distribution to determine the probability of each class. Fact: if we model the probabilities as Gaussians with the **same covariance** matrix, then the optimal decision boundry is linear. 

Note to self: Look at derivation and do math yourself.

Logistic regression is more flexible about data distribution, but GDA works well when the distribution follows the Gaussian assumption. In contrast, logistic regression requries costly iterative training procedures, but requires less parameters.
