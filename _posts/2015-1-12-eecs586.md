---
layout: post
title:  
category: eecs586
---
## Back to Sequential Search
Remembering our example from last time, we talked about how making assumptions on sequential search was the only way to find various running times, especially average. What if all items don't have the same probability? Let's suppose there's some probability distribution on key requests, and requests are independent. How do we choose items to look at?

* Sort keys by probability, decreasing order. Prove this is correct.

Proof (extremely questionable):

*Let O be the optimal ordering, O' be ordering by probability order.*

*Suppose there is a case where one key has a higher probability and is farther in the search. If we switch these, we've decreased expected search time. Therefore, we never put the ordering out of sorted order in an optimal ordering.* 

It is most likely, however, the the distribution will be uniform. Additionally, in many scenarios the item is not likely to be requested. Let's look at an example:

Zipf's law makes statements about the probability of words in English, based on empirical research:

* *the*: 6.18%
* *I*: 1.17%
* *you*: 0.83%

Pareto asked a similar question, but about distribution of income. They both found that the ith most likely word occurs $$\frac{1}{i}$$ times as likely as the most likely word. Let's look at randomly choosing A, B, or space on a keyboard. We can look at expected length of the word:

* len(0) = .33
* len(1) = .66 * .33
* len(i) = (.66)^i * .33

This is **not** what Zipf looked at though. The sequence is not geometric because we must account for various transpositions of letters. See the CTools paper for full paper. The important observation of this is that **in computer science, heavy tailed distributions are much more likely than in other disciplines.** This occurs because in CS many things are built up over time.

The number of expected items examined will generally be calculable, but in cases where we don't know the symbolic error we'll need to learn the distribution. We'll look at different caching schemes. Easy solution: back to front, just move cache around when new requests come in. Slightly better: count heuristic, keep track of counts. 


