---
layout: post
title: Graphical Models 
category: eecs545
---
## Joint Probability
Marginalization is the process of computing total probabilities by summing over the joint probabilities for a given variable. How do we learn and represent these joint probability tables. This is basically graphical models of 492, and will probably be about as fun.

We'll use the chain rule (many times) for most computations in this section. Remember that:

$$ P(a,b,c) = p(c \mid a,b)p(b \mid a)p(a) $$

and et cetera. The directed acyclic graph that this model forms is based on conditional probabilities. A child is conditioned on its parents. A fully connected graph, in an *acyclic graph*, connects all parents to all their children. If every node is connected to all the others it is no longer acyclic. Ordering in these graphs matters hugely. For notation, we show repeating cycles by boxes in graphical models. Shaded circles are observed. 

There are various parameterizations for Bayesian graphs. Overview of parameterizations, we've seen all of these in AI. Check the notes for a refresher.
