---
layout: post
title: Data Analysis I 
category: eecs598
---
## Classification
Predicting outcomes

### Nearest Neighbors
Simple model, look at what happened to similar data. We can use a few metrics to determine "distance", but in the end it's all the same idea. Note we should normalize the data because otherwise large distances screw with us.

### Decision Trees
Another basic model, attempts to split data based on features. We use entropy to quantify the amount of disorder in a set, aka the amount of information required to represent a node. The feature space of the decision tree is partitions aligned with the axis, which is hard for diagonals or etc.

### Bayesian Learning
We want to represent things probabilistically and choose our hypothesis with the max probability. We'll use Baye's rule to determine probabilities of each possible classification. Because we decompose into separate P's, we make a big assumption about independence, but this may not be true. On the other hand, we actually get a probability out.

### SVM (Support Vector Machine)
We want to separate the data in the *best way* possible, aka provide the maximum separating hyperplane. We find the "support vectors" that support the hyperplane, we only need these points to classify. We can add in a kernel function to extrapolate to higher dimensions. The big SVM advantage is that you avoid overfitting my maximizing the hyperplane.

## Regression
Predicting values

### Logistic Regression
Good for when there's a range over which things matter. At the extremes things don't matter as much. This is good for many medical applications. We'll solve this via maximum likelihood estimation. We want to find \beta values for our probability function.

