---
layout: post
title:  
category: eecs545
---

## Updated Grading Policy
Now midterm and final, instead there's a Kaggle based project. We can still do a project if we want. Talk to Kostas about this one to replace the Kaggle with research.

## Linear Regression
We'll use x for data, $$\phi$$ for features, and y or t for targets. 

General formulation, learn the function:

$$ y(x,\mathbf{w}) = \sum_{j=0}^M w_jx^j = w_0 + \sum_{j=1}^{M-1}w_j \phi_j(\mathbf{x}) $$

This function is linear (why is this so?) Note that the basis functions do not need to be linear. The basis could be polynomia, or Guassian, or Sigmoidal, or whatever. We want to minimize some objective function, eg SSE. 

**Batch gradient descent**:

$$ \nabla E(w) = \sum_{n=1}^N (w^T \phi(x_n) - t_n)\phi(x_n) $$

$$ w := w - \eta \nabla_w E(w) $$

Also popular is **stocastic gradient descent**, which is online. We also have the option of using the **closed form solution**, which is only possible if we can do batch gradient descent. The closed form solution is:

$$ \hat w = (X^T X)^{-1} X^T y $$

Note that X^T X is invertible. Also important is that we scale inversion on number of features, not number of datapoints.

## Intro to Regularization
How do we choose the degree of the polynomial for least squares? We can regularize, where we penalize for high amounts of "bend" or "magnitude" in the weights. Quick discussion on L2 vs L1 regularization and the sparsity that L1 provides.
