---
layout: post
title: Data Analysis II 
category: eecs598
---
##Clustering

### K-Means
We'll randomly assign some centroids to data, assign points to them, and iteratively cluster. Good because it's simple, but choosing the starting point can be an issue. Density must be "bloblike" and we have to choose starting parameters to begin with. The process is stochastic and so not reproducible. 

### Hierarchical Clustering
We start with each point as a cluster and we merge clusters repeatedly. If we merge the closest clusters repeatedly we'll eventually end up with a tree of clusters. Once we have a cluster, how do we decide which method to compare the distances between them? We can use minimum, maximum, avg, or centroid.

### Fuzzy Clustering
Basically K-means but we assign membership to multiple sets and allow each point to "maybe" be in a cluster.

## Feature Selection
We want to rank features for how well they do in our algorithms. How do we choose the features? We can use correlation, try each one, etc. Correlation only lets us look at one feature at a time but in multiple dimensions we might be able to classify. In most cases we'll use forward or backward feature selection.

### Principal Component Analysis
We try and extract uncorrelated features by creating a set of basis functions in the original feature space. We want to maximize the variance for each component with respect to the original data.

### Multidimensional Scaling
We want to capture the distance between points in our lower dimensional representation. In MDS we only require a notion of how different points are. This is considered a type of embedding.

### Manifolds
What do we do when the distribution of values follows a structured layout, aka a manifold. In this case the Euclidian distance no longer applies. Many times we'll just embed a manifold into Euclidian space. A common way to do this is via isomap, where we look at the distance over a graph of the points in manifold space.
