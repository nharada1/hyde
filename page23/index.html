<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/12/eecs584/">
        Paper Review - The Google File System
      </a>
    </h1>

    <span class="post-date">12 Sep 2014</span>

    <p>This paper details the Google File System, a distributed, fault tolerant file system in use by Google to handle their massive amounts of data on commodity hardware. The GFS uses a master-node configuration, running many small “chunkservers” on commodity Linux machines and a master server to coordinate operations between those nodes. The file system runs in user space (ie is not a kernel process) and <em>expects</em> some nodes to be in a failure state at some time. This expectation allows for scalability and a highly fault tolerant system. </p>

<p>Clients communicate first with the master server, which holds in memory a hash of each file path to the corresponding chunk location (or locations as the data is duplicated on multiple machines at all times). The master server returns the server(s) location(s) and the client does the majority of its communication with the chunkserver directly. This maximizes system throughput, as each node uses its own Ethernet connection and there is no bottleneck at the master server for many simultaneous clients. This high throughput generally comes at the expense of latency. The chunkservers also supports fast, simultaneous writes to a single file via “record appends”, which guarantee only that the data will be written, and not in what order.</p>

<p>The master servers themselves (plural because even though one master runs at a time there are duplicates to prevent data loss) manage the network and chunk layout. The master holds metadata about each chunk, and requires each chunkserver to periodically check in with a “heartbeat” message. The master allows “snapshots” to be made by creating a second pointer to a chunk and requiring the chunkserver to duplicate that chunk at next access. Additionally, the master manages garbage collection by simply marking items as “deleted” and occasionally running a garbage collector to lazily delete the items. </p>

<p>This paper is an interesting read because this is the paper written for a commercial system that was designed with specific industry goals in mind. As a consequence of being designed by Google, the GFS has very specific performance goals in mind. Google knows that most of their data will be written or read as a stream by many producers and consumers, and thus made tradeoffs to support this. The filesystem is not optimized for many small writes, although it would have been interesting if the paper examined these possibilities in their conclusions. Additionally, Google requires high throughput and dependency at the expense of hardware. Thus, the high level design emphasizes that hardware is cheap and downtime is expensive. For a home or small business user, this design is totally impractical because it requires so many machines (even the small test clusters use 16 nodes and 3 masters). </p>

<p>The paper briefly mentions diagnostic tools that the GFS uses, such as RPC logs and other diagnostic logs, but does not disclose them or provide details. It would have been nice if the paper went into further detail, as diagnostics are important in production ready systems. I suspect that these logs are partially industry secrets, and Google feels no need to disclose their proprietary diagnostic systems in the paper. This weakness is a general weakness of all industry papers – the authors cannot and will not tell us certain things about their system because doing so would compromise the company’s proprietary data. Even in this paper, we are given very little information about the benchmark system’s actual function, as the production server and especially research server details are likely shrouded in secrecy by the company. </p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs598/2014/09/11/eecs598/">
        Data Representation
      </a>
    </h1>

    <span class="post-date">11 Sep 2014</span>

    <h2 id="time-vs-frequency-domain">Time vs Frequency Domain</h2>
<p>We can represent signals in the time or frequency domain. Time can be okay, but many biological processes are repeating and so we can use the frequencies instead for analysis. Basic idea behind fourier analysis, won’t bother going into details here. Read an undergrad textbook.</p>

<p>It’s hard to model signals as time and space localized, however. For example, we can do a STFT and that will give us a look, but our window determines resolution and we can never have it all.</p>

<h2 id="non-linear-approaches">Non-linear approaches</h2>
<p>A few non-linear approaches are interested:</p>

<h4 id="poincare-plot-analysis">Poincare plot analysis</h4>
<p>Plot the signal as a function of itself for various time points. This lets us see variability over time.</p>

<h4 id="entropy">Entropy</h4>
<p>Lets us quantify how regular or disordered a set A with categories a is. How do we extend to real signals? Approximate entropy. We basically window our signal and look for similar windows within the time series.</p>

<h4 id="lyapunov-exponents">Lyapunov Exponents</h4>
<p>Measures the rate at which closely spaced trajectories diverge. We want to see how signals relate.</p>

<h4 id="detrended-fluctuation-analysis">Detrended Fluctuation Analysis</h4>
<p>Useful for revealing the extent of long-range correlation in a time series. Usually we remove the mean, divide the signal into windows, and then study by windows.</p>

<h2 id="template-matching">Template Matching</h2>
<p>We want to somehow match templates to the signal.</p>

<h4 id="cross-correlation">Cross-correlation</h4>
<p>Convolve two signals. Problematic because time variations change cross-correlation. We generally will use some time warping to compare. Chih-chun uses this in his morphological variability papers.</p>

<h4 id="alignment-techniques">Alignment-techniques</h4>
<p>Instead of just correlating, we’ll use time warping. Good because it’s simple and generally gives good performance. However, it’s computationally expensive.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/11/eecs584/">
        Paper Review - RAID, High-Performance, Reliable Secondary Storage
      </a>
    </h1>

    <span class="post-date">11 Sep 2014</span>

    <p>Mechanical disk drives are by far the most common point of failure in modern computer systems. As a result, disk redundancy is of upmost importance in any database system. Even if the individual disks have an acceptable median time to failure (MTTF), the large data storage requirements of any current database requires additional drives, and thus decreases the MTTF by magnitudes of time. RAID, or Redundant Array of Inexpensive Disks, provides standard configurations to aggregate multiple disks into one logical volume. This paper provides an overview of the various RAID configuration options (referred to as <em>levels</em>), as well as performance and implementation considerations (although some of this was outside the scope of this reading).</p>

<p>The RAID standard specifies techniques used to partition data and store/restore data, but does not specify exact implementations. These configuration options are split into seven <em>levels</em>.</p>

<ul>
  <li>Level 0 – This level simply uses multiple disks as one large volume, with no redundancy across disks.</li>
  <li>Level 1 – A basic data redundancy, level 1 creates two copies of data, one on each disk.</li>
  <li>Level 2 – Level 2 uses error correcting codes as used in main memory hardware. This reduces the disks required to store redundancy, but multiple recovery disks are required to identify failures.</li>
  <li>Level 3 – Uses bit interleaved parity and stores parity information on extra disks. This allows the volume to use only one parity disk, but for writes the volume must write to that additional disk and thus the volume can only service a single write request at once.</li>
  <li>Level 4 – Block interleaved parity is similar to level 3 but parity is written in blocks called striping units. Again the parity disk easily becomes a bottleneck.</li>
  <li>Level 5 – Block interleaved distributed parity is similar to level 4 but instead interleaves parity blocks throughout the regular data. This removes the parity disk as a bottleneck and allows fast small read, large read and write access. The distributed parity is a weakness, however, for small writes that require syncs across multiple volumes.</li>
  <li>Level 6 – The final level uses Reed-Soloman codes to allow for additional disk failure over level 5. This level offers improved redundancy at the expense of space, and compounds the small write problem of level 5 by using more volumes.</li>
</ul>

<p>The paper further visits reliability of the different RAID levels, as well as offering a look at differing failure modes and adjusting calculations for more reasonable failure conditions. Besides double disk failure, the paper examines system crashes and uncorrectable bit error for both level 5 and level 6 disk arrays. As expected, level 6 provides strong protection against failure, with even the catastrophic system crash having a 10 year probability of less than 8%. The level 5 RAID is reasonably well protected against double disk failure and system crashes, but has an almost 25% chance to fail due to bit error within 10 years. The final section of our reading concerns physical layout concerns and practical issues in the RAID system.</p>

<p>I felt this paper did a good job of addressing the RAID specification in a technical but accessible manner. I especially liked that it offered more practical considerations, such as the orthogonal RAID section, while still offering theoretical conclusions. While the paper did briefly discuss and hint that only RAID level 5 and 6 were widely used in large systems, I felt it failed to offer much discussion about why this was the case, or offer counter examples of when lower levels would be used in real-world scenarios. It also would have been interesting to see tangible examples of when varying RAID levels would be used in industrial applications. Last, I would have been interested in a discussion about implementation, especially operating system support. This paper is old enough, however, that such standards may not have evolved yet.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/11/eecs492/">
        Uninformed Search
      </a>
    </h1>

    <span class="post-date">11 Sep 2014</span>

    <h2 id="how-do-we-search">How do we search?</h2>
<p>Create a tree with nodes, we’ll define nodes as either:</p>

<ul>
  <li>Root node - Initial state</li>
  <li>Fringe node - A node along a path in the graph</li>
  <li>Leaf node - A node without children</li>
</ul>

<p>Note, there is a difference between a state and a node. A node represents a state, but the state can be represented in different nodes. The path cost can be simple or complex.</p>

<p>Our node strategy dictates which node to expand to in any particular search situation. Candidates are nodes at the <strong>frontier</strong>, which we maintain in some kind of queue.</p>

<h2 id="types-of-search">Types of Search</h2>

<h3 id="tree-search">Tree Search</h3>
<p>We’ll assume the state space is a tree. The frontier is a queue of nodes (or partial paths). This search is basically how we’ll look for a leaf of a tree data structure. The tree is built as we go along, and how we do the actual search is specific to depth or breadth first search.</p>

<p>What’s the issue with this? If we have a cyclic graph, we can end up looping forever looking for a goal. What if we keep track of visited states?</p>

<h3 id="graph-search">Graph Search</h3>
<p>A more general search where the state space is a graph. The explored set holds nodes already explored, and when we search we only add nodes to the frontier if they are unexplored. </p>

<h2 id="infrastructure-for-search">Infrastructure for Search</h2>
<p>Each child node has a state, a parent, and a current path cost to get to that node. Usually we’ll use a type of queue to store the data for our search, and this queue will change types depending on the search. </p>

<p>How can we measure performance?</p>

<ul>
  <li>Completeness - Can an algorithm find a solution</li>
  <li>Optimality - Can the algorithm find the optimal solution, usually in terms of path cost</li>
  <li>Time complexity - How long does it take to search</li>
  <li>Space complexity - How much memory is needed to search</li>
</ul>

<p>Generally, uninformed search is limited in complexity.</p>

<h2 id="uninformed-search">Uninformed Search</h2>
<p>The simplest form of search, uses only information available to the algorithm. The distinguishing factor is the <strong>search order</strong>.</p>

<h3 id="breadth-first-search">Breadth-first search</h3>
<p>We’ll look for the solution at each layer, generally we’ll find the shallowest solution first. For this search we use a FIFO queue. We test when the node is generated not expanded. This search discards any new path to a state we’ve already explored. The algorithm is complete, and possibly optimal if there is a non-decreasing path cost as we move down nodes.</p>

<p><strong>Time Complexity</strong> - <script type="math/tex">O(b^d)</script></p>

<p><strong>Space Complexity</strong> - <script type="math/tex">O(b^d)</script></p>

<p>Both are exponential, and breadth-first will almost always run out of space first.</p>

<h3 id="uniform-cost-search">Uniform-Cost Search</h3>
<p>When step costs are equal the BFS is optimal. If not, we can use UCS instead, where we care about path cost instead of hops. Uniform-Cost search expands node n with the lowest path cost. Now we use a priority queue for our data structure. The goal is tested for when a node is selected for expansion. All nodes at cost less than c are expanded first. The UCS is optimal and complete (given step cost above zero). For complexity, C-star is optimal solution and <script type="math/tex">\epsilon</script> is step size.</p>

<p><strong>Time Complexity</strong> - <script type="math/tex">O(b^{1+[C^* / \epsilon]})</script></p>

<p><strong>Space Complexity</strong> - <script type="math/tex">O(b^{1+[C^* / \epsilon]})</script></p>

<h3 id="depth-first-search">Depth First Search</h3>
<p>We’ll search from top to bottom first, using a LIFO queue. Although DFS is complete as a graph search, it’s not optimal.</p>

<p><strong>Time Complexity</strong> - Still <script type="math/tex">O(b^m)</script> where m is the maximum depth. This is even worse.</p>

<p><strong>Space Complexity</strong> - In graph search there is no advantage of BFS, but in tree search we can remove a node from memory after it’s been explored. Storage requirement is now <script type="math/tex">O(bm)</script> which is <em>linear</em>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/09/eecs492/">
        Problem Solving and Search
      </a>
    </h1>

    <span class="post-date">09 Sep 2014</span>

    <h2 id="agents">Agents</h2>
<p>Simple reflex agents based only on world at this moment.</p>

<h3 id="types-of-agents">Types of Agents</h3>

<h4 id="model-based-reflex-agent">Model based reflex agent</h4>
<p>Now we want to use a model to estimate state from the percepts. We don’t need to see the whole world state. Agents in this model will update their state according to a model (make a best guess at true world state) and then match a rule to execute an action.</p>

<h4 id="goal-based-agent">Goal based agent</h4>
<p>The agent wants to reach a goal, so it looks for desirable states. Now we need to consider what will happen if we take an action and if we want the result.</p>

<h4 id="utility-based-agent">Utility based agent</h4>
<p>Uses a performance measure to differentiate world states. This uses a utility function, which is an internalization of the agent’s performance. An agent like this can deal with conflicting goals.</p>

<h4 id="learning-agent">Learning agent</h4>
<p>An agent that learns over time and becomes better as it explores an unknown environment. This model contains a critic to pass information back to the learning element.</p>

<h3 id="evaluating-agent-designs">Evaluating Agent Designs</h3>
<p>We need to somehow find optimality with respect to PEAS characterization. This can be either via trials or proof.</p>

<h2 id="search">Search</h2>
<p>We want to represent a path to our goals. In many cases we can represent all possible states as a space, eg the search space. For example, we can represent the search state as a physical map. Right now we’re finding our path through the world, and assuming the world won’t change. How do we do search?</p>

<ul>
  <li>Characterize the World</li>
  <li>Formulate a Goal</li>
  <li>Formulate the Problem</li>
</ul>

<p><strong>Search</strong>: The process of looking for a sequence of actions that records a goal.</p>

<p>Even for seemingly non-graph problems we can define a state space graph for each state. </p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page24">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page22">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
