<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/04/06/eecs586/">
        Reductions
      </a>
    </h1>

    <span class="post-date">06 Apr 2015</span>

    <h2 id="sat-reduces-to-3-dimensional-mapping">3-SAT reduces to 3 dimensional mapping</h2>
<p>For an instance of 3SAT you have a set of U variables and clasuses C. For 3DM you have variables U, X, Y. </p>

<p>Notes:</p>

<p><a href="https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/3dm.pdf">Slides</a></p>

<p><a href="http://courses.cs.vt.edu/cs5114/spring2009/lectures/lecture24-np-complete-problems.pdf">Slides</a></p>

<p><a href="http://www.cs.berkeley.edu/~vazirani/algorithms/chap8.pdf">Writeup</a></p>

<h2 id="knapsack">Knapsack</h2>
<p>Given a finite set U, weight w(u) and value v(u), where w and v are positive integers. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/06/eecs545/">
        Markov Decision Processes
      </a>
    </h1>

    <span class="post-date">06 Apr 2015</span>

    <h2 id="markov-process">Markov Process</h2>
<p>At each time step we get a reward from our action. Consider the problem in the book found <a href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node33.html">here</a>. Transitions are random, so even without noise in the reward there is an expected value. The optimal policy is simultaneously optimal for every state, although the optimal policy may not be unique. As a Markov problem, the probabilities don’t change over time, so the value function <em>is a function of just state</em>. </p>

<h2 id="problems">Problems</h2>
<p>We start with a given policy and want to compute how good it is. Optimal control problem is find the optimal policy. Today will cover policy evaluation.</p>

<h2 id="policy-evaluation">Policy Evaluation</h2>
<p>Implementation note: to construct a policy matrix, take a row from each matrix of actions corresponding to the policy we are given. Given a pi, how do we find a Vpi? </p>

<p>First option: Monte Carlo approach. Just sample a ton of times. We’ll stop early because eventually the discount factor creates a vanishing function.
Second option: Matrix inversion method. Requires a model. Uses the value equation and solves.
Third option: Iterative approach. Same as previous, but we solve recursively.</p>

<p>Why Monte Carlo?</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/04/01/eecs586/">
        Complexity
      </a>
    </h1>

    <span class="post-date">01 Apr 2015</span>

    <h2 id="np-complete">NP-complete</h2>
<p>We say A is NP-complete if </p>

<ul>
  <li>
    <script type="math/tex; mode=display"> A \in NP </script>
  </li>
  <li>
    <script type="math/tex; mode=display"> B \leq_p A \exists B \in NP </script>
  </li>
</ul>

<p>We can say C is NP-complete if</p>

<ul>
  <li>
    <script type="math/tex; mode=display"> C \in NP </script>
  </li>
  <li>There is an NP-complete D s.t. <script type="math/tex">D \leq_p C</script></li>
</ul>

<h2 id="version-of-satisfiability">Version of Satisfiability</h2>
<p>U is a set of boolean variables and a literal is either u or not u. A clause is a set of literals. A clause is satisfied if one of the literals is true. A collection of clauses is satisfied if there is a truth assignment that satisfies all the clauses. </p>

<p>In 3-SAT the clauses only have 3 literals. It turns out we can reduce many problems into a 3-SAT problem. </p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/01/eecs545/">
        Reinforcement Learning
      </a>
    </h1>

    <span class="post-date">01 Apr 2015</span>

    <h2 id="bandits">Bandits</h2>
<p>Recall the bandit problem. We don’t know what the distribution of each bandit is, but we want to maximize our reward. This will transition into MDPs. First let’s talk about notation:</p>

<ul>
  <li>Action values (Q) is the average reward for a specific action.</li>
</ul>

<p>Epsilon greedy algorithm for playing bandits:</p>

<p>We select a greedy arm most of the time, and a random arm some other percentage of the time. We can also start with a high epsilon and decrease it. With a very small epsilon we’ll find almost the optimal. How fast can we shrink epsilon though? We need to explore but if we shrink too slow we’ll perform worse. We’ll call this change in the parameter the <strong>schedule</strong>. </p>

<p>For episilon-decreasing, we can bound our sum on epsilon by a few things. One, we want the sum to be infinity. Two, we want the sum of epsilon to be finite. A harmonic series satisfies this property. In practice we have bigger steps and we reduce the learning rate, because in practice 1/t is too slow. </p>

<p>Softmax Procedure:</p>

<p>Another version is to set the probability of choosing a lever as the softmax of the lever. Temperature is a parameter of the softmax, and with a higher temperature you’ll have a more even probability. </p>

<p>Bandits are a pure exploration-exploitation problem. They have no generalization problems.</p>

<h2 id="final-and-logistics">Final and logistics</h2>

<p>No project. Two more homeworks. LOL. Remaining 20 percent will go to the better of the exams. </p>

<h2 id="contextual-bandits">Contextual-bandits</h2>
<p>We can choose levers based on context, and then we can generalize the actions based on new contexts. </p>

<h2 id="markov-decision-process">Markov Decision Process</h2>
<p>Notation</p>

<ul>
  <li>Ot is the observation at time t</li>
  <li>Rt is the reward at time t</li>
  <li>At is the action at time t</li>
</ul>

<p>The agent wants to maximize a reward given observations by taking actions. There is some discount factor because doing something now is better than later. </p>

<p>How does a world satisfy a Markov property? If we have the last observation and action, then in a Markov world this is all we need to characterize the world. If observations are perfect, then we call the observation the state.</p>

<p>An MDP is defined by:</p>

<ul>
  <li>A set of states</li>
  <li>A set of actions</li>
  <li>Transition probabilities</li>
  <li>A reward function</li>
  <li>A discount factor</li>
</ul>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/03/30/">
        Online Learning
      </a>
    </h1>

    <span class="post-date">30 Mar 2015</span>

    <h2 id="online-learning">Online Learning</h2>
<p>We want to learn as we get new data in the system. Generally we can use the multiarmed bandit problem to try and minimize regret, where at each point we select an arm. There exists a strategy where our regret is no worse than O(log n). The UCB algorithm says that you should choose the arm with the highest uncertainty and the highest reward. </p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page5">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page3">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
