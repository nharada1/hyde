<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/12/01/eecs584/">
        Paper Review - Science in an Exponential World
      </a>
    </h1>

    <span class="post-date">01 Dec 2014</span>

    <p>In this short paper, the authors provide a quick look at the rapid development of big science and the need for databases to support scientific applications. They point out that while in the past scientists would keep a lab notebook of observations, modern scientific toolchains require complex data analysis and terabytes of actual storage. They (correctly in my opinion) point out that the problem is compouding, increasing at an exponential rate as scientists become more familiar with software and hardware simultaneously drops in cost. The strong technical point of this article is the idea that datacubes can provide the analytic power that scientists expect on an interactive timescale for larger sets of data. I agree this is likely true, but I think that in many cases scientists don’t want to commit to learning one of the many databases that’s available, and would rather learn a standard. I think if a standard is accepted that would be great for the science community. It seems that Matlab and more recently Python tend to be computational standards in science. We will see if a database steps up, and if Hadoop comes out on top due to its wide adoption and low cost.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/11/30/eecs584/">
        Paper Reivew - Requirements for Science Databases and SciDB
      </a>
    </h1>

    <span class="post-date">30 Nov 2014</span>

    <p>As scientific endeavors generate more information on larger scales, big projects generally turn to databases as a way to maintain their experimental data. In this paper, the authors introduce SciDB, a database designed to accommodate scientists and their “big science” endeavors. The authors of the paper met with various members of large scientific projects, including the Large Hadron Collider and the Large Synoptic Survey Telescope. Also present were members of the industry, who were included ostensibly to increase the scope of the project but likely provided the majority of the funding for the database. The paper discusses the various features a database for scientists would require. Because the various scientific disciplines require a wide variety of data representations, it’s difficult to include everyone’s feature requests. In the end, SciDB appears to be a Matlab style “matrix” storage system, that allows for various dimensions of arrays, including arrays that are jagged to support structures such as graphs and other exotic designs.</p>

<p>While previous work has been done on scientific databases, it appears that all of the previous efforts failed in an attempt to be everything to everyone. Thus, some features of SciDB are left to the end user in hopes that they can support most people. Two important features of SciDB are quite novel, and powerful tools for the scientific community. The first is non-destructive edits, meaning that previous values are saved to allow scientists to see all experimental results. While other tools like BigTable have accommodated a time slice, this tool allows easier management by including an attribute titled “updatable” to arrays. Additionally, SciDB supports uncertainty in its queries. Thus, error bars (assuming a normal distribution) are computed and saved throughout the computation, allowing the user to keep track of precision easily and throughout the database chain. This is one of the concepts behind BlinkDB, although in Blink the error bars are specifically computed by the sampling interface and in SciDB they may be added by the user. Extending SciDB to include sampling may have been a better idea than extending BlinkDB to include matricies. </p>

<p>The error bars also present a weakness in that they are parametric, and they assume that the underlying distribution is normal. It appears that the user is able to extend the error bar handling, which may be able to alleviate the problems. It’s also worth noting that this paper had no actual product at the time of release. Although SciDB now exists, it was bold of the authors to release a paper based on theory alone (although the paper is more a proposal than a research product). One more interesting observation is that, somewhat not-unexpectedly, Stonebraker again uses this paper as an opportunity to tell his audience that “Postgres supported features years ago”. While it does get tiresome to see him constantly push Postgres, it is nice to be reminded that object-relational databases exist and in many cases may provide a solution to a problem without building a new database.</p>

<p>One more thing - the SciDB website treats SciDB like a product even though it’s open source, requring personal information to download. This is not the direction SciDB should be moving if it hopes to achieve widespread adoption.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/11/29/eecs584/">
        Paper Review - Join Synopses for Approximate Query Answering
      </a>
    </h1>

    <span class="post-date">29 Nov 2014</span>

    <p>This paper is quite similar the BlinkDB, and as such I will gloss over some technical aspects even though this paper was published first. Again, creating a query system that can answer with approximate answers is a powerful solution to large data warehousing and data mining. By allowing interactive queries (meaning queries that run in near real time), the user is hidden from the complexities of dealing with massive amounts of data as long as they remember their answers aren’t perfect. In this paper the authors introduce join synopses for relational databases, and show that by using these join synopses a system can answer approximate queries with foreign key joins.</p>

<p>The paper introduces the problems with joining random samples, and shows that by doing so the results of the join will be poor. This is because (1) the results of a sample before the join and of a sample after the join are not the same and (2) joining random samples will result in even smaller outputs which reduce confidence in the results. Like BlinkDB, this paper proposes a sampling method based on distinguished samples. Unlike BlinkDB, Aqua is designed to compute “join synopses”, which work to allow any arbitrary join. BlinkDB, on the other hand, samples from tables directly and does not specifically support joins. </p>

<p>The authors also introduce a method of computing confidence bounds by extracting sub-samples from samples. To me this sounds just like bootstrapping, and I wonder why this terminology wasn’t used in this paper and how this is different from the bootstrapping that BlinkDB does. I suspect that this is simply a change in terminology that occurred over the years. While bootstrapping is not a new technique, it has gained popularity in recent years. I did like that this paper offered some theoretical insight into its techniques and provided proofs of the join’s correctness (although not fully here they do outline the basics and refer the reader to the full version of their paper). Because my project for this class involves modifying BlinkDB, I had trouble finding the actual mathematical underpinnings of the work (the references simply send the reader to a textbook). One obvious strength of the BlinkDB system is that its designed for distributed computing, while the Aqua system is overlaid on a commercial DBMS that is likely not designed to be shared-nothing. It remains to be seen as to whether or not the system can work atop a shared-nothing architecture such as Teradata. Part of this is because the paper is old enough that shared-nothing DBMS systems were not on the forefront of computing. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/11/28/eecs584/">
        Paper Review - BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data
      </a>
    </h1>

    <span class="post-date">28 Nov 2014</span>

    <p>As data warehouses grow larger, the amount of time required to process queries begins to outpace the development of the hardware and software that runs the system. Ethernet links and hard-drives have limits on speed, and interactive queries become impossible in petabyte scale systems on commodity hardware. To remedy this, the authors of this paper propose a sampling based system. The core concept of BlinkDB is that by sampling the tables of a database, the system can return approximate answers to queries. These answers may not be exact, but should be close enough for many OLAP and data mining tasks. For example, if we wanted to calculate the average income of a town, we could sample the residents to get an approximate answer, which in many cases will be close enough. The paper also stresses the idea of error bounds for the returned results. This is important because a user wants to know how close their answer is to the true result. By returning error bounds the user can keep in mind the statistical consequences of their actions. </p>

<p>BlinkDB is built atop two key ideas: a stratified sampling method and a dynamic sample selection strategy. The first important contribution is stratified sampling. Because the underlying samples may be inadequately distributed, BlinkDB maintains a set of stratified samples over the original data. Stratified sampling is the technique of sampling from different groups unequally in order to reduce bias. The authors do this because if some members of a subset are rare, we will be unable to construct accurate estimates over them if we sample indiscriminately. BlinkDB also introduces optimizations that allow the user to specify the latency or accuracy of a query in SQL. For example, a user can declare they would like results WITHIN 5 SECONDS or ERROR WITHIN 10% AT CONFIDENCE 95%. The paper outlines a method in which multiple samples are kept of the data, and the appropriate sample set is used to properly answer a query. Additionally, BlinkDB must keep an error latency profile of various queries in order to estimate the time cost of a query - the software uses both an error and latency profile for this. The system itself is built atop Spark and Hive, intercepting queries in the Shark driver to apply Hive UDAFs to the data. </p>

<p>The strength of this paper lies in its stratified sampling method and the error latency profile used to estimate cost of a query. The results are promising, and sampling a database allows for interactive query sessions on databases that are otherwise too large. The system also is quite powerful because it’s built atop Spark, a memory resident extension to the distributed computation ecosystem Hadoop. The paper does have some weaknesses not addressed in this revision. One major weakness is that the aggregate functions must assume a normally distributed super-sample because they rely on closed-form estimators. Later versions of the paper introduce a bootstrapping method to deal with non-parametric datasets. Unrelated to the paper, the actual code released by the project is quite poor, with few features implemented and many discrepancies to the original work. This makes it impossible to duplicate the results given in the paper.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/11/25/eecs492/">
        Complex Decisions and Learning
      </a>
    </h1>

    <span class="post-date">25 Nov 2014</span>

    <h2 id="partially-observable-markov-decision-processes">Partially Observable Markov Decision Processes</h2>
<p>Aspects of a POMPDP:</p>

<ul>
  <li>Transition model</li>
  <li>Actions</li>
  <li>Reward function</li>
  <li>Sensor model (new to POMDP)</li>
</ul>

<p>In a POMDP, we don’t know the state of the work or the agent. In this case, we can make the gridworld a POMDP simply by removing the knowledge of our location. In a POMDP, a <strong>belief state</strong> is a probability distribution over possible states. We want to determine the optimal action by mapping belief states to actions. Notice that this doesn’t rely on the actual state.</p>

<p>How can we update our belief states? The probability of perceiving e, given action a is performed in belief state b:</p>

<script type="math/tex; mode=display"> P(e \mid a,b) = \sum\limits_{s'} P(e \mid a,s',b)P(s' \mid a,b) </script>

<h2 id="decisions-with-multiple-agents">Decisions with Multiple Agents</h2>
<p>Draws from game theory in many cases. For example: prisoner’s dilemma. </p>

<ul>
  <li>Dominant strategy: We always testify because it minimizes our losses deterministically, but this assumes that every outcome is equal.
    <ul>
      <li>Strongly or weakly dominates</li>
      <li>Pareto optimal, pareto dominated</li>
    </ul>
  </li>
</ul>

<p>Equalibriums exist in games, in this case if both testify that’s an equalibrium. John Nash proved that all games have at least <em>one</em> equalibrium.</p>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page13">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page11">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
