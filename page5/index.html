<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/03/25/eecs586/">
        Convex Hulls
      </a>
    </h1>

    <span class="post-date">25 Mar 2015</span>

    <h2 id="convex-hulls">Convex Hulls</h2>
<p>We have some points in two space. This is <strong>Graham Scan</strong>. We want to find extreme points of convex hull. Start with a point guaranteed to be extreme. Take rightmost, lowest point. Sort remaining points by angle relative to start. Each time we reach one add it to a list of tentative extreme points. Test convexity a point at a time and remove non-convex points. When we remove a point we must check the ONE previous point until we hit an okay point. Time is O(n log n) based on amortized analysis. In this analysis, we must charge a point when we delete it.</p>

<p>For <strong>Jarvis’ March</strong>, we again start with the bottom left point. Note that the smallest angle <em>must</em> be an extreme point. Now we move from extreme point to extreme point, going around in a circle until all of our extreme points are added. This is O(nh) time where n is the number of points and h is the number of extreme points. </p>

<p>If we’re given the points of each polygon in order, we can find the convex hull in O(P1+P2). </p>

<p>We can also do a divide and conquer method for finding the convex hull. Divide the middle and combine the two by finding the secant lines that are invalid and removing them. Why do this? In higher dimensions. </p>

<p>Expected case with IID points? Jarvis’ march is fairly slow, but divide and conquer is faster expected. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/03/25/eecs545/">
        Hidden Markov Models
      </a>
    </h1>

    <span class="post-date">25 Mar 2015</span>

    <h2 id="sequential-data">Sequential Data</h2>
<p>In many cases you don’t want to model things as IID, generally because they have dependencies over time (or sequence, whatever). </p>

<h2 id="markov-chain">Markov Chain</h2>
<p>A Markov chain is a series of random variables where future is independent of the past, given the present. Basically, we know the future state from today’s state. This is still simplified, but isn’t IID (where tomorrow is independent of today). We can also create higher order Markov models, where the dependencies are extended going further back.</p>

<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<p>An HMM is a Markov chain that emits observations in every time step. We can’t see the hidden state, but we can assume it’s there and we can model it. In HMM, the latent variable is discrete. You can also imagine that the Z’s are not discrete, which creates a linear dynamical system.</p>

<p>In many cases, we will need to add constraints to the transitions. </p>

<h2 id="maximum-likelihood-for-em">Maximum Likelihood for EM</h2>
<p>In this case, gamma is the probability that a particular state is created by a given point. </p>

<h2 id="backwards-forwards-algorithm">Backwards-Forwards Algorithm</h2>
<p>Message passing algorithm that sends alpha and beta backwards and forwards through the model. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/03/18/eecs586/">
        Closest Pairs
      </a>
    </h1>

    <span class="post-date">18 Mar 2015</span>

    <h2 id="closest-pairs">Closest Pairs</h2>
<p>How do we find closest pairs in 2d? Use the same algorithm as for 1d. As we raise the number of dimensions we’ll have to raise the number of neighbors we look at, but it’ll still be O(n) because the neighbors are fixed. What if we want the problem analysis in terms of d and n? </p>

<p><script type="math/tex">\Omega(3^d n)</script>
<script type="math/tex">\Theta(d3^d n)</script></p>

<p>So this is O(n) where the implied constants are a function of d. </p>

<p>In 2d what is the worst case? If we split into two parts, the middle needs to be examined. If we divide a space into patches of delta, then were are no more than 5 points in that space (in 2d). </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/03/18/eecs545/">
        Decision Trees
      </a>
    </h1>

    <span class="post-date">18 Mar 2015</span>

    <h2 id="information-theory">Information Theory</h2>
<p>Decision trees need a way to quantify uncertainty. We’ll use entropy, which is “how many bits does it take to transmit this information”. In the sense that if the data is completely random you’ll need one bit for two symbols, but if it’s not you need fewer bits. If it’s only one, you need zero bits (since the data is always the same).</p>

<p>We can also define joint entropy and conditional entropy.</p>

<p>Information gain describes how many bits on average it would save me if both ends of the channel already <em>knew</em> X. At each node, we want to compute the information gain and that’s how we will construct our tree.</p>

<h2 id="decision-trees">Decision Trees</h2>
<p>Decision trees are easy ways to make decisions — they don’t use much computational power and they’re interpretable. Note that a decision tree can perfectly fit data as long as there isn’t noise. Thus, we either need to stop early or prune the tree in order to prevent overfitting. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/03/11/eecs545/">
        Expectation Maximization
      </a>
    </h1>

    <span class="post-date">11 Mar 2015</span>

    <h2 id="mixture-model-em">Mixture model EM</h2>
<p>Given observation X and latent variable Z, we can say:</p>

<script type="math/tex; mode=display"> \log P_{\Theta}(X) = \log \sum_Z P_{\Theta}(x,z) </script>

<p>There is a full derivation on CTools. We’ll use (Carly Rae) Jensen’s inequality and introduce a Q(z) that is the expected value of the log likelihood function, with respect to the conditional distribution of Z given X, under the <strong>current parameter estimates</strong>. </p>

<p>The final algorithm has two steps:</p>

<ul>
  <li>Expectation: Find Q, or the expected value with the current parameters</li>
  <li>Maximization: Fix z, and find the parameter that maximizes Q</li>
</ul>

<p>We are maximizing the lower bound! Claim: we have guarenteed to have improved the log likelihood. </p>

<p><strong>Exam Question</strong>: Given a simple variation of GMM, give the E and M steps of the model</p>

<h2 id="principal-component-analysis">Principal Component Analysis</h2>
<p>PCA maximizes the variance of the projection of data into a lower dimensional space. Basically we choose the direction of projection so that the resulting projections have the highest variance. We can find principal components one at a time. We want to find the vector u1 that maximizes the projection variance, using lagrange multipliers. This ends up being the same as finding the eigenvector with the largest eigenvalue of the covariance matricies. BOOM THIS IS MATH.</p>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page6">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page4">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
