<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/01/21/eecs545/">
        Classification
      </a>
    </h1>

    <span class="post-date">21 Jan 2015</span>

    <h2 id="classification-problem">Classification Problem</h2>
<p>Given an input vector, assign it to one of K distinct classes. We can define 0-1 loss as the number of incorrect labels.</p>

<p>Approaches:</p>

<ul>
  <li>Nearest neighbor classifiers</li>
  <li>Discriminant functions (SVM, Perceptron, etc)</li>
  <li>Learn distributions
    <ul>
      <li>Discriminative models (Logistic Regression)</li>
      <li>Generative models (HMM, Naive Bayes)</li>
    </ul>
  </li>
</ul>

<p><strong>Exam Question:</strong> Why is the posterior the quantity of interest?</p>

<h2 id="logistic-regression">Logistic Regression</h2>
<p>Model class posterior using a sigmoid applied to a linear function of the feature vector:</p>

<script type="math/tex; mode=display"> p(C_1 \mid \phi) = y(\phi) = \sigma(\mathbf{x}^T \phi(x)) </script>

<p>Sigmoid is a smooth squashing function, logit function is the inverse of the sigmoid. This classifier generalizes to the <em>softmax</em> function, which allows many classes and not just two. See slides for usual tricks on finding <script type="math/tex"> P(t \mid w) </script> and on gradient descent derivation.</p>

<p><strong>Exam Question:</strong> Why is placing the decision boundary at 0.5 for the logistic classifier correct?</p>

<h2 id="generative-models">Generative Models</h2>
<p>Generative models learn the joint probabilities while discriminative models learn the conditional probabilities. In logistic regression, we make the assumption that the log odds are a linear function of x. </p>

<h2 id="gaussian-linear-discriminant-analysis">Gaussian (Linear) Discriminant Analysis</h2>
<p>Assume that the class conditional density is Gaussian. We make an assumption of what the Gaussians looks like, and then we estimate the parameters from the data. When we get a new data point, we sample from the distribution to determine the probability of each class. Fact: if we model the probabilities as Gaussians with the <strong>same covariance</strong> matrix, then the optimal decision boundry is linear. </p>

<p>Note to self: Look at derivation and do math yourself.</p>

<p>Logistic regression is more flexible about data distribution, but GDA works well when the distribution follows the Gaussian assumption. In contrast, logistic regression requries costly iterative training procedures, but requires less parameters.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/01/14/eecs586/">
        Sequential Search Analysis
      </a>
    </h1>

    <span class="post-date">14 Jan 2015</span>

    <h2 id="caching">Caching</h2>
<p>For move to front: What is the asymptotic number of expected keys encountered in a search?</p>

<script type="math/tex; mode=display"> 1 + \sum_{i=1}^{n} p_i \sum_{j \dne i}\frac{p_j}{p_i+p_j} </script>

<p>Where pi is the probability of request k_i, and the second term is the prob that kj is in front of ki.</p>

<p>For transposition caching: We’ll assume n is more than 2 and that all probabilities are not the same. R.Rivset proved that transposition has a lower expected number of items examined than MTF does.</p>

<p>In either case, we can represent the state space as a markov chain and take it to steady state to find expected number of items. There are two major methods of analyzing sequential search algorithms: probabilistically or via amortization. </p>

<p>Amortized Analysis of Self-Organizing Sequential Search Heuristics:</p>

<p>There is a sequence of requests, and the optimal ordering is order of decreasing m value. In this analysis we will not go to infinity.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/01/14/eecs545/">
        Linear Regression and Classification
      </a>
    </h1>

    <span class="post-date">14 Jan 2015</span>

    <h2 id="probability-review">Probability Review</h2>
<p>Not many notes here. Review axioms. Review additional properties such as unions, law of total prob, Bayes’ rule. Likelihood functions, applications of Bayes’ rule:</p>

<p>Posterior <script type="math/tex">\propto</script> likelihood x prior</p>

<p>Remember that it’s easier to find the likelihood than it is to find the posterior. </p>

<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<p>Choose a parameter setting w that maximizes the likelihood function p(D|w). We’ll use log-likelihood so we can minimize the log (which has nice properties that let us sum instead of multiply.) There is also the Maximum a posteriori estimation, where we maximize p(w|D) instead of p(D|w). In this case the prior is important. These two different estimates will be the same when the prior is uniform.</p>

<h2 id="maximum-likelihood-interpretation-of-linear-least-squares">Maximum Likelihood interpretation of Linear Least Squares</h2>
<p>Assume a stochastic model:</p>

<p><script type="math/tex"> t = y(x, w) + \epsilon </script> where <script type="math/tex"> \epsilon \sim \mathcal{N}(0, \beta^{-1}) </script></p>

<p>this gives likelihood function:</p>

<script type="math/tex; mode=display"> p(t \shortmid x,w,\beta) = \mathcal{N}(t \shortmid y(x,w),\beta^{-1}) </script>

<p>Minimizing the log likelihood will be exactly minimizing the sum of squares error function.</p>

<p><strong>Exam question:</strong> Given a different stochastic model, derive the maximum likelihood solution.</p>

<p><strong>Exam question:</strong> L2 regularized least squares is the MAP estimate of w with a prior of <script type="math/tex"> p(w) \propto e^{\frac{\lambda}{2} \parallel w \parallel^2} </script></p>

<h2 id="locally-weighted-linear-regression">Locally Weighted Linear Regression</h2>
<p>Weighted regression where the weights are dependent on x (query point), and you solve linear regression for each query point x. The wider the Gaussian, the less complex the model. Note that you’ll need to refit the regression for every single query that comes in.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/01/12/eecs586/">
        
      </a>
    </h1>

    <span class="post-date">12 Jan 2015</span>

    <h2 id="back-to-sequential-search">Back to Sequential Search</h2>
<p>Remembering our example from last time, we talked about how making assumptions on sequential search was the only way to find various running times, especially average. What if all items don’t have the same probability? Let’s suppose there’s some probability distribution on key requests, and requests are independent. How do we choose items to look at?</p>

<ul>
  <li>Sort keys by probability, decreasing order. Prove this is correct.</li>
</ul>

<p>Proof (extremely questionable):</p>

<p><em>Let O be the optimal ordering, O’ be ordering by probability order.</em></p>

<p><em>Suppose there is a case where one key has a higher probability and is farther in the search. If we switch these, we’ve decreased expected search time. Therefore, we never put the ordering out of sorted order in an optimal ordering.</em> </p>

<p>It is most likely, however, the the distribution will be uniform. Additionally, in many scenarios the item is not likely to be requested. Let’s look at an example:</p>

<p>Zipf’s law makes statements about the probability of words in English, based on empirical research:</p>

<ul>
  <li><em>the</em>: 6.18%</li>
  <li><em>I</em>: 1.17%</li>
  <li><em>you</em>: 0.83%</li>
</ul>

<p>Pareto asked a similar question, but about distribution of income. They both found that the ith most likely word occurs <script type="math/tex">\frac{1}{i}</script> times as likely as the most likely word. Let’s look at randomly choosing A, B, or space on a keyboard. We can look at expected length of the word:</p>

<ul>
  <li>len(0) = .33</li>
  <li>len(1) = .66 * .33</li>
  <li>len(i) = (.66)^i * .33</li>
</ul>

<p>This is <strong>not</strong> what Zipf looked at though. The sequence is not geometric because we must account for various transpositions of letters. See the CTools paper for full paper. The important observation of this is that <strong>in computer science, heavy tailed distributions are much more likely than in other disciplines.</strong> This occurs because in CS many things are built up over time.</p>

<p>The number of expected items examined will generally be calculable, but in cases where we don’t know the symbolic error we’ll need to learn the distribution. We’ll look at different caching schemes. Easy solution: back to front, just move cache around when new requests come in. Slightly better: count heuristic, keep track of counts. </p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/01/12/eecs545/">
        Linear Regression
      </a>
    </h1>

    <span class="post-date">12 Jan 2015</span>

    <h2 id="updated-grading-policy">Updated Grading Policy</h2>
<p>Now midterm and final, instead there’s a Kaggle based project. We can still do a project if we want. Talk to Kostas about this one to replace the Kaggle with research.</p>

<h2 id="linear-regression">Linear Regression</h2>
<p>We’ll use x for data, <script type="math/tex">\phi</script> for features, and y or t for targets. </p>

<p>General formulation, learn the function:</p>

<script type="math/tex; mode=display"> y(x,\mathbf{w}) = \sum_{j=0}^M w_jx^j = w_0 + \sum_{j=1}^{M-1}w_j \phi_j(\mathbf{x}) </script>

<p>This function is linear (why is this so?) Note that the basis functions do not need to be linear. The basis could be polynomia, or Guassian, or Sigmoidal, or whatever. We want to minimize some objective function, eg SSE. </p>

<p><strong>Batch gradient descent</strong>:</p>

<script type="math/tex; mode=display"> \nabla E(w) = \sum_{n=1}^N (w^T \phi(x_n) - t_n)\phi(x_n) </script>

<script type="math/tex; mode=display"> w := w - \eta \nabla_w E(w) </script>

<p>Also popular is <strong>stocastic gradient descent</strong>, which is online. We also have the option of using the <strong>closed form solution</strong>, which is only possible if we can do batch gradient descent. The closed form solution is:</p>

<script type="math/tex; mode=display"> \hat w = (X^T X)^{-1} X^T y </script>

<p>Note that X^T X is invertible. Also important is that we scale inversion on number of features, not number of datapoints.</p>

<h2 id="intro-to-regularization">Intro to Regularization</h2>
<p>How do we choose the degree of the polynomial for least squares? We can regularize, where we penalize for high amounts of “bend” or “magnitude” in the weights. Quick discussion on L2 vs L1 regularization and the sparsity that L1 provides.</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page11">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page9">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
