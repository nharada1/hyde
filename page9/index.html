<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/01/28/eecs586/">
        Quicksort
      </a>
    </h1>

    <span class="post-date">28 Jan 2015</span>

    <h2 id="reviewing-the-homework">Reviewing the Homework</h2>
<p>Only thing of note is that number 2 is not divide and conquer, we can skip by k instead and end up with an algorithm with identical efficiency. Goddamn that’s easier than divide and conquer.</p>

<p>To find the lower bound:</p>

<p>I didn’t catch this, go in and ask Quentin or TA.</p>

<h2 id="quicksort">Quicksort</h2>
<p>Overview of quicksort:</p>

<p>We set a pivot point, then split into parts above and below that pivot. Then we sort those by quicksort. Do this by swaps.</p>

<h3 id="worst-case">Worst Case</h3>
<p>Partition is linear. Worst case of actual sort is in the case that one partition is the whole list (ie X is the largest or smallest value) and then we have <script type="math/tex">\Theta(n^2)</script>. Note this assumption requires convexity of quicksort! This means that having one big piece is ALWAYS same/worse than two pieces made from that big piece. </p>

<h3 id="best-case">Best Case</h3>
<p>Partitions are exactly half. This means <script type="math/tex">\Theta(n \log n)</script>. </p>

<h3 id="expected-case">Expected Case</h3>
<p>We’ll say that all input orderings are equally likely and no duplicates. Expected comparisons:</p>

<p><script type="math/tex">\sum_{i=1}^{n-1} \sum_{j=i+1}^n</script> prob that ki and kj are compared. This is only if ki and kj are pivots.</p>

<p>Also note that book picks pivot by choosing the rightmost element. Pick the middle! Or we can sample the distribution and find the median and choose that one.</p>

<h2 id="order-statistics-ch-9">Order Statistics (Ch 9)</h2>
<p>The order statistic is the ith item when the items are sorted (basically). We’ll just pick the lower median. </p>

<p>Median: n/2 statistic</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/01/28/eecs545/">
        Discriminant Functions and Kernel Methods
      </a>
    </h1>

    <span class="post-date">28 Jan 2015</span>

    <h2 id="linear-discriminant-functions-again">Linear Discriminant Functions (again)</h2>
<p>In discriminant functions we directly represent the decision boundary as opposed to learning the distribution. Why is the decision region convex and linear?</p>

<p><strong>Exam question:</strong> Show that the decision region in a linear discriminant function is linear and convex.</p>

<h2 id="perceptron">Perceptron</h2>
<p>A generalized linear function of the form</p>

<script type="math/tex; mode=display">y(x) = f(w^T \phi(x))</script>

<p>where f(a) is the sgn function. We claim that the perceptron will <em>always</em> converge if the data is linearly separable. This is unaffected by learning rate.</p>

<h2 id="kernel-methods">Kernel Methods</h2>
<p>Remember that linear regression is not too computationally expensive because it doesn’t scale with data but with features. We can use the kernel trick to work in high dimensions and only use inner products. </p>

<script type="math/tex; mode=display">k(x,x') = \phi(x)^T\phi(x')</script>

<h2 id="dual-representations">Dual Representations</h2>
<p>See slides for the math. We’ll work through, substituting <script type="math/tex">w = \phi^T a</script> in the objective function (for linear regression) and eventually we end up with:</p>

<script type="math/tex; mode=display">y(x) = k(x)^T(K+\lambdaI_N)^{-1}t</script>

<p>Kernel method scales quadratically with input datapoints, so when to use this vs non-kernel methods. We can use kernel methods for any algorithm that requires a distance between points. However, we can’t find the mean. Distances to the mean is totally fine though.</p>

<p>Can we do stochastic gradient descent with just kernels? Who the fuck knows, but try and figure it out</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/01/26/eecs586/">
        Sorting
      </a>
    </h1>

    <span class="post-date">26 Jan 2015</span>

    <h2 id="master-theorem">Master Theorem</h2>
<p>Three cases for theorem, intuition says:</p>

<ul>
  <li><script type="math/tex">a=b</script> means each layer does about as much work as the previous</li>
  <li><script type="math/tex">a \lt b</script> means each layer does less work</li>
  <li><script type="math/tex">a \gt b</script> means each layer does more work</li>
</ul>

<p>Review the book and notes, because there are fringe cases where we can’t actually apply the theorem.</p>

<h2 id="heapsort">Heapsort</h2>
<p>Uses a heap to maintain data, and reads from heap (which is sorted by definition) to sort. In a heap:</p>

<script type="math/tex; mode=display"> A(i) \geq A(2i) </script>

<script type="math/tex; mode=display"> A(i) \geq A(2i+1) </script>

<p>Maximum element at the root as a result of these local properties. </p>

<p>Why are all these algorithms n log n? We want a lower bound on comparison based sort (remember no assumptions about the data, just comparison.) The intuition is the binary result of sorting algorithms creates the lg bounds. Two different inital sorted orders can’t end up at the same leaf, so the number of leaves must be &gt;= number of initial input orderings. There are n! input orderings. For tree height h # of leaves <script type="math/tex">\leq 2^h</script>, we need:</p>

<p><script type="math/tex">2^h \geq n!</script> and <script type="math/tex">h \geq \log n!</script></p>

<script type="math/tex; mode=display"> h \geq \sum_{i=1}^n \log i </script>

<p>so the lower bound is</p>

<script type="math/tex; mode=display"> \Theta(n \log n) </script>

<p>What about the expected case? We can convert trees to each other while retaining leaf number and depth will differ by only one. Average case is <script type="math/tex">\Theta(n \log n)</script>. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/01/26/eecs545/">
        Naive Bayes
      </a>
    </h1>

    <span class="post-date">26 Jan 2015</span>

    <h2 id="naive-bayes">Naive Bayes</h2>
<p>Classifier that assumes conditional independence and uses Bayes’ rule to make classifications. Remember Bayes’ rule:</p>

<script type="math/tex; mode=display"> p(C_k \vert \mathbf{x}) = \frac{p(C_k) \ p(\mathbf{x} \vert C_k)}{p(\mathbf{x})} </script>

<p>We’ll draw our distribution from a Bernoulli or Multinomial distribution. With independence assumptions, the conditional distribution over class C is:</p>

<script type="math/tex; mode=display"> p(C_k \vert x_1, \dots, x_n) = \frac{1}{Z} p(C_k) \prod_{i=1}^n p(x_i \vert C_k) </script>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/01/21/eecs586/">
        Sorting
      </a>
    </h1>

    <span class="post-date">21 Jan 2015</span>

    <h2 id="sorting-basics">Sorting Basics</h2>
<p>Sorting Assumptions:
* n items
* key (perhaps plus data)
* key type has {=,&lt;,&gt;} operations</p>

<p>Major Dichotomy:
* Internal, or all data in RAM
* External, or all data in ROM (IOPs dominate)</p>

<p>We’ll focus on RAM and not worry about IO. The book mentions selection/insertion sort (<script type="math/tex">\Theta(n^2)</script>) and merge sort (<script type="math/tex">\Theta(n \log n)</script>). Counting sort (section 8.2) is a stable sort with <script type="math/tex">\Theta(n+k)</script> time if keys are in k. </p>

<p><strong>Stable Sort</strong> - If key i and key j are the same, and <script type="math/tex">i \lt j</script>, then the final sorted order results in key i (and its data) occuring before key j (and its data).</p>

<h2 id="radix-sort-83">Radix sort (8.3)</h2>
<p>Keys are d digits base b. Sort digit by digit (stable). In many cases we’ll claim that radix is linear time (<script type="math/tex">\Theta(d(n+b))</script>) but it depends. If d and b are fixed it’s linear time. If we fix b, we want key space to be large enough so that n keys distinct. In this case we need to grow d, where <script type="math/tex"> d \gte \log_b n </script>.</p>

<h2 id="bucket-sort-84">Bucket sort (8.4)</h2>
<p>Keys are numbers in [0,1). Suppose uniform random IID keys. This assumption is big. We sort into buckets and then insertion sort through them. Worst case of this sort is <script type="math/tex">O(n^3)</script>, where all keys are in the same bucket. See handouts. </p>

<p>Expected time of bucket sort?</p>

<p>Book shows the proof using indicator variables. Let’s talk about a different proof. Expected bucket size is n. Time to sort bucket is constant. This isn’t always right though - the expected time of sorting the bucket isn’t equal to the sorting time of expected bucket size.</p>

<p>Why do I go to this class?</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page10">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page8">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
