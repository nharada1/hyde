<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs583/2015/09/28/eecs583/">
        Dynamic Binary Operations
      </a>
    </h1>

    <span class="post-date">28 Sep 2015</span>

    <h2 id="why-dynamic-optimization">Why Dynamic Optimization?</h2>
<p>Many times we don’t know characteristics about the program until it actually runs. Static profiling is great but if we can change the program as it runs we can take advantage of knowledge available only dynamically. Since we don’t have source code we must do it at the binary level.</p>

<p>What can we do? We can hoist instructions out of one block into another if execution order allows, based on which branch executes more often. We can also create superblocks to exploit instruction locality when certain branches are less common.</p>

<h2 id="how-to-profile">How To Profile</h2>
<p>We can either take edge or node profiles. While edge is more expensive it’s higher fidelity. How do we collect these profiles though? </p>

<ul>
  <li>Instrumentation Based - Software or hardware options. Software slows the program, while hardware is less supported.</li>
  <li>Sampling Based - Interrupt at random intervals and take sample. Slows the program less but requires more passes.</li>
</ul>

<h2 id="interpretation">Interpretation</h2>
<p>What is interpretation? We’ll start with JVM style interpretation. Jason Mars is very surprised nobody knows what this is.</p>

<p>The interpreter is essentially a giant switch statement on each OpCode of the bytecode. The interpreter basically just inputs JVM bytecodes and outputs native instructions. This is, as Jason says, “hella slow”. When the JVM sees a block of code that it suspects is going to be hot, it compiles it to try and get the speedup benefit. Even though the compilation may be slow, the JVM suspects that the benefit over time will be worth it. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs583/2015/09/21/eecs583/">
        Control Flow Analysis
      </a>
    </h1>

    <span class="post-date">21 Sep 2015</span>

    <h2 id="dominators-and-such">Dominators and Such</h2>
<p>Last time we talked about dominators and control flow graphs. But why do we care at all? One example is to detect loops. We can also use these constructs to optimize other parts of the program.</p>

<h3 id="loops">Loops</h3>
<ul>
  <li>A loop has a single entry point called the <strong>header</strong> which is a BB that dominates all other BB in the loop. </li>
  <li>There must be one way to iterate the loop, called a <strong>backedge</strong>.</li>
</ul>

<p>Loops are optimization targets because they run many times. To detect a loop we identify all backedges and merge loops with the same header. Remember, the header dominates all loop basic blocks.</p>

<p>In a loop the <strong>trip count</strong> is the number of times that the loop runs, generally we’ll use the average trip count.</p>

<h2 id="regions">Regions</h2>
<p>A region is a collection of operations that are treated as a single unit. This could be a basic block, a body of a loop, etc. Regions allow for essentially larger basic blocks. In this case we are optimizting frequently executed code instead of basic blocks. We want to optimize the 10% of code that takes up 90% of the time. Generally we want to find the most likely path through the code, or the best <strong>trace</strong>.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs583/2015/09/09/eecs583/">
        Introduction
      </a>
    </h1>

    <span class="post-date">09 Sep 2015</span>

    <h2 id="about-this-class">About this class</h2>
<p>This class will focus on basics for the first half (lectures) and papers during the second. </p>

<p>Books for this class:</p>

<ul>
  <li>Compilers by Aho, Lam, et. al.</li>
  <li>Adv Compiler Design and Implementation by Muchnick</li>
</ul>

<p>No midterm?</p>

<h2 id="architectures">Architectures</h2>
<p>VLIW (Very Long Instruction Word) basically combines many ops into one instruction, allowing fast execution. Like SIMD but more. Not very effective so far though. Still valid in the microcontroller domain.</p>

<p>Multicore is what we all know and may or may not love. Sequential programs only use one core. How do we convert sequential to multiple? We haven’t solved it.</p>

<p>SIMD (Single Instruction Multiple Data) encompasses GPU, DSP, etc. Good for math. </p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/04/20/eecs586/">
        
      </a>
    </h1>

    <span class="post-date">20 Apr 2015</span>

    <h2 id="exam">Exam</h2>
<p>No books, no electronics. Can bring one 8.5x11 sheet of paper with anything written on two sides. Will post last year’s final.</p>

<h2 id="homework">Homework</h2>

<ol>
  <li>Subset sum reduces. If bounded by B there are BE possibilities which is polynomial. Apparently?</li>
  <li>Ham-Path to One-Longer can be done by adding a path p in addition to a Hamiltonian cycle, which makes it easy. Basically we just ignore path p.</li>
  <li>Reduces from independent set to nearly independent set. Adds additional vertices that allow us to sum to the weight properly.</li>
  <li>Same as ours. There is a divide and conquer method for this where were remove larger sections at once. We can get $\Theta(N*m \log m)$.</li>
</ol>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/04/20/eecs545/">
        RL Review
      </a>
    </h1>

    <span class="post-date">20 Apr 2015</span>

    <h2 id="review">Review</h2>
<p>Supervised Learning Problems:</p>

<ul>
  <li>Generalizability</li>
</ul>

<p>Reinforcement Learning Problems:</p>

<ul>
  <li>Exploration-exploitation problem</li>
  <li>Credit assignment/delayed reward</li>
</ul>

<h3 id="bandit-problem">Bandit Problem</h3>
<p>Bandit problem suffers from exploration-exploitation problem. At the limit, you’ll be picking the optimal arm with 1-e probability. You are <em>not</em> picking optimally because at the limit you can still choose randomly. If you want to always pick the optimal arm you need to decrease epsilon to zero at the appropriate rate. The harmonic series works for this. </p>

<p>Sum of epsilon should be infinity, sum of epsilon squared should be finite.</p>

<p>Contextual bandits add the problem of generalization to the problem of exploration-exploitation. In this case, we get a context vector at each time and have to pick one of the arms. In this case we need a model by making the assumption that each arm has a theta vector (regression problem) that is time invariant. </p>

<script type="math/tex; mode=display"> x^T \Theta_i = E[R_i \mid x] </script>

<h3 id="ucb">UCB</h3>
<p>At least know what this <strong>is</strong> because we did cover it.</p>

<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>First we say that there are transition probabilities given the past actions and observations. Then we make the Markov assumption, which (first order) says that we only rely on the previous time step. When we are in a Markov setting we replace the O (observation) with S (state space). The problem is to find a policy that maps from histories to actions. We want to maximize the expected value of the discounted sum of rewards over time. </p>

<p>** Exam Question: ** What happens in a k-th order Markov process?</p>

<p>Let’s talk discount factors. If we have a gamma of one, we have a problem where an infinite horizon occurs and then our expected reward can diverge or become infinite. If we have an <em>absorbing state</em> then we can use a gamma of one, because that represents an <em>episode</em> where thing eventually end. For example, in chess there is an end and after that there are no more moves possible. In many real world problems you will use gamma of one and have an episodic problem.</p>

<ul>
  <li>Fixed finite horizon: You have n steps to act</li>
  <li>Episodic: Act forever, but stop if you hit a certain state</li>
</ul>

<h3 id="policy-evaluation-problem">Policy Evaluation Problem</h3>
<p>We’re given a policy, now evaluate its quality. We discussed three algorithms. Planning setting is matrix inversion and iteration, learning is temporal difference. Monte Carlo can be applied to the learning setting, but is generally planning. Planning means we have reward function and transition probabilities, in learning we have to act in the world to gain information:</p>

<p>** Exam Question: ** How does the proof for convergence work?</p>

<ul>
  <li>Monte Carlo - $O(n)$. Start in state i, we get a trajectory, and eventually we stop. We average the rewards we get for many trajectories. This algorithm is model based because we are required to come back to state i, which we can’t always do in the real world.</li>
  <li>Matrix Inversion - $O(n^3)$. Allows us to exactly solve the problem in one step, but takes a long time depending on size of the state space.</li>
  <li>Iteration - $O(n^2)$. Iterative method to solve matrix inversion indirectly. Faster per iteration but requires many steps.</li>
  <li>Temporal Difference (indirect) - Estimate our values by acting and observing and then estimating the expected reward at each state. We look ahead a certain amount.</li>
</ul>

<h3 id="optimal-control">Optimal Control</h3>
<p>We want to find the best policy for an MDP.</p>

<ul>
  <li>Value Iteration - Use the Bellman optimality equation. We replace the iterating equation in policy evaluation with a similar function but instead we take the max action value. Note that Q-value iteration is similar here, but that the max is inside the expectation which allows us to get unbiased samples. In Q-learning, we can prove that we will converge to the optimal policy as long as we decrease alpha similar to epsilon in the bandit problem. Similarly, epsilon greedy gets infinite exploration, and like bandits does not get optimal control without decreasing epsilon.</li>
  <li>Policy Iteration - Two step process, we calculate the policy, then improve the policy iteratively. This converges much more quickly than value iteration and has definitive stopping conditions</li>
  <li>SARSA - Similar to Q-learning, but is on-policy, meaning that we must behave in a special way for convergence. This allows us to converge faster if we behave correctly. In order to converge to the optimal value we have to be greedy in the limit. Greedily in the limit with infinite exploration (GLIE). Note that SARSA has no max at all, because we evaluate based on what we are doing at the time.</li>
</ul>

<h3 id="depth-d-planningreceeding-horizon-control">Depth D Planning/Receeding Horizon Control</h3>
<p>Build out a tree of actions of depth D, and do backwards induction to determine the best option. Basically we plan ahead for D steps and take an action, then the horizon receeds, and we repeat (we still plan ahead D steps). The horizon is exponential, but we can also do a sampling based version which allows us to compute this much more quickly. When we do this we lose dependence on the size of the state space (but we are still exponential on size of horizon).</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page3">Older</a>
  
  
    
      <a class="pagination-item newer" href="/">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
