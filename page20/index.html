<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs598/2014/09/25/eecs598/">
        Data Analysis I
      </a>
    </h1>

    <span class="post-date">25 Sep 2014</span>

    <h2 id="classification">Classification</h2>
<p>Predicting outcomes</p>

<h3 id="nearest-neighbors">Nearest Neighbors</h3>
<p>Simple model, look at what happened to similar data. We can use a few metrics to determine “distance”, but in the end it’s all the same idea. Note we should normalize the data because otherwise large distances screw with us.</p>

<h3 id="decision-trees">Decision Trees</h3>
<p>Another basic model, attempts to split data based on features. We use entropy to quantify the amount of disorder in a set, aka the amount of information required to represent a node. The feature space of the decision tree is partitions aligned with the axis, which is hard for diagonals or etc.</p>

<h3 id="bayesian-learning">Bayesian Learning</h3>
<p>We want to represent things probabilistically and choose our hypothesis with the max probability. We’ll use Baye’s rule to determine probabilities of each possible classification. Because we decompose into separate P’s, we make a big assumption about independence, but this may not be true. On the other hand, we actually get a probability out.</p>

<h3 id="svm-support-vector-machine">SVM (Support Vector Machine)</h3>
<p>We want to separate the data in the <em>best way</em> possible, aka provide the maximum separating hyperplane. We find the “support vectors” that support the hyperplane, we only need these points to classify. We can add in a kernel function to extrapolate to higher dimensions. The big SVM advantage is that you avoid overfitting my maximizing the hyperplane.</p>

<h2 id="regression">Regression</h2>
<p>Predicting values</p>

<h3 id="logistic-regression">Logistic Regression</h3>
<p>Good for when there’s a range over which things matter. At the extremes things don’t matter as much. This is good for many medical applications. We’ll solve this via maximum likelihood estimation. We want to find \beta values for our probability function.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/25/eecs584/">
        Paper Review - H-Store&#58 A High-Performance, Distributed Main Memory Transaction Processing System
      </a>
    </h1>

    <span class="post-date">25 Sep 2014</span>

    <p>In this demo paper from 2008, the authors demonstrate H-Store, a shared nothing main memory database system. In their previous paper on OTLP and main memory databases, the authors lay the groundwork for this database and describe some of the problems of traditional DMBSes and potential new architectures to be considered. Specifically, the authors argue that extending traditional structures such as locks and buffer managers to main memory databases is detrimental, and that an entirely new architecture could be constructed to better take advantage of the capabilities inherent in an MMDB. </p>

<p>In H-Store, the authors completely eliminate locking and latching by distributing the filesystem throughout multiple sites, and storing disjoint sets of data among them. Because each H-Store site is single threaded and operates on its own memory, the system is able to remove locks entirely while maintaining parallelism via multiple sites per CPU and multiple CPUs per node. This architecture also allows H-Store to easily redistribute processing load across the cluster - incoming transactions are handled at each site and then, if possible, rerouted to their proper sites.</p>

<p>Another important architecture decision of H-Store was the separation of deployment and runtime. During system deployment, the database administrator must predefine transactions in the form of <em>stored procedures</em>. By precompiling transactions, H-Store is able to improve performance (by allowing direct access to memory), as well as eliminate locking (by reasoning about data layout at dispatch time). In addition, after construction, the database is optimized to the parameters at deployment and will not fare well when adding new procedures (although this is possible). This separation of deploy and runtime is a point of strength and weakness in H-Store, and allows the database to trade performance for flexibility. </p>

<p>H-Store uses redundancy to operate without any disk storage at all. This presents a potential problem that goes un-addressed in this paper - what happens if power fails to a data center. The way H-Store is described here, all data will be permanently lost because there is no disk storage at all. Redundancy assumes individual sites or nodes may fail, but that in the end there will be enough of the cluster up to recover the lost data. For a production system, this would be catastrophic, and a real-world deployment of H-Store would certainly require checkpointing or snapshotting to disk. Another weakness of this system is its deploy-time capability. Not only is everything required at deploy time, but there is no automatic database layout design, which leaves work to the administrator. This is a weakness of the system, although the authors do acknowledge that the limitation exists and there is active work to remedy the problem.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/25/eecs492/">
        Constraint Satisfaction
      </a>
    </h1>

    <span class="post-date">25 Sep 2014</span>

    <p>How do we introduce constraints to solve problems where the variables are linked. A few definitions:</p>

<ul>
  <li><script type="math/tex">X</script> - a set of variables <script type="math/tex">\lbrace X_1, ..., X_n \rbrace</script></li>
  <li><script type="math/tex">D</script> - set of domains <script type="math/tex">\lbrace D_1, ..., D_n \rbrace</script></li>
  <li><script type="math/tex">C</script> - a set of constraints</li>
</ul>

<p>A few more definitions in a constrained search problem, describes how variables are assigned.</p>

<ul>
  <li>Consistent assignment - violates no constraints</li>
  <li>Partial assignment - assigns values to some variables</li>
  <li>Complete assignment - assigns values to all variables</li>
</ul>

<p>We can create a constraint graph that represents variables and constraints. Once we introduce constraints, we vastly decrease the search space. <strong>Constraints describe relationships</strong>.</p>

<h3 id="types-of-constraints">Types of constraints</h3>

<ul>
  <li>Precedence constraints - things must be done in order</li>
  <li>Disjunctive constraints - limited resources affect order</li>
  <li>Time constraints - domain of others shrinks</li>
</ul>

<h3 id="consistency">Consistency</h3>

<ul>
  <li>Node consistency - All values in domain satisfy unary constraints</li>
  <li>Arc consistency - Consistency between the domain and range</li>
  <li>Path consistency - Uses implicit constraints inferred by triples of variables</li>
  <li>K-consistency - Generalization of consistency with more relationships</li>
</ul>

<p>In the case of multiple constraints, we can propagate constraints throughout the network. For arc consistency, we can use AC3. Path consistency lacks enforcement of unary constraints, and for this reason it’s <strong>not</strong> arc consistent. </p>

<h3 id="solving-csps">Solving CSPs</h3>
<p>The simplest approach is uninformed search, but the search space is huge. Additionally, ordering doesn’t matter. We need to combine search and consistency, we’ll use <em>backtracking</em> to use search once we’ve reduced the space. The key is that consistency depends only on assigned variables, eg order doesn’t matter. At a basic level this is like depth first search. In backtracking we use a heuristic to choose what to expand first (minimum remaining value).  </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/24/eecs584/">
        Paper Review - Main Memory Database Systems - An Overview
      </a>
    </h1>

    <span class="post-date">24 Sep 2014</span>

    <p>Main memory database systems offer notable benefits for small databases such as online transaction processing systems. These benefits include faster queries, higher throughput, and simplified system architecture. Written in 1992, this paper has powerful and forward looking insights about the architecture decisions that need to go into a main memory database system. This paper is ahead of its time, as many of the concepts could not be applied for years after publication - main memory was falling in price but fitting large customer databases in memory for a reasonable price was still almost a decade away. Indeed, it wasn’t until recently that modern juggernauts such as Microsoft SQL and Oracle started supporting memory resident systems. This paper also correctly predicted non-volatile main memory years before it had come to market.</p>

<p>This paper acts almost as a survey to main memory database systems, and looks at each part of a traditional DMBS and how it relates to a MMDB. The recurring observation made by the authors is that the disk is no longer the bottleneck of the system. This has several implications. First, contention on disk is almost eliminated, and as a result we can use larger lock granule sizes. Additionally, because main memory no longer favors sequential access, we can utilize different index structures and almost completely remove buffer management. Although beneficial, MMDBs do present some problems. Most critically, because memory is volatile, there must exist some persistent storage in case of power failure or crashes. Logging can be done in group commits to a disk, or using some sort of “stable main memory”, which was a hypothetical concept when this paper was published. The database also flushes the entire system to disk occasionally, in the form of snapshots or checkpoints. The problem of how to store data on disk is also briefly mentioned, although the author’s main point here is simply that there is no analogue to migration in a traditional DBMS. In the last section of this paper, the authors examine several main memory database systems available at the time of writing. However, being such an old paper (in terms of technology), much of this section is outdated.</p>

<p>This paper is quite ahead of its time, but it still has a few weaknesses. First, some of the technology discussed did not exist in 1992 at the time of publication. Non-volatile memory is just now coming out. While this is okay from a theoretical perspective, the paper doesn’t point out that this technology is undeveloped. I am a bit surprised that the authors didn’t foresee the use of shared nothing systems in this paper - the idea of shared nothing wasn’t unheard of at the time of writing and the authors likely were aware that extremely high density main memory would be expensive, even in the future. Finally, the authors advocate T-trees for memory resident database systems, but research has shown that T-trees may not actually offer noticeable benefits to a DBMS. The authors were likely simply using the results of T-tree research.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/24/eecs492/">
        Search Implementation
      </a>
    </h1>

    <span class="post-date">24 Sep 2014</span>

    <h2 id="generic-best-first">Generic Best First</h2>
<p>Start with a queue, pop the most promising node according to f. If n is a goal node, return it otherwise add successors. The choice of f changes the search algorithm.</p>

<p>We’ll say each node has a few attributes:</p>

<ul>
  <li>Depth</li>
  <li>g - Actual path cost from root to here</li>
  <li>h - Estimated cost from this node to goal</li>
</ul>

<p>So how to do choose f?</p>

<ul>
  <li>BFS: f = depth</li>
  <li>DFS: f = 0-depth</li>
  <li>UCS: f = g</li>
  <li>GBFS: f = h</li>
  <li>A-star: f = g+h</li>
</ul>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page21">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page19">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
