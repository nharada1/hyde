<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/03/09/eecs586/">
        Negative Edges in Search
      </a>
    </h1>

    <span class="post-date">09 Mar 2015</span>

    <p>Negative edges are less common, but in Dijkstra’s algorithm we won’t be able to keep track of the best possible choice at each step (because edges can be negative and thus actually improve our score moving outwards). How do we fix it? We can use Bellman-Ford to search instead. This algorithm relaxes <strong>all</strong> edges, not just the best looking ones. Unfortunately this can result in an infinite running time if we have a negative cycle. We’ll need to keep track of these negative cycles or we’re screwed.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/03/09/eecs545/">
        Expectation Maximization
      </a>
    </h1>

    <span class="post-date">09 Mar 2015</span>

    <h2 id="learning-in-bayes-nets">Learning in Bayes Nets</h2>
<p>If we know the structure, there are two methods of learning: full observation (maximum likelihood) and partial observation (EM). </p>

<h2 id="maximum-likelihood-for-bayes-nets">Maximum Likelihood for Bayes Nets</h2>
<p>Given a simple Bayes net X-&gt;Y, what are the parameters?</p>

<ul>
  <li>
    <script type="math/tex; mode=display">P(X)</script>
  </li>
  <li>
    <script type="math/tex; mode=display">P(Y \mid X)</script>
  </li>
</ul>

<p>We can take derivatives, set them to zero, and find the maximum likelihood. In Bayesian nets, the likelihood term decomposes with respect to local conditional probability tables. If we know all of the values of each parameter, it’s very easy to find the correct values. The complexity depends on the size of the parent sets, which is good for scaling as long as parent sets aren’t massive. This is equivalent to finding weights in linear regression. </p>

<p>In a Markov network, have a probability distribution and a structure. We want to take derivative with respect to the parameters. We have representation of each clique. The solution isn’t closed form, but we can use derivatives for an interative approach. </p>

<h2 id="expectation-maximization">Expectation Maximization</h2>
<p>For parameter learning when the data is not fully observed. Realistically, this is what we’re going to get. Suppose we have a set of variables X and the remaining set of variables are hidden Z. We treat Q=P(Z|X) and then we update the observations by treating Q as an observation. </p>

<p>We’ll start with K-means: K-means is iterative and we try to minimize distortion measure J. The K-Means algorithm can be viewed as an EM sequence! In the E step we assign each point to its closest center. In the M step we update the centers based on distance. </p>

<p>Mixture of Gaussians allows soft clustering, where K means requires that each point is in one cluster. In the GMM, each point is generated by a Gaussian, but they overlap so we don’t know for sure which point comes from where. E step is to compute the probability that point was generated by Gaussian K. The M step is that given the responsibilities of each Gaussian, we reevaluate the coefficients. Derivation of EM is available online, but suggest <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Stanford 229 notes</a>. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/02/25/eecs586/">
        TITLE
      </a>
    </h1>

    <span class="post-date">25 Feb 2015</span>

    <h2 id="homework-review">Homework Review</h2>
<p>We can do problem 3 in <script type="math/tex">\Theta(cn \log n)</script> time instead of <script type="math/tex">\Theta(cn^2)</script>. </p>

<h2 id="shortest-paths">Shortest paths</h2>
<p>We have a graph with weights on each edge (directed). Find the shortest path from u to v (sum of weights). Many variations of this problem:</p>

<ul>
  <li>Go from single source to single destination</li>
  <li>Single source to all destinations</li>
  <li>All sources to one destination</li>
  <li>All sources all destinations</li>
</ul>

<p>The middle two are the same problem. You’d expect single single to be easier but nobody has a better worst case for any of the first three. This is chapter 24/25.</p>

<p>The solution is a rooted tree where everyone points to their parent. This is similar to Prim’s algorithm. Prim’s algorithm picks the edge of minimal weight but this doesn’t work for us. For each vertex not in the tree, keep track of the shortest path to get to it so far. We instead pick the edge with the minimum distance to the endpoint. </p>

<script type="math/tex; mode=display"> d(u) = \min(d(u), d(v)+w(e)) </script>

<p>This is Dijkstra’s algorithm. Time complexity of Prim is the same as this one. Single shortest path is easy, but what if we want a parallel version of this.</p>

<p>All to all distance? We could repeat the algorithm over and over, but we’re going to throw away a ton of stuff by doing that. Make a weighted adjacency matrix W, if no edge weight is infinity. Interpret this as the shortest path using one edge. </p>

<script type="math/tex; mode=display"> W_2(i,j) = \min \{ \min\limits_{k=1..n}W(i,k) + W(k,j), W(i,j) \}</script>

<p>This is the shortest path of two or fewer steps. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/02/25/eecs545/">
        More Graphical Models
      </a>
    </h1>

    <span class="post-date">25 Feb 2015</span>

    <p>A bayesian network cannot always be represented by a Markov network. Every joint distribution can be represented by a directed OR undirected graphical model. </p>

<h2 id="inference-in-graphical-models">Inference in Graphical Models</h2>
<p>We can marginalize probabilities to do inference in graphical models. Remember from AI, we want to move around summations by using the graphical structure to minimize calculation. </p>

<h2 id="factor-graphs">Factor graphs</h2>
<p>The factor graph is the combination of various “factors” that let us represent a DAG in a more compact way. It’s not unique, but allows us to do inference by representing probabilities in a less parametric form. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/02/23/eecs586/">
        Dynamic Programming Part 2
      </a>
    </h1>

    <span class="post-date">23 Feb 2015</span>

    <h2 id="minimal-spanning-tree">Minimal Spanning Tree</h2>
<p>Trying to show that starting with the antiset and adding safe edges is correct. This is proving that a greedy algorithm is optimal. Suppose T is a MST that contains A. In T there is a path between A and B. We want to pick an f that is on the path. We pick a greedy edge, so weight(f) &gt;= weight(e). Now we remove f and add e. We are trying to make the greedy algorithm look like the optimal one. This is in the book.</p>

<p>Prim: Grow a tree by adding in new edges over and over.
Kruskal: Grow a forest by adding together subsets of forests.</p>

<p>What are running times? We need to ensure we aren’t creating cycles. Time is <script type="math/tex">\Theta(E \log V)</script>. For Kruskal we want the minimum weight among all edges that doesn’t create a cycle. Chapter 21 shows a data structure for this.</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page7">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page5">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
