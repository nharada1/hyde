<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Notes</title>
 <link href="/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2015-11-11T12:07:11-05:00</updated>
 <id></id>
 <author>
   <name>Nate Harada</name>
   <email></email>
 </author>

 
 <entry>
   <title>Classic Optimization</title>
   <link href="/eecs583/2015/11/11/"/>
   <updated>2015-11-11T00:00:00-05:00</updated>
   <id>/eecs583/2015/11/</id>
   <content type="html">&lt;h2 id=&quot;standard-optimizations&quot;&gt;Standard Optimizations&lt;/h2&gt;
&lt;p&gt;Many optimizations are classic and have been long implemented by compilers&lt;/p&gt;

&lt;h3 id=&quot;removing-unreachable-code&quot;&gt;Removing Unreachable Code&lt;/h3&gt;
&lt;p&gt;Many times there is code that can never run. Obviously this should be removed. It turns out that any code that doesn’t influence a store or a branch is removable. This is a simple way to say what should go, but many things influence these two, even indirectly. For example, drawing to the screen is a store. &lt;/p&gt;

&lt;h3 id=&quot;constant-propagation&quot;&gt;Constant Propagation&lt;/h3&gt;
&lt;p&gt;We can propagate local constants forward by subbing constants wherever possible instead of registered. We can also use global constant propagation to do this for the whole program. &lt;/p&gt;

&lt;h3 id=&quot;constant-folding&quot;&gt;Constant Folding&lt;/h3&gt;
&lt;p&gt;If there is an expression of constants with all literals, we want to do this at compile time. However, there are exceptions! For example, what if the constant expression is impossible. If you want to do &lt;code&gt;3 / 0&lt;/code&gt; you need to leave it so that the program fails correctly. You also don’t want to mess with floating point, because the floating point epsilon error is so dependent on the architecture. &lt;/p&gt;

&lt;h3 id=&quot;forward-copy-propagation&quot;&gt;Forward Copy Propagation&lt;/h3&gt;
&lt;p&gt;We want to forward propagate the right hand sides of moves to eliminate dependencies and eliminate the move. &lt;/p&gt;

&lt;h3 id=&quot;common-subexpression-elimination&quot;&gt;Common Subexpression Elimination&lt;/h3&gt;
&lt;p&gt;We want to save expressions we’ve already evaluated if they’re used again. For example, if a smaller subexpression is used many times we can save a lot of time by evaluating once only. &lt;/p&gt;

&lt;h3 id=&quot;loop-invariant-code-motion&quot;&gt;Loop Invariant Code Motion&lt;/h3&gt;
&lt;p&gt;Any instructions that don’t change during the loop can be hoisted out of the loop and calculated once instead of each time. We did this in homework.&lt;/p&gt;

&lt;h3 id=&quot;global-variable-migration&quot;&gt;Global Variable Migration&lt;/h3&gt;
&lt;p&gt;Instead of doing a load and a store in a loop repeatedly, we can load and store into a register and then use that register repeatedly in the loop instead. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pin Paper</title>
   <link href="/eecs583/2015/10/14/eecs583/"/>
   <updated>2015-10-14T00:00:00-04:00</updated>
   <id>/eecs583/2015/10/14/eecs583</id>
   <content type="html">&lt;h2 id=&quot;why-pin-is-interesting&quot;&gt;Why Pin is interesting&lt;/h2&gt;
&lt;p&gt;Pin is essentially a good API version of Dynamo. While Pin is different, for example closed source, it is mostly the same. It does not have a threshold to form traces like Dynamo. It also used to not support AMD processors. &lt;/p&gt;

&lt;p&gt;Pin’s JIT goes from one ISA to the same, there’s no IR representation. Only code in the code cache is executed. It’s also important for Pin to change the branch targets of code in the code cache so that Pin doesn’t lose control of the program.&lt;/p&gt;

&lt;h2 id=&quot;looking-inside-basic-blocks&quot;&gt;Looking Inside Basic Blocks&lt;/h2&gt;
&lt;p&gt;Things become more complicated when we stop looking at basic blocks like black boxes. We can eliminate instructions, for example, when code is never used (is not live). &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Reviews 1 and 2</title>
   <link href="/eecs583/2015/10/12/"/>
   <updated>2015-10-12T00:00:00-04:00</updated>
   <id>/eecs583/2015/10/</id>
   <content type="html">&lt;h2 id=&quot;dynamo-paper-review&quot;&gt;Dynamo Paper Review&lt;/h2&gt;
&lt;p&gt;With the rise of object oriented programming and DLLs, it became less beneficial to use static optimization. What were the goals of the Dynamo program?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No user/compiler dependency&lt;/li&gt;
  &lt;li&gt;No offline analysis&lt;/li&gt;
  &lt;li&gt;Machine independent&lt;/li&gt;
  &lt;li&gt;Transparent &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dynamo works by identifying hot traces and optimizing those fragments. These fragments are stored in the fragment cache. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>TITLE</title>
   <link href="/eecs583/2015/10/07/eecs583/"/>
   <updated>2015-10-07T00:00:00-04:00</updated>
   <id>/eecs583/2015/10/07/eecs583</id>
   <content type="html">&lt;h2 id=&quot;homework-two-overview&quot;&gt;Homework Two Overview&lt;/h2&gt;
&lt;p&gt;In a loop invariant we can move other parts of the code outside of the loop if they don’t depend on the internal registers. This saves execution because we’ve already stored the value. How to move code? Either move the instructions to the previous block or add a block in between the two blocks. Loop Invariant Code Motion (LICM) moves operations whose source operands don’t change to pre-header. &lt;/p&gt;

&lt;p&gt;Reach: Which instructions can reach at a certain place
Available: Which instructions are guaranteed to be a certain value at a location&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>TITLE</title>
   <link href="/eecs583/2015/09/30/eecs583/"/>
   <updated>2015-09-30T00:00:00-04:00</updated>
   <id>/eecs583/2015/09/30/eecs583</id>
   <content type="html">&lt;h2 id=&quot;logistic-notes&quot;&gt;Logistic Notes&lt;/h2&gt;
&lt;p&gt;Homework two is due Oct 23rd, and will be released on Monday. This homework is much harder than number one. Paper reviews are due Monday before class and the reviews are 5\% of the presentation grade.&lt;/p&gt;

&lt;h2 id=&quot;protean-code&quot;&gt;Protean Code&lt;/h2&gt;
&lt;p&gt;Why should we revisit dynamic compilation? It was dead for a solid 6 years for native binaries. The motivating context here was datacenters. In a cluster environment there is heavy dynamic change. What do we want to fix? &lt;/p&gt;

&lt;p&gt;For example, we can module low priority processes on a server to make room for higher priority processes on the same chip. The typical approach is that we just throttle with napping/sleeping. Insight: the instruction set architecture contains tools that can be leveraged to avoid sleeping (e.g. we can tell an instruction to not access cache NTA hint). By not caching the process we want to throttle, we avoid cache contention at the expense of performance for the slowed application.&lt;/p&gt;

&lt;h2 id=&quot;deploying-dynamic-compilation&quot;&gt;Deploying Dynamic Compilation&lt;/h2&gt;
&lt;p&gt;In conventional binary rewriting you have to retain fine grain control at the BB level. Execution &lt;strong&gt;must&lt;/strong&gt; come from code in the code cache. Protean code is able to not halt execution while the code is recompiled. Protean is a &lt;strong&gt;compiler&lt;/strong&gt; approach which expands the design space for optimization. Weakness: You cannot use an already compiled binary!&lt;/p&gt;

&lt;p&gt;During compilation we convert jmp to indirect jmp (which does not actually incur a performance hit). Thus, when you have a runtime compiler, we can rewrite the function pointers to point at a different version. N.B. This runs at a function level. This allows us to optimize functions on the fly while the program is actually running. To properly allow the datacenter to utilize the protean code, we want to select program variants with the right mix of NT cache hint instructions. We want to eliminate or minimize the use of naps/sleeps. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dynamic Binary Operations</title>
   <link href="/eecs583/2015/09/28/eecs583/"/>
   <updated>2015-09-28T00:00:00-04:00</updated>
   <id>/eecs583/2015/09/28/eecs583</id>
   <content type="html">&lt;h2 id=&quot;why-dynamic-optimization&quot;&gt;Why Dynamic Optimization?&lt;/h2&gt;
&lt;p&gt;Many times we don’t know characteristics about the program until it actually runs. Static profiling is great but if we can change the program as it runs we can take advantage of knowledge available only dynamically. Since we don’t have source code we must do it at the binary level.&lt;/p&gt;

&lt;p&gt;What can we do? We can hoist instructions out of one block into another if execution order allows, based on which branch executes more often. We can also create superblocks to exploit instruction locality when certain branches are less common.&lt;/p&gt;

&lt;h2 id=&quot;how-to-profile&quot;&gt;How To Profile&lt;/h2&gt;
&lt;p&gt;We can either take edge or node profiles. While edge is more expensive it’s higher fidelity. How do we collect these profiles though? &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Instrumentation Based - Software or hardware options. Software slows the program, while hardware is less supported.&lt;/li&gt;
  &lt;li&gt;Sampling Based - Interrupt at random intervals and take sample. Slows the program less but requires more passes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;interpretation&quot;&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;What is interpretation? We’ll start with JVM style interpretation. Jason Mars is very surprised nobody knows what this is.&lt;/p&gt;

&lt;p&gt;The interpreter is essentially a giant switch statement on each OpCode of the bytecode. The interpreter basically just inputs JVM bytecodes and outputs native instructions. This is, as Jason says, “hella slow”. When the JVM sees a block of code that it suspects is going to be hot, it compiles it to try and get the speedup benefit. Even though the compilation may be slow, the JVM suspects that the benefit over time will be worth it. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Control Flow Analysis</title>
   <link href="/eecs583/2015/09/21/eecs583/"/>
   <updated>2015-09-21T00:00:00-04:00</updated>
   <id>/eecs583/2015/09/21/eecs583</id>
   <content type="html">&lt;h2 id=&quot;dominators-and-such&quot;&gt;Dominators and Such&lt;/h2&gt;
&lt;p&gt;Last time we talked about dominators and control flow graphs. But why do we care at all? One example is to detect loops. We can also use these constructs to optimize other parts of the program.&lt;/p&gt;

&lt;h3 id=&quot;loops&quot;&gt;Loops&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A loop has a single entry point called the &lt;strong&gt;header&lt;/strong&gt; which is a BB that dominates all other BB in the loop. &lt;/li&gt;
  &lt;li&gt;There must be one way to iterate the loop, called a &lt;strong&gt;backedge&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Loops are optimization targets because they run many times. To detect a loop we identify all backedges and merge loops with the same header. Remember, the header dominates all loop basic blocks.&lt;/p&gt;

&lt;p&gt;In a loop the &lt;strong&gt;trip count&lt;/strong&gt; is the number of times that the loop runs, generally we’ll use the average trip count.&lt;/p&gt;

&lt;h2 id=&quot;regions&quot;&gt;Regions&lt;/h2&gt;
&lt;p&gt;A region is a collection of operations that are treated as a single unit. This could be a basic block, a body of a loop, etc. Regions allow for essentially larger basic blocks. In this case we are optimizting frequently executed code instead of basic blocks. We want to optimize the 10% of code that takes up 90% of the time. Generally we want to find the most likely path through the code, or the best &lt;strong&gt;trace&lt;/strong&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs583/2015/09/09/eecs583/"/>
   <updated>2015-09-09T00:00:00-04:00</updated>
   <id>/eecs583/2015/09/09/eecs583</id>
   <content type="html">&lt;h2 id=&quot;about-this-class&quot;&gt;About this class&lt;/h2&gt;
&lt;p&gt;This class will focus on basics for the first half (lectures) and papers during the second. &lt;/p&gt;

&lt;p&gt;Books for this class:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Compilers by Aho, Lam, et. al.&lt;/li&gt;
  &lt;li&gt;Adv Compiler Design and Implementation by Muchnick&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No midterm?&lt;/p&gt;

&lt;h2 id=&quot;architectures&quot;&gt;Architectures&lt;/h2&gt;
&lt;p&gt;VLIW (Very Long Instruction Word) basically combines many ops into one instruction, allowing fast execution. Like SIMD but more. Not very effective so far though. Still valid in the microcontroller domain.&lt;/p&gt;

&lt;p&gt;Multicore is what we all know and may or may not love. Sequential programs only use one core. How do we convert sequential to multiple? We haven’t solved it.&lt;/p&gt;

&lt;p&gt;SIMD (Single Instruction Multiple Data) encompasses GPU, DSP, etc. Good for math. &lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="/eecs586/2015/04/20/eecs586/"/>
   <updated>2015-04-20T00:00:00-04:00</updated>
   <id>/eecs586/2015/04/20/eecs586</id>
   <content type="html">&lt;h2 id=&quot;exam&quot;&gt;Exam&lt;/h2&gt;
&lt;p&gt;No books, no electronics. Can bring one 8.5x11 sheet of paper with anything written on two sides. Will post last year’s final.&lt;/p&gt;

&lt;h2 id=&quot;homework&quot;&gt;Homework&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Subset sum reduces. If bounded by B there are BE possibilities which is polynomial. Apparently?&lt;/li&gt;
  &lt;li&gt;Ham-Path to One-Longer can be done by adding a path p in addition to a Hamiltonian cycle, which makes it easy. Basically we just ignore path p.&lt;/li&gt;
  &lt;li&gt;Reduces from independent set to nearly independent set. Adds additional vertices that allow us to sum to the weight properly.&lt;/li&gt;
  &lt;li&gt;Same as ours. There is a divide and conquer method for this where were remove larger sections at once. We can get $\Theta(N*m \log m)$.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>RL Review</title>
   <link href="/eecs545/2015/04/20/eecs545/"/>
   <updated>2015-04-20T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/20/eecs545</id>
   <content type="html">&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;p&gt;Supervised Learning Problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generalizability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reinforcement Learning Problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exploration-exploitation problem&lt;/li&gt;
  &lt;li&gt;Credit assignment/delayed reward&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bandit-problem&quot;&gt;Bandit Problem&lt;/h3&gt;
&lt;p&gt;Bandit problem suffers from exploration-exploitation problem. At the limit, you’ll be picking the optimal arm with 1-e probability. You are &lt;em&gt;not&lt;/em&gt; picking optimally because at the limit you can still choose randomly. If you want to always pick the optimal arm you need to decrease epsilon to zero at the appropriate rate. The harmonic series works for this. &lt;/p&gt;

&lt;p&gt;Sum of epsilon should be infinity, sum of epsilon squared should be finite.&lt;/p&gt;

&lt;p&gt;Contextual bandits add the problem of generalization to the problem of exploration-exploitation. In this case, we get a context vector at each time and have to pick one of the arms. In this case we need a model by making the assumption that each arm has a theta vector (regression problem) that is time invariant. &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; x^T \Theta_i = E[R_i \mid x] &lt;/script&gt;

&lt;h3 id=&quot;ucb&quot;&gt;UCB&lt;/h3&gt;
&lt;p&gt;At least know what this &lt;strong&gt;is&lt;/strong&gt; because we did cover it.&lt;/p&gt;

&lt;h3 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h3&gt;
&lt;p&gt;First we say that there are transition probabilities given the past actions and observations. Then we make the Markov assumption, which (first order) says that we only rely on the previous time step. When we are in a Markov setting we replace the O (observation) with S (state space). The problem is to find a policy that maps from histories to actions. We want to maximize the expected value of the discounted sum of rewards over time. &lt;/p&gt;

&lt;p&gt;** Exam Question: ** What happens in a k-th order Markov process?&lt;/p&gt;

&lt;p&gt;Let’s talk discount factors. If we have a gamma of one, we have a problem where an infinite horizon occurs and then our expected reward can diverge or become infinite. If we have an &lt;em&gt;absorbing state&lt;/em&gt; then we can use a gamma of one, because that represents an &lt;em&gt;episode&lt;/em&gt; where thing eventually end. For example, in chess there is an end and after that there are no more moves possible. In many real world problems you will use gamma of one and have an episodic problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fixed finite horizon: You have n steps to act&lt;/li&gt;
  &lt;li&gt;Episodic: Act forever, but stop if you hit a certain state&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;policy-evaluation-problem&quot;&gt;Policy Evaluation Problem&lt;/h3&gt;
&lt;p&gt;We’re given a policy, now evaluate its quality. We discussed three algorithms. Planning setting is matrix inversion and iteration, learning is temporal difference. Monte Carlo can be applied to the learning setting, but is generally planning. Planning means we have reward function and transition probabilities, in learning we have to act in the world to gain information:&lt;/p&gt;

&lt;p&gt;** Exam Question: ** How does the proof for convergence work?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Monte Carlo - $O(n)$. Start in state i, we get a trajectory, and eventually we stop. We average the rewards we get for many trajectories. This algorithm is model based because we are required to come back to state i, which we can’t always do in the real world.&lt;/li&gt;
  &lt;li&gt;Matrix Inversion - $O(n^3)$. Allows us to exactly solve the problem in one step, but takes a long time depending on size of the state space.&lt;/li&gt;
  &lt;li&gt;Iteration - $O(n^2)$. Iterative method to solve matrix inversion indirectly. Faster per iteration but requires many steps.&lt;/li&gt;
  &lt;li&gt;Temporal Difference (indirect) - Estimate our values by acting and observing and then estimating the expected reward at each state. We look ahead a certain amount.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-control&quot;&gt;Optimal Control&lt;/h3&gt;
&lt;p&gt;We want to find the best policy for an MDP.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value Iteration - Use the Bellman optimality equation. We replace the iterating equation in policy evaluation with a similar function but instead we take the max action value. Note that Q-value iteration is similar here, but that the max is inside the expectation which allows us to get unbiased samples. In Q-learning, we can prove that we will converge to the optimal policy as long as we decrease alpha similar to epsilon in the bandit problem. Similarly, epsilon greedy gets infinite exploration, and like bandits does not get optimal control without decreasing epsilon.&lt;/li&gt;
  &lt;li&gt;Policy Iteration - Two step process, we calculate the policy, then improve the policy iteratively. This converges much more quickly than value iteration and has definitive stopping conditions&lt;/li&gt;
  &lt;li&gt;SARSA - Similar to Q-learning, but is on-policy, meaning that we must behave in a special way for convergence. This allows us to converge faster if we behave correctly. In order to converge to the optimal value we have to be greedy in the limit. Greedily in the limit with infinite exploration (GLIE). Note that SARSA has no max at all, because we evaluate based on what we are doing at the time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;depth-d-planningreceeding-horizon-control&quot;&gt;Depth D Planning/Receeding Horizon Control&lt;/h3&gt;
&lt;p&gt;Build out a tree of actions of depth D, and do backwards induction to determine the best option. Basically we plan ahead for D steps and take an action, then the horizon receeds, and we repeat (we still plan ahead D steps). The horizon is exponential, but we can also do a sampling based version which allows us to compute this much more quickly. When we do this we lose dependence on the size of the state space (but we are still exponential on size of horizon).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>General NP and Parallelism</title>
   <link href="/eecs586/2015/04/15/eecs586/"/>
   <updated>2015-04-15T00:00:00-04:00</updated>
   <id>/eecs586/2015/04/15/eecs586</id>
   <content type="html">&lt;h2 id=&quot;general-stuff&quot;&gt;General Stuff&lt;/h2&gt;
&lt;p&gt;Talked about wall-time NP-complete solvers. Discussed exotic solutions such as Moore’s Law and relativistic effects. Most prove impossible. Quick discussion about bitonic sort.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Sparse Sampling and POMDPs</title>
   <link href="/eecs545/2015/04/15/eecs545/"/>
   <updated>2015-04-15T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/15/eecs545</id>
   <content type="html">&lt;h2 id=&quot;what-should-we-know-about-optimal-control&quot;&gt;What Should We Know About Optimal Control?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;How does value iteration work&lt;/li&gt;
  &lt;li&gt;How does policy iteration work&lt;/li&gt;
  &lt;li&gt;Why does value iteration converge&lt;/li&gt;
  &lt;li&gt;Why does policy iteration converge&lt;/li&gt;
  &lt;li&gt;Describe Q-learning and why/how it works&lt;/li&gt;
  &lt;li&gt;Describe SARSA and the difference from Q-learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sparse-sampling&quot;&gt;Sparse Sampling&lt;/h2&gt;
&lt;p&gt;Creates a generative model to allow us to look ahead in an MDP and choose an optimal action instead of running through the entire MDP. This is basically a greedy algorithm but instead of looking only at the best it looks ahead a certain number. We can do backwards induction to determine our best action. We can relate the search depth to gamma by looking farther ahead the closer our gamma is to one. &lt;/p&gt;

&lt;h2 id=&quot;partially-observable-markov-decision-process&quot;&gt;Partially Observable Markov Decision Process&lt;/h2&gt;
&lt;p&gt;Satinder currently giving a fucked up version of the Monty Hall Gameshow problem where you get mauled by a tiger if you lose. In this game there are two possible world states, True State=R or S=L door. We can either open a door or listen. Listening gives us a probability of the true state, but we still don’t know for sure. This is a POMDP. &lt;/p&gt;

&lt;p&gt;In a POMDP, we add a sensing model to the MDP, so now some things are not as clear-cut. We can’t see the state directly, but instead have a sensor that tells us with some probability what the actual state is. A POMDP is an HMM with actions! &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Partially Observable and Q-Learning</title>
   <link href="/eecs545/2015/04/13/eecs545/"/>
   <updated>2015-04-13T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/13/eecs545</id>
   <content type="html">&lt;h2 id=&quot;optimal-control&quot;&gt;Optimal Control&lt;/h2&gt;
&lt;p&gt;Remember that in the planning setting we did value iteration, where we knew everything except the optimal values. This includes the reward function as well as the transition probabilities. Value iteration just iterates, updating the policy as we reevaluate the value function. Can we prove that this algorithm converges? Yes&lt;/p&gt;

&lt;p&gt;Similar to previous proof, somehow has to do with contraction. See picture or result in book.&lt;/p&gt;

&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;
&lt;p&gt;Start with an arbitrary policy. Do policy evaluation to get the value function for this policy. Then update the policy using the greedy update procedure and repeat. Stop when the value function stops changing. Why is this guaranteed to improve?&lt;/p&gt;

&lt;h2 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h2&gt;
&lt;p&gt;Let’s say that Q* is the best you can do in terms of rewards at state S taking action A. We can do Q-value iteration, which is the Q value version of value iteration. Again, we assume we know the reward function and transition probabilities. To update we do something similar to temporal difference. We mix the old value plus a new guess to get the next iteration. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam Question&lt;/strong&gt;: stochastic approximation conditions&lt;/p&gt;

&lt;p&gt;In Q-Learning, the non-linear max operator is inside the expectation, so we can get a non-biased estimate. How do we initialize? Optimistic initialization. If we do this then we explore early. &lt;/p&gt;

&lt;h2 id=&quot;sarsa&quot;&gt;SARSA&lt;/h2&gt;
&lt;p&gt;Very similar to Q learning. Q learning considers actions you could take in the next state, while SARSA considers actions you actually take. Basically just no max. This algorithm converges to the expected value of the discounted rewards you’d get if you started in state s and took action a then followed policy pi thereafter. &lt;/p&gt;

&lt;p&gt;How can we pick actions to get Q*? We could pick actions greedily, but this would get us stuck. Behave with an epsilon-greedy policy, shrinking epsilon over time. SARSA is &lt;em&gt;on-policy&lt;/em&gt; while Q-learning is &lt;em&gt;off-policy&lt;/em&gt;, meaning that SARSA converges to the value of the policy you are behaving on.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>NP-Complete</title>
   <link href="/eecs586/2015/04/08/eecs586/"/>
   <updated>2015-04-08T00:00:00-04:00</updated>
   <id>/eecs586/2015/04/08/eecs586</id>
   <content type="html">&lt;h2 id=&quot;hardness&quot;&gt;Hardness&lt;/h2&gt;
&lt;p&gt;A problem is NP-Hard if giving a polynomial time algorithm for it shows that P=NP. Say you wanna solve an NP-hard problem? Now what?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Maybe you have a small number of instances&lt;/li&gt;
  &lt;li&gt;Maybe some parameters are fixed&lt;/li&gt;
  &lt;li&gt;Maybe it’s easy for special cases of interest&lt;/li&gt;
  &lt;li&gt;Maybe you only care about the expected case&lt;/li&gt;
  &lt;li&gt;Maybe you only need a close approximation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How do we know when special cases are easy? Who knows?&lt;/p&gt;

&lt;h2 id=&quot;expected-case-analysis&quot;&gt;Expected Case Analysis&lt;/h2&gt;
&lt;p&gt;For expected case we need to know the probability distribution. &lt;/p&gt;

&lt;p&gt;Hamiltonian cycles: recall that it’s a cycle through all verticies where we don’t repeat any of them. Problem: for every pair of vertices, we add an edge with probability p. As n verticies goes to infinity, the probability of having a Hamiltonian cycle goes to 1. In 1987, Gurerich and Shiloch proved that you can find a Hamiltonian cycle or prove that non exists in expected O(n) time in this specific case.&lt;/p&gt;

&lt;p&gt;In many cases the NP part of the problem won’t be reached in the expected case if the distribution you’re working with is right.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Monte Carlo and POMDP</title>
   <link href="/eecs545/2015/04/08/eecs545/"/>
   <updated>2015-04-08T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/08/eecs545</id>
   <content type="html">&lt;h2 id=&quot;monte-carlo&quot;&gt;Monte Carlo&lt;/h2&gt;
&lt;p&gt;Use first-visit Monte Carlo for MDP evaluation, or we can use the linear relationship between value functions. For the equation, &lt;script type=&quot;math/tex&quot;&gt;R_i^{\pi}&lt;/script&gt; represents the you get for a state pi at an immediate action state.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam Question:&lt;/strong&gt; Prove that the iterative policy evaluation converges, and the error of Vk+1 is no less than gamma.&lt;/p&gt;

&lt;p&gt;This is a contraction mapping. No idea what that is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam Question:&lt;/strong&gt; How many iterations of value iteration are required to get an epsilon answer?&lt;/p&gt;

&lt;p&gt;What about for Monte Carlo? To get an epsilon answer, the number of MC simulations required is independent of the size of the state space! The variance matters, but the size of the state space does not. How many random samples do we need to draw from an arbitrary distribution so that the empirical mean is within epsilon of the true mean with probability of 1-alpha? &lt;/p&gt;

&lt;h2 id=&quot;temporal-difference-0&quot;&gt;Temporal Difference 0&lt;/h2&gt;
&lt;p&gt;The idea is that want to update the value of the state at time t, so you look ahead one step. Take the difference of the lookahead estimate and the current estimate. You update the value of your state proportional to the difference.&lt;/p&gt;

&lt;h2 id=&quot;optimal-policy&quot;&gt;Optimal Policy&lt;/h2&gt;
&lt;p&gt;Finds the maximum policy via iteration. We can’t solve in closed form, but we can use dynamic programming.&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h2&gt;
&lt;p&gt;A dynamic programming method to compute optimal policies. Next time is Q learning.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reductions</title>
   <link href="/eecs586/2015/04/06/eecs586/"/>
   <updated>2015-04-06T00:00:00-04:00</updated>
   <id>/eecs586/2015/04/06/eecs586</id>
   <content type="html">&lt;h2 id=&quot;sat-reduces-to-3-dimensional-mapping&quot;&gt;3-SAT reduces to 3 dimensional mapping&lt;/h2&gt;
&lt;p&gt;For an instance of 3SAT you have a set of U variables and clasuses C. For 3DM you have variables U, X, Y. &lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/3dm.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://courses.cs.vt.edu/cs5114/spring2009/lectures/lecture24-np-complete-problems.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~vazirani/algorithms/chap8.pdf&quot;&gt;Writeup&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;knapsack&quot;&gt;Knapsack&lt;/h2&gt;
&lt;p&gt;Given a finite set U, weight w(u) and value v(u), where w and v are positive integers. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Markov Decision Processes</title>
   <link href="/eecs545/2015/04/06/eecs545/"/>
   <updated>2015-04-06T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/06/eecs545</id>
   <content type="html">&lt;h2 id=&quot;markov-process&quot;&gt;Markov Process&lt;/h2&gt;
&lt;p&gt;At each time step we get a reward from our action. Consider the problem in the book found &lt;a href=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node33.html&quot;&gt;here&lt;/a&gt;. Transitions are random, so even without noise in the reward there is an expected value. The optimal policy is simultaneously optimal for every state, although the optimal policy may not be unique. As a Markov problem, the probabilities don’t change over time, so the value function &lt;em&gt;is a function of just state&lt;/em&gt;. &lt;/p&gt;

&lt;h2 id=&quot;problems&quot;&gt;Problems&lt;/h2&gt;
&lt;p&gt;We start with a given policy and want to compute how good it is. Optimal control problem is find the optimal policy. Today will cover policy evaluation.&lt;/p&gt;

&lt;h2 id=&quot;policy-evaluation&quot;&gt;Policy Evaluation&lt;/h2&gt;
&lt;p&gt;Implementation note: to construct a policy matrix, take a row from each matrix of actions corresponding to the policy we are given. Given a pi, how do we find a Vpi? &lt;/p&gt;

&lt;p&gt;First option: Monte Carlo approach. Just sample a ton of times. We’ll stop early because eventually the discount factor creates a vanishing function.
Second option: Matrix inversion method. Requires a model. Uses the value equation and solves.
Third option: Iterative approach. Same as previous, but we solve recursively.&lt;/p&gt;

&lt;p&gt;Why Monte Carlo?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Complexity</title>
   <link href="/eecs586/2015/04/01/eecs586/"/>
   <updated>2015-04-01T00:00:00-04:00</updated>
   <id>/eecs586/2015/04/01/eecs586</id>
   <content type="html">&lt;h2 id=&quot;np-complete&quot;&gt;NP-complete&lt;/h2&gt;
&lt;p&gt;We say A is NP-complete if &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; A \in NP &lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; B \leq_p A \exists B \in NP &lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can say C is NP-complete if&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; C \in NP &lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;There is an NP-complete D s.t. &lt;script type=&quot;math/tex&quot;&gt;D \leq_p C&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;version-of-satisfiability&quot;&gt;Version of Satisfiability&lt;/h2&gt;
&lt;p&gt;U is a set of boolean variables and a literal is either u or not u. A clause is a set of literals. A clause is satisfied if one of the literals is true. A collection of clauses is satisfied if there is a truth assignment that satisfies all the clauses. &lt;/p&gt;

&lt;p&gt;In 3-SAT the clauses only have 3 literals. It turns out we can reduce many problems into a 3-SAT problem. &lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Reinforcement Learning</title>
   <link href="/eecs545/2015/04/01/eecs545/"/>
   <updated>2015-04-01T00:00:00-04:00</updated>
   <id>/eecs545/2015/04/01/eecs545</id>
   <content type="html">&lt;h2 id=&quot;bandits&quot;&gt;Bandits&lt;/h2&gt;
&lt;p&gt;Recall the bandit problem. We don’t know what the distribution of each bandit is, but we want to maximize our reward. This will transition into MDPs. First let’s talk about notation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Action values (Q) is the average reward for a specific action.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Epsilon greedy algorithm for playing bandits:&lt;/p&gt;

&lt;p&gt;We select a greedy arm most of the time, and a random arm some other percentage of the time. We can also start with a high epsilon and decrease it. With a very small epsilon we’ll find almost the optimal. How fast can we shrink epsilon though? We need to explore but if we shrink too slow we’ll perform worse. We’ll call this change in the parameter the &lt;strong&gt;schedule&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;For episilon-decreasing, we can bound our sum on epsilon by a few things. One, we want the sum to be infinity. Two, we want the sum of epsilon to be finite. A harmonic series satisfies this property. In practice we have bigger steps and we reduce the learning rate, because in practice 1/t is too slow. &lt;/p&gt;

&lt;p&gt;Softmax Procedure:&lt;/p&gt;

&lt;p&gt;Another version is to set the probability of choosing a lever as the softmax of the lever. Temperature is a parameter of the softmax, and with a higher temperature you’ll have a more even probability. &lt;/p&gt;

&lt;p&gt;Bandits are a pure exploration-exploitation problem. They have no generalization problems.&lt;/p&gt;

&lt;h2 id=&quot;final-and-logistics&quot;&gt;Final and logistics&lt;/h2&gt;

&lt;p&gt;No project. Two more homeworks. LOL. Remaining 20 percent will go to the better of the exams. &lt;/p&gt;

&lt;h2 id=&quot;contextual-bandits&quot;&gt;Contextual-bandits&lt;/h2&gt;
&lt;p&gt;We can choose levers based on context, and then we can generalize the actions based on new contexts. &lt;/p&gt;

&lt;h2 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h2&gt;
&lt;p&gt;Notation&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ot is the observation at time t&lt;/li&gt;
  &lt;li&gt;Rt is the reward at time t&lt;/li&gt;
  &lt;li&gt;At is the action at time t&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The agent wants to maximize a reward given observations by taking actions. There is some discount factor because doing something now is better than later. &lt;/p&gt;

&lt;p&gt;How does a world satisfy a Markov property? If we have the last observation and action, then in a Markov world this is all we need to characterize the world. If observations are perfect, then we call the observation the state.&lt;/p&gt;

&lt;p&gt;An MDP is defined by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A set of states&lt;/li&gt;
  &lt;li&gt;A set of actions&lt;/li&gt;
  &lt;li&gt;Transition probabilities&lt;/li&gt;
  &lt;li&gt;A reward function&lt;/li&gt;
  &lt;li&gt;A discount factor&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Online Learning</title>
   <link href="/2015/03/30/"/>
   <updated>2015-03-30T00:00:00-04:00</updated>
   <id>/2015/03/</id>
   <content type="html">&lt;h2 id=&quot;online-learning&quot;&gt;Online Learning&lt;/h2&gt;
&lt;p&gt;We want to learn as we get new data in the system. Generally we can use the multiarmed bandit problem to try and minimize regret, where at each point we select an arm. There exists a strategy where our regret is no worse than O(log n). The UCB algorithm says that you should choose the arm with the highest uncertainty and the highest reward. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Convex Hulls</title>
   <link href="/eecs586/2015/03/25/eecs586/"/>
   <updated>2015-03-25T00:00:00-04:00</updated>
   <id>/eecs586/2015/03/25/eecs586</id>
   <content type="html">&lt;h2 id=&quot;convex-hulls&quot;&gt;Convex Hulls&lt;/h2&gt;
&lt;p&gt;We have some points in two space. This is &lt;strong&gt;Graham Scan&lt;/strong&gt;. We want to find extreme points of convex hull. Start with a point guaranteed to be extreme. Take rightmost, lowest point. Sort remaining points by angle relative to start. Each time we reach one add it to a list of tentative extreme points. Test convexity a point at a time and remove non-convex points. When we remove a point we must check the ONE previous point until we hit an okay point. Time is O(n log n) based on amortized analysis. In this analysis, we must charge a point when we delete it.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;Jarvis’ March&lt;/strong&gt;, we again start with the bottom left point. Note that the smallest angle &lt;em&gt;must&lt;/em&gt; be an extreme point. Now we move from extreme point to extreme point, going around in a circle until all of our extreme points are added. This is O(nh) time where n is the number of points and h is the number of extreme points. &lt;/p&gt;

&lt;p&gt;If we’re given the points of each polygon in order, we can find the convex hull in O(P1+P2). &lt;/p&gt;

&lt;p&gt;We can also do a divide and conquer method for finding the convex hull. Divide the middle and combine the two by finding the secant lines that are invalid and removing them. Why do this? In higher dimensions. &lt;/p&gt;

&lt;p&gt;Expected case with IID points? Jarvis’ march is fairly slow, but divide and conquer is faster expected. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hidden Markov Models</title>
   <link href="/eecs545/2015/03/25/eecs545/"/>
   <updated>2015-03-25T00:00:00-04:00</updated>
   <id>/eecs545/2015/03/25/eecs545</id>
   <content type="html">&lt;h2 id=&quot;sequential-data&quot;&gt;Sequential Data&lt;/h2&gt;
&lt;p&gt;In many cases you don’t want to model things as IID, generally because they have dependencies over time (or sequence, whatever). &lt;/p&gt;

&lt;h2 id=&quot;markov-chain&quot;&gt;Markov Chain&lt;/h2&gt;
&lt;p&gt;A Markov chain is a series of random variables where future is independent of the past, given the present. Basically, we know the future state from today’s state. This is still simplified, but isn’t IID (where tomorrow is independent of today). We can also create higher order Markov models, where the dependencies are extended going further back.&lt;/p&gt;

&lt;h2 id=&quot;hidden-markov-model&quot;&gt;Hidden Markov Model&lt;/h2&gt;
&lt;p&gt;An HMM is a Markov chain that emits observations in every time step. We can’t see the hidden state, but we can assume it’s there and we can model it. In HMM, the latent variable is discrete. You can also imagine that the Z’s are not discrete, which creates a linear dynamical system.&lt;/p&gt;

&lt;p&gt;In many cases, we will need to add constraints to the transitions. &lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-for-em&quot;&gt;Maximum Likelihood for EM&lt;/h2&gt;
&lt;p&gt;In this case, gamma is the probability that a particular state is created by a given point. &lt;/p&gt;

&lt;h2 id=&quot;backwards-forwards-algorithm&quot;&gt;Backwards-Forwards Algorithm&lt;/h2&gt;
&lt;p&gt;Message passing algorithm that sends alpha and beta backwards and forwards through the model. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Closest Pairs</title>
   <link href="/eecs586/2015/03/18/eecs586/"/>
   <updated>2015-03-18T00:00:00-04:00</updated>
   <id>/eecs586/2015/03/18/eecs586</id>
   <content type="html">&lt;h2 id=&quot;closest-pairs&quot;&gt;Closest Pairs&lt;/h2&gt;
&lt;p&gt;How do we find closest pairs in 2d? Use the same algorithm as for 1d. As we raise the number of dimensions we’ll have to raise the number of neighbors we look at, but it’ll still be O(n) because the neighbors are fixed. What if we want the problem analysis in terms of d and n? &lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega(3^d n)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\Theta(d3^d n)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So this is O(n) where the implied constants are a function of d. &lt;/p&gt;

&lt;p&gt;In 2d what is the worst case? If we split into two parts, the middle needs to be examined. If we divide a space into patches of delta, then were are no more than 5 points in that space (in 2d). &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Decision Trees</title>
   <link href="/eecs545/2015/03/18/eecs545/"/>
   <updated>2015-03-18T00:00:00-04:00</updated>
   <id>/eecs545/2015/03/18/eecs545</id>
   <content type="html">&lt;h2 id=&quot;information-theory&quot;&gt;Information Theory&lt;/h2&gt;
&lt;p&gt;Decision trees need a way to quantify uncertainty. We’ll use entropy, which is “how many bits does it take to transmit this information”. In the sense that if the data is completely random you’ll need one bit for two symbols, but if it’s not you need fewer bits. If it’s only one, you need zero bits (since the data is always the same).&lt;/p&gt;

&lt;p&gt;We can also define joint entropy and conditional entropy.&lt;/p&gt;

&lt;p&gt;Information gain describes how many bits on average it would save me if both ends of the channel already &lt;em&gt;knew&lt;/em&gt; X. At each node, we want to compute the information gain and that’s how we will construct our tree.&lt;/p&gt;

&lt;h2 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;Decision trees are easy ways to make decisions — they don’t use much computational power and they’re interpretable. Note that a decision tree can perfectly fit data as long as there isn’t noise. Thus, we either need to stop early or prune the tree in order to prevent overfitting. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Expectation Maximization</title>
   <link href="/eecs545/2015/03/11/eecs545/"/>
   <updated>2015-03-11T00:00:00-04:00</updated>
   <id>/eecs545/2015/03/11/eecs545</id>
   <content type="html">&lt;h2 id=&quot;mixture-model-em&quot;&gt;Mixture model EM&lt;/h2&gt;
&lt;p&gt;Given observation X and latent variable Z, we can say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \log P_{\Theta}(X) = \log \sum_Z P_{\Theta}(x,z) &lt;/script&gt;

&lt;p&gt;There is a full derivation on CTools. We’ll use (Carly Rae) Jensen’s inequality and introduce a Q(z) that is the expected value of the log likelihood function, with respect to the conditional distribution of Z given X, under the &lt;strong&gt;current parameter estimates&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;The final algorithm has two steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Expectation: Find Q, or the expected value with the current parameters&lt;/li&gt;
  &lt;li&gt;Maximization: Fix z, and find the parameter that maximizes Q&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are maximizing the lower bound! Claim: we have guarenteed to have improved the log likelihood. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam Question&lt;/strong&gt;: Given a simple variation of GMM, give the E and M steps of the model&lt;/p&gt;

&lt;h2 id=&quot;principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/h2&gt;
&lt;p&gt;PCA maximizes the variance of the projection of data into a lower dimensional space. Basically we choose the direction of projection so that the resulting projections have the highest variance. We can find principal components one at a time. We want to find the vector u1 that maximizes the projection variance, using lagrange multipliers. This ends up being the same as finding the eigenvector with the largest eigenvalue of the covariance matricies. BOOM THIS IS MATH.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Negative Edges in Search</title>
   <link href="/eecs586/2015/03/09/eecs586/"/>
   <updated>2015-03-09T00:00:00-04:00</updated>
   <id>/eecs586/2015/03/09/eecs586</id>
   <content type="html">&lt;p&gt;Negative edges are less common, but in Dijkstra’s algorithm we won’t be able to keep track of the best possible choice at each step (because edges can be negative and thus actually improve our score moving outwards). How do we fix it? We can use Bellman-Ford to search instead. This algorithm relaxes &lt;strong&gt;all&lt;/strong&gt; edges, not just the best looking ones. Unfortunately this can result in an infinite running time if we have a negative cycle. We’ll need to keep track of these negative cycles or we’re screwed.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Expectation Maximization</title>
   <link href="/eecs545/2015/03/09/eecs545/"/>
   <updated>2015-03-09T00:00:00-04:00</updated>
   <id>/eecs545/2015/03/09/eecs545</id>
   <content type="html">&lt;h2 id=&quot;learning-in-bayes-nets&quot;&gt;Learning in Bayes Nets&lt;/h2&gt;
&lt;p&gt;If we know the structure, there are two methods of learning: full observation (maximum likelihood) and partial observation (EM). &lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-for-bayes-nets&quot;&gt;Maximum Likelihood for Bayes Nets&lt;/h2&gt;
&lt;p&gt;Given a simple Bayes net X-&amp;gt;Y, what are the parameters?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y \mid X)&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can take derivatives, set them to zero, and find the maximum likelihood. In Bayesian nets, the likelihood term decomposes with respect to local conditional probability tables. If we know all of the values of each parameter, it’s very easy to find the correct values. The complexity depends on the size of the parent sets, which is good for scaling as long as parent sets aren’t massive. This is equivalent to finding weights in linear regression. &lt;/p&gt;

&lt;p&gt;In a Markov network, have a probability distribution and a structure. We want to take derivative with respect to the parameters. We have representation of each clique. The solution isn’t closed form, but we can use derivatives for an interative approach. &lt;/p&gt;

&lt;h2 id=&quot;expectation-maximization&quot;&gt;Expectation Maximization&lt;/h2&gt;
&lt;p&gt;For parameter learning when the data is not fully observed. Realistically, this is what we’re going to get. Suppose we have a set of variables X and the remaining set of variables are hidden Z. We treat Q=P(Z|X) and then we update the observations by treating Q as an observation. &lt;/p&gt;

&lt;p&gt;We’ll start with K-means: K-means is iterative and we try to minimize distortion measure J. The K-Means algorithm can be viewed as an EM sequence! In the E step we assign each point to its closest center. In the M step we update the centers based on distance. &lt;/p&gt;

&lt;p&gt;Mixture of Gaussians allows soft clustering, where K means requires that each point is in one cluster. In the GMM, each point is generated by a Gaussian, but they overlap so we don’t know for sure which point comes from where. E step is to compute the probability that point was generated by Gaussian K. The M step is that given the responsibilities of each Gaussian, we reevaluate the coefficients. Derivation of EM is available online, but suggest &lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes8.pdf&quot;&gt;Stanford 229 notes&lt;/a&gt;. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>TITLE</title>
   <link href="/eecs586/2015/02/25/eecs586/"/>
   <updated>2015-02-25T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/25/eecs586</id>
   <content type="html">&lt;h2 id=&quot;homework-review&quot;&gt;Homework Review&lt;/h2&gt;
&lt;p&gt;We can do problem 3 in &lt;script type=&quot;math/tex&quot;&gt;\Theta(cn \log n)&lt;/script&gt; time instead of &lt;script type=&quot;math/tex&quot;&gt;\Theta(cn^2)&lt;/script&gt;. &lt;/p&gt;

&lt;h2 id=&quot;shortest-paths&quot;&gt;Shortest paths&lt;/h2&gt;
&lt;p&gt;We have a graph with weights on each edge (directed). Find the shortest path from u to v (sum of weights). Many variations of this problem:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Go from single source to single destination&lt;/li&gt;
  &lt;li&gt;Single source to all destinations&lt;/li&gt;
  &lt;li&gt;All sources to one destination&lt;/li&gt;
  &lt;li&gt;All sources all destinations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The middle two are the same problem. You’d expect single single to be easier but nobody has a better worst case for any of the first three. This is chapter 24/25.&lt;/p&gt;

&lt;p&gt;The solution is a rooted tree where everyone points to their parent. This is similar to Prim’s algorithm. Prim’s algorithm picks the edge of minimal weight but this doesn’t work for us. For each vertex not in the tree, keep track of the shortest path to get to it so far. We instead pick the edge with the minimum distance to the endpoint. &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; d(u) = \min(d(u), d(v)+w(e)) &lt;/script&gt;

&lt;p&gt;This is Dijkstra’s algorithm. Time complexity of Prim is the same as this one. Single shortest path is easy, but what if we want a parallel version of this.&lt;/p&gt;

&lt;p&gt;All to all distance? We could repeat the algorithm over and over, but we’re going to throw away a ton of stuff by doing that. Make a weighted adjacency matrix W, if no edge weight is infinity. Interpret this as the shortest path using one edge. &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; W_2(i,j) = \min \{ \min\limits_{k=1..n}W(i,k) + W(k,j), W(i,j) \}&lt;/script&gt;

&lt;p&gt;This is the shortest path of two or fewer steps. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>More Graphical Models</title>
   <link href="/eecs545/2015/02/25/eecs545/"/>
   <updated>2015-02-25T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/25/eecs545</id>
   <content type="html">&lt;p&gt;A bayesian network cannot always be represented by a Markov network. Every joint distribution can be represented by a directed OR undirected graphical model. &lt;/p&gt;

&lt;h2 id=&quot;inference-in-graphical-models&quot;&gt;Inference in Graphical Models&lt;/h2&gt;
&lt;p&gt;We can marginalize probabilities to do inference in graphical models. Remember from AI, we want to move around summations by using the graphical structure to minimize calculation. &lt;/p&gt;

&lt;h2 id=&quot;factor-graphs&quot;&gt;Factor graphs&lt;/h2&gt;
&lt;p&gt;The factor graph is the combination of various “factors” that let us represent a DAG in a more compact way. It’s not unique, but allows us to do inference by representing probabilities in a less parametric form. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dynamic Programming Part 2</title>
   <link href="/eecs586/2015/02/23/eecs586/"/>
   <updated>2015-02-23T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/23/eecs586</id>
   <content type="html">&lt;h2 id=&quot;minimal-spanning-tree&quot;&gt;Minimal Spanning Tree&lt;/h2&gt;
&lt;p&gt;Trying to show that starting with the antiset and adding safe edges is correct. This is proving that a greedy algorithm is optimal. Suppose T is a MST that contains A. In T there is a path between A and B. We want to pick an f that is on the path. We pick a greedy edge, so weight(f) &amp;gt;= weight(e). Now we remove f and add e. We are trying to make the greedy algorithm look like the optimal one. This is in the book.&lt;/p&gt;

&lt;p&gt;Prim: Grow a tree by adding in new edges over and over.
Kruskal: Grow a forest by adding together subsets of forests.&lt;/p&gt;

&lt;p&gt;What are running times? We need to ensure we aren’t creating cycles. Time is &lt;script type=&quot;math/tex&quot;&gt;\Theta(E \log V)&lt;/script&gt;. For Kruskal we want the minimum weight among all edges that doesn’t create a cycle. Chapter 21 shows a data structure for this.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Networks</title>
   <link href="/eecs545/2015/02/23/eecs545/"/>
   <updated>2015-02-23T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/23/eecs545</id>
   <content type="html">&lt;h2 id=&quot;network-parameterization&quot;&gt;Network Parameterization&lt;/h2&gt;
&lt;p&gt;Explaining away: when you have two causes for a symptom and one cause is true, the belief that the other cause is true goes down. This is the heart of inference and is intuitive. &lt;/p&gt;

&lt;p&gt;D-separation: If all paths from A to B are blocked, A is d-separated from B by C. If A is d-separated from B to C, the joint distribution over all variables in the graph are A dep B given C.&lt;/p&gt;

&lt;p&gt;Markov Blanket: A set S is a Markov blanket if if X is conditionally independent of all other vars given S. We must include all children, parents, and parents of children. We need to include parents of children because otherwise we could feed through head to head knowledge by changing the co-parent.&lt;/p&gt;

&lt;p&gt;Noisy-OR Parameterization: In context of diseases: for every disease you have the prob of you not having a symptom goes down by a multiplicative amount. We claim this is a compact version of the conditional prob tables.&lt;/p&gt;

&lt;h2 id=&quot;hidden-markov-models&quot;&gt;Hidden Markov Models&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Exam Question&lt;/strong&gt;: Given a diagram write down the joint probability (slide 55).&lt;/p&gt;

&lt;h2 id=&quot;markov-random-fields&quot;&gt;Markov Random Fields&lt;/h2&gt;
&lt;p&gt;A clique is a set of nodes connected to its neighbors. A maximal clique means the nodes are fully connected. If our intuition is about causality, then we want a directed graph. If there isn’t a causal intuition, then we can go for an undirected graph. To convert directed to undirected graphs we need to ‘moralize’ the graph by marrying the parents.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="/eecs586/2015/02/18/eecs586/"/>
   <updated>2015-02-18T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/18/eecs586</id>
   <content type="html">&lt;h2 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h2&gt;
&lt;p&gt;For activities that may overlap, find a set that don’t. First option is to build an interval tree, this is &lt;script type=&quot;math/tex&quot;&gt;\Theta(n \log n)&lt;/script&gt; but not always correct. Second is to build a graph and solve the graph using a topological sort. This is &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt; but is always correct.&lt;/p&gt;

&lt;p&gt;Can we do better?&lt;/p&gt;

&lt;p&gt;How to we compare greedy and non-greedy algorithms? We want to try and force the non-greedy solution to look like ours. Chapter 16.&lt;/p&gt;

&lt;p&gt;What if we don’t generate the graph explicitly? We need to keep track of when shorter events finish in a sweep. Check out the augmented tree chapter to see the sweep line technique in other problems.&lt;/p&gt;

&lt;h2 id=&quot;graphs---chapter-23&quot;&gt;Graphs - Chapter 23&lt;/h2&gt;
&lt;p&gt;More greedy algorithms. Note: we can represent a graph as an edge list or an adjacency matrix. Be careful if you give a time for an algorithm – what’s your input. For an adj list it takes time to answer queries. More notes: using V when we mean absolute value of V is a trip up. &lt;/p&gt;

&lt;p&gt;Traveling salesman problem: We have an acyclic graph and a set of roads. We want to find a subset of roads that connect all the cities with minimal cost. We’ll assume that weights are positive. There exists a generic greedy approach for finding the minimum spanning tree. This is the major way to do it. Given S, an edge E is a “safe” edge if S set of edges is in some MST and S union e set of edges in some MST.&lt;/p&gt;

&lt;p&gt;A cut is a partition of verticies in V1, V2 so that S &amp;lt; V1. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Graphical Models</title>
   <link href="/eecs545/2015/02/18/eecs545/"/>
   <updated>2015-02-18T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/18/eecs545</id>
   <content type="html">&lt;h2 id=&quot;joint-probability&quot;&gt;Joint Probability&lt;/h2&gt;
&lt;p&gt;Marginalization is the process of computing total probabilities by summing over the joint probabilities for a given variable. How do we learn and represent these joint probability tables. This is basically graphical models of 492, and will probably be about as fun.&lt;/p&gt;

&lt;p&gt;We’ll use the chain rule (many times) for most computations in this section. Remember that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(a,b,c) = p(c \mid a,b)p(b \mid a)p(a) &lt;/script&gt;

&lt;p&gt;and et cetera. The directed acyclic graph that this model forms is based on conditional probabilities. A child is conditioned on its parents. A fully connected graph, in an &lt;em&gt;acyclic graph&lt;/em&gt;, connects all parents to all their children. If every node is connected to all the others it is no longer acyclic. Ordering in these graphs matters hugely. For notation, we show repeating cycles by boxes in graphical models. Shaded circles are observed. &lt;/p&gt;

&lt;p&gt;There are various parameterizations for Bayesian graphs. Overview of parameterizations, we’ve seen all of these in AI. Check the notes for a refresher.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dynamic Programming</title>
   <link href="/eecs586/2015/02/16/eecs586/"/>
   <updated>2015-02-16T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/16/eecs586</id>
   <content type="html">&lt;h2 id=&quot;homework-advice&quot;&gt;Homework Advice&lt;/h2&gt;
&lt;p&gt;Try and find a minimal property that will lead you to the thing you want. This invariance is usually the easiest way to solve these problems.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h2&gt;
&lt;p&gt;See CTools paper.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Cross Validation</title>
   <link href="/eecs545/2015/02/16/eecs545/"/>
   <updated>2015-02-16T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/16/eecs545</id>
   <content type="html">&lt;h2 id=&quot;applications-of-ml&quot;&gt;Applications of ML&lt;/h2&gt;
&lt;p&gt;Two approaches: you can go bottom up or top down. Bottom up is for research and new methods while top down is more for applications. Which way you go is up to you.&lt;/p&gt;

&lt;h2 id=&quot;bias-variance-tradeoff&quot;&gt;Bias Variance Tradeoff&lt;/h2&gt;
&lt;p&gt;Variance is how much does the model fit, bias is how well a model fits the data on average. We can view bias as “accuracy” and variance as “precision”. Our expected loss is bias^2 + variance + noise. This is a brilliant decomposition: the expected error is &lt;strong&gt;exactly&lt;/strong&gt; these three terms and we can prove it. &lt;/p&gt;

&lt;p&gt;This nature of bias and variance means that we generally must choose a tradeoff between the two. Because noise is always present, we have to pick between these parameters. Usually we can find a minima of these terms (especially because bias is squared). In these cases, we usually find that the minimum of the test squared error occurs at this minimum. We find a good balance between the two via cross-validation.&lt;/p&gt;

&lt;h2 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h2&gt;
&lt;p&gt;Generally the best way to cross validate is to use a train, validation, and test set to avoid overfitting the hyperparameters to the dataset you have. Cross validation is fairly basic - k-Fold or however many folds you want. Leave one out is the same as k-fold if k is n. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Optimization</title>
   <link href="/eecs586/2015/02/11/eecs586/"/>
   <updated>2015-02-11T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/11/eecs586</id>
   <content type="html">&lt;h2 id=&quot;different-types-of-optimizing&quot;&gt;Different Types of Optimizing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;NP-hard: Try every possible solution&lt;/li&gt;
  &lt;li&gt;Dynamic Programming: Try everything but do it efficiently&lt;/li&gt;
  &lt;li&gt;Greedy: Don’t try everything&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;optimal-binary-search&quot;&gt;Optimal Binary Search&lt;/h2&gt;
&lt;p&gt;Given keys and probabilities in advance. Our first try is an exhaustive search: just try everything. Obviously that sucks. We could also try a quick-sortish method where we find the optimal tree on smaller trees until the base case. This is better, but not optimal – we’ve already seen many of the trees we create. We are solving subproblems repeatedly. We need to &lt;em&gt;memoize&lt;/em&gt; our values we’ve seen. This is a key part of dynamic programming.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Bayesian Learning</title>
   <link href="/eecs545/2015/02/11/eecs545/"/>
   <updated>2015-02-11T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/11/eecs545</id>
   <content type="html">&lt;h2 id=&quot;gaussian-processes&quot;&gt;Gaussian Processes&lt;/h2&gt;
&lt;p&gt;A Gaussian process is a probability distribution over functions y(x), such that the set of values of y(x) evaluated jointly have a Guassian distribution. What determines a GP? A mean function, a covariance function, and that’s all. Normally we say the mean is zero, so the prior is represented by the kernel entirely. &lt;/p&gt;

&lt;p&gt;Linear regression, and Gaussian processes. Bayesian linear regression is just a special instance of a Guassian process. Features in Bayesian LR are kernel functions in GPs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fix this&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;y(x) = w^T \phi(x)&lt;/script&gt; with a weight prior &lt;script type=&quot;math/tex&quot;&gt;p(w) = N(w \mid 0, &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;A Gaussian process will generate data from its parameters (plus noise) but how do we actually learn one of these processes? We can find the next covariance given the current data. The predictive distribution is a Gaussian whose mean and variance depend on X(n+1). Try and ifnd a correspondence between GP and linear regression on your own time.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>More Trees</title>
   <link href="/eecs586/2015/02/09/eecs586/"/>
   <updated>2015-02-09T00:00:00-05:00</updated>
   <id>/eecs586/2015/02/09/eecs586</id>
   <content type="html">&lt;h2 id=&quot;red-black-trees&quot;&gt;Red Black Trees&lt;/h2&gt;
&lt;p&gt;Red black trees are always balanced by nature. For red black tree insertion we generally need to rotate trees. &lt;/p&gt;

&lt;h2 id=&quot;problem-for-freshman&quot;&gt;Problem for Freshman&lt;/h2&gt;
&lt;p&gt;We want a BST with insert, delete, search, and search by rank (order statistic). By order statistic, something like “find the 17th key”. Once we have the count and the tree constructed, the second two become trivial. We want insert and delete. We can use “augmented trees” where the trees have a count at each node. When we rotate, we update each node with the count of the child nodes. This additional operation is supported by the augment, and we can still insert and delete in log(n) time. &lt;/p&gt;

&lt;p&gt;To update the count of each node, we only require two rotations at most during insertion, and we can compute size attributes in constant time, so insertion is lg(n) because insertion is just lg(n). Deletion is similar, reqiring only 3 rotations at most and is again lg(n). Doing augmentaion like this changes our indexing, and changes which query is efficient. We’ll also look at interval trees, read the book for this one.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>SVM and Algebra</title>
   <link href="/eecs545/2015/02/09/eecs545/"/>
   <updated>2015-02-09T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/09/eecs545</id>
   <content type="html">&lt;h2 id=&quot;svm-reviews&quot;&gt;SVM Reviews&lt;/h2&gt;
&lt;p&gt;It’s an optimization problem, we want to solve a dual optimization with a Lagrangian function. We introduce slack variables to allow misclassifications. The dual formulation just introduces more constraints. The higher the C value the stricter the classifier is. A value of C=inf will result in the hard boundary classifier.&lt;/p&gt;

&lt;h2 id=&quot;multivariate-gaussians&quot;&gt;Multivariate Gaussians&lt;/h2&gt;
&lt;p&gt;Covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; is the outer product of &lt;script type=&quot;math/tex&quot;&gt;[x-E[x]]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Partitioned Gaussians: We can combine Gaussians via partitions xa and xb, which allows us to partition the covariance matrix into only four covariances! Also notational is that the inverse of the covariance is the precision matrix &lt;script type=&quot;math/tex&quot;&gt;\Lambda&lt;/script&gt;. The partitioned Guassian can use the Woodbury Matrix Inversion Lemma to invert which is easier than computing it by hand. &lt;/p&gt;

&lt;p&gt;In general we can treat matricies that are partitioned as matricies themselves. That is, linear algebra we do with numbers can extend to matricies. Fact: given a linear combo of Gaussian random vars, its distribution is also Gaussian. Its marginal distribution is also Gaussian. Its conditional distribution is also Gaussian.&lt;/p&gt;

&lt;h2 id=&quot;bayes-theorem-for-gaussians&quot;&gt;Bayes’ Theorem for Gaussians&lt;/h2&gt;
&lt;p&gt;We define our probabilities in terms of Guassians. Then we can find the posterior Gaussians just like with normal numbers but in symbolic form.&lt;/p&gt;

&lt;h1 id=&quot;bayesian-linear-regression&quot;&gt;Bayesian Linear Regression&lt;/h1&gt;
&lt;p&gt;Doing regression from a truly Bayesian perspective. We have a likelihood and a prior, which gives us a posterior. We also now have probabilities for the weights, which we might assume has zero mean and identity covariance. We get the variance of the weights fo’ free, which allows us to do regularization using weight variance instead of L2 norm. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Trees</title>
   <link href="/eecs545/2015/02/04/eecs545/"/>
   <updated>2015-02-04T00:00:00-05:00</updated>
   <id>/eecs545/2015/02/04/eecs545</id>
   <content type="html">&lt;h2 id=&quot;binary-search-trees&quot;&gt;Binary Search Trees&lt;/h2&gt;
&lt;p&gt;Binary search has insert, delete, and search. These operations are the important part, because searching is easy. To search we just do a compare at each level of the tree, we can search in log(n) no problem. Insert is also easy, just do a search and if it’s not where we expect put in our element. Delete is a little more complicated. If our element to delete is a leaf we remove it. If it’s a node with one child, just remove it and connect the severed parts. If it’s a node with two children, we could just promote one subtree and then hang the other one off of that. This is valid but poor. The better option is to take the smallest part of the right tree and promote it to the delete node’s location.&lt;/p&gt;

&lt;p&gt;For a given tree, the worst cases are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Search: &lt;script type=&quot;math/tex&quot;&gt;\Theta(n)&lt;/script&gt; for an unbalanced tree&lt;/li&gt;
  &lt;li&gt;Insert: &lt;script type=&quot;math/tex&quot;&gt;\Theta(n)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Delete: &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The best case to build up a tree is n log n, because any faster would mean that we have a sorting algorithm that’s better than n log n, and that’s provably impossible. &lt;/p&gt;

&lt;p&gt;Expected Search time is different. If all trees are equally likely the expected depth is about &lt;script type=&quot;math/tex&quot;&gt;\sqrt(\pi n)&lt;/script&gt;. This assumption is dumb though…we don’t build trees to be equally likely. We’ll instead assume that all input orderings are equally likely (no duplicates). &lt;/p&gt;

&lt;p&gt;Time to insert = time to find
average search time = time to build/n&lt;/p&gt;

&lt;p&gt;When we build up trees the balanced tree is most likely. The average number of comparisons to build a tree is the same as quicksort. Building a tree is basically quicksort anyway. &lt;/p&gt;

&lt;p&gt;Now something totally different:&lt;/p&gt;

&lt;p&gt;Assume each new insertion is a new key we’ve never seen.&lt;/p&gt;

&lt;p&gt;IIIDI, this is 72 equally likely trees (4!x3 options for each D). There are 25 possible balanced trees, which is better than 1/3 which is good. &lt;/p&gt;

&lt;h2 id=&quot;red-black-trees&quot;&gt;Red-Black Trees&lt;/h2&gt;
&lt;p&gt;Every node will be either red or black (these are NOT keys). Rule are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Red parent cannot have red child&lt;/li&gt;
  &lt;li&gt;From any node to a leaf, the number of blacks is always the same&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Quicksort</title>
   <link href="/eecs586/2015/01/28/eecs586/"/>
   <updated>2015-01-28T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/28/eecs586</id>
   <content type="html">&lt;h2 id=&quot;reviewing-the-homework&quot;&gt;Reviewing the Homework&lt;/h2&gt;
&lt;p&gt;Only thing of note is that number 2 is not divide and conquer, we can skip by k instead and end up with an algorithm with identical efficiency. Goddamn that’s easier than divide and conquer.&lt;/p&gt;

&lt;p&gt;To find the lower bound:&lt;/p&gt;

&lt;p&gt;I didn’t catch this, go in and ask Quentin or TA.&lt;/p&gt;

&lt;h2 id=&quot;quicksort&quot;&gt;Quicksort&lt;/h2&gt;
&lt;p&gt;Overview of quicksort:&lt;/p&gt;

&lt;p&gt;We set a pivot point, then split into parts above and below that pivot. Then we sort those by quicksort. Do this by swaps.&lt;/p&gt;

&lt;h3 id=&quot;worst-case&quot;&gt;Worst Case&lt;/h3&gt;
&lt;p&gt;Partition is linear. Worst case of actual sort is in the case that one partition is the whole list (ie X is the largest or smallest value) and then we have &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt;. Note this assumption requires convexity of quicksort! This means that having one big piece is ALWAYS same/worse than two pieces made from that big piece. &lt;/p&gt;

&lt;h3 id=&quot;best-case&quot;&gt;Best Case&lt;/h3&gt;
&lt;p&gt;Partitions are exactly half. This means &lt;script type=&quot;math/tex&quot;&gt;\Theta(n \log n)&lt;/script&gt;. &lt;/p&gt;

&lt;h3 id=&quot;expected-case&quot;&gt;Expected Case&lt;/h3&gt;
&lt;p&gt;We’ll say that all input orderings are equally likely and no duplicates. Expected comparisons:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^{n-1} \sum_{j=i+1}^n&lt;/script&gt; prob that ki and kj are compared. This is only if ki and kj are pivots.&lt;/p&gt;

&lt;p&gt;Also note that book picks pivot by choosing the rightmost element. Pick the middle! Or we can sample the distribution and find the median and choose that one.&lt;/p&gt;

&lt;h2 id=&quot;order-statistics-ch-9&quot;&gt;Order Statistics (Ch 9)&lt;/h2&gt;
&lt;p&gt;The order statistic is the ith item when the items are sorted (basically). We’ll just pick the lower median. &lt;/p&gt;

&lt;p&gt;Median: n/2 statistic&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Discriminant Functions and Kernel Methods</title>
   <link href="/eecs545/2015/01/28/eecs545/"/>
   <updated>2015-01-28T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/28/eecs545</id>
   <content type="html">&lt;h2 id=&quot;linear-discriminant-functions-again&quot;&gt;Linear Discriminant Functions (again)&lt;/h2&gt;
&lt;p&gt;In discriminant functions we directly represent the decision boundary as opposed to learning the distribution. Why is the decision region convex and linear?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam question:&lt;/strong&gt; Show that the decision region in a linear discriminant function is linear and convex.&lt;/p&gt;

&lt;h2 id=&quot;perceptron&quot;&gt;Perceptron&lt;/h2&gt;
&lt;p&gt;A generalized linear function of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(x) = f(w^T \phi(x))&lt;/script&gt;

&lt;p&gt;where f(a) is the sgn function. We claim that the perceptron will &lt;em&gt;always&lt;/em&gt; converge if the data is linearly separable. This is unaffected by learning rate.&lt;/p&gt;

&lt;h2 id=&quot;kernel-methods&quot;&gt;Kernel Methods&lt;/h2&gt;
&lt;p&gt;Remember that linear regression is not too computationally expensive because it doesn’t scale with data but with features. We can use the kernel trick to work in high dimensions and only use inner products. &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k(x,x') = \phi(x)^T\phi(x')&lt;/script&gt;

&lt;h2 id=&quot;dual-representations&quot;&gt;Dual Representations&lt;/h2&gt;
&lt;p&gt;See slides for the math. We’ll work through, substituting &lt;script type=&quot;math/tex&quot;&gt;w = \phi^T a&lt;/script&gt; in the objective function (for linear regression) and eventually we end up with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(x) = k(x)^T(K+\lambdaI_N)^{-1}t&lt;/script&gt;

&lt;p&gt;Kernel method scales quadratically with input datapoints, so when to use this vs non-kernel methods. We can use kernel methods for any algorithm that requires a distance between points. However, we can’t find the mean. Distances to the mean is totally fine though.&lt;/p&gt;

&lt;p&gt;Can we do stochastic gradient descent with just kernels? Who the fuck knows, but try and figure it out&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sorting</title>
   <link href="/eecs586/2015/01/26/eecs586/"/>
   <updated>2015-01-26T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/26/eecs586</id>
   <content type="html">&lt;h2 id=&quot;master-theorem&quot;&gt;Master Theorem&lt;/h2&gt;
&lt;p&gt;Three cases for theorem, intuition says:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;a=b&lt;/script&gt; means each layer does about as much work as the previous&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;a \lt b&lt;/script&gt; means each layer does less work&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;a \gt b&lt;/script&gt; means each layer does more work&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Review the book and notes, because there are fringe cases where we can’t actually apply the theorem.&lt;/p&gt;

&lt;h2 id=&quot;heapsort&quot;&gt;Heapsort&lt;/h2&gt;
&lt;p&gt;Uses a heap to maintain data, and reads from heap (which is sorted by definition) to sort. In a heap:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; A(i) \geq A(2i) &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; A(i) \geq A(2i+1) &lt;/script&gt;

&lt;p&gt;Maximum element at the root as a result of these local properties. &lt;/p&gt;

&lt;p&gt;Why are all these algorithms n log n? We want a lower bound on comparison based sort (remember no assumptions about the data, just comparison.) The intuition is the binary result of sorting algorithms creates the lg bounds. Two different inital sorted orders can’t end up at the same leaf, so the number of leaves must be &amp;gt;= number of initial input orderings. There are n! input orderings. For tree height h # of leaves &lt;script type=&quot;math/tex&quot;&gt;\leq 2^h&lt;/script&gt;, we need:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;2^h \geq n!&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;h \geq \log n!&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; h \geq \sum_{i=1}^n \log i &lt;/script&gt;

&lt;p&gt;so the lower bound is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \Theta(n \log n) &lt;/script&gt;

&lt;p&gt;What about the expected case? We can convert trees to each other while retaining leaf number and depth will differ by only one. Average case is &lt;script type=&quot;math/tex&quot;&gt;\Theta(n \log n)&lt;/script&gt;. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Naive Bayes</title>
   <link href="/eecs545/2015/01/26/eecs545/"/>
   <updated>2015-01-26T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/26/eecs545</id>
   <content type="html">&lt;h2 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Classifier that assumes conditional independence and uses Bayes’ rule to make classifications. Remember Bayes’ rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; p(C_k \vert \mathbf{x}) = \frac{p(C_k) \ p(\mathbf{x} \vert C_k)}{p(\mathbf{x})} &lt;/script&gt;

&lt;p&gt;We’ll draw our distribution from a Bernoulli or Multinomial distribution. With independence assumptions, the conditional distribution over class C is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; p(C_k \vert x_1, \dots, x_n) = \frac{1}{Z} p(C_k) \prod_{i=1}^n p(x_i \vert C_k) &lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Sorting</title>
   <link href="/eecs586/2015/01/21/eecs586/"/>
   <updated>2015-01-21T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/21/eecs586</id>
   <content type="html">&lt;h2 id=&quot;sorting-basics&quot;&gt;Sorting Basics&lt;/h2&gt;
&lt;p&gt;Sorting Assumptions:
* n items
* key (perhaps plus data)
* key type has {=,&amp;lt;,&amp;gt;} operations&lt;/p&gt;

&lt;p&gt;Major Dichotomy:
* Internal, or all data in RAM
* External, or all data in ROM (IOPs dominate)&lt;/p&gt;

&lt;p&gt;We’ll focus on RAM and not worry about IO. The book mentions selection/insertion sort (&lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt;) and merge sort (&lt;script type=&quot;math/tex&quot;&gt;\Theta(n \log n)&lt;/script&gt;). Counting sort (section 8.2) is a stable sort with &lt;script type=&quot;math/tex&quot;&gt;\Theta(n+k)&lt;/script&gt; time if keys are in k. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stable Sort&lt;/strong&gt; - If key i and key j are the same, and &lt;script type=&quot;math/tex&quot;&gt;i \lt j&lt;/script&gt;, then the final sorted order results in key i (and its data) occuring before key j (and its data).&lt;/p&gt;

&lt;h2 id=&quot;radix-sort-83&quot;&gt;Radix sort (8.3)&lt;/h2&gt;
&lt;p&gt;Keys are d digits base b. Sort digit by digit (stable). In many cases we’ll claim that radix is linear time (&lt;script type=&quot;math/tex&quot;&gt;\Theta(d(n+b))&lt;/script&gt;) but it depends. If d and b are fixed it’s linear time. If we fix b, we want key space to be large enough so that n keys distinct. In this case we need to grow d, where &lt;script type=&quot;math/tex&quot;&gt; d \gte \log_b n &lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bucket-sort-84&quot;&gt;Bucket sort (8.4)&lt;/h2&gt;
&lt;p&gt;Keys are numbers in [0,1). Suppose uniform random IID keys. This assumption is big. We sort into buckets and then insertion sort through them. Worst case of this sort is &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt;, where all keys are in the same bucket. See handouts. &lt;/p&gt;

&lt;p&gt;Expected time of bucket sort?&lt;/p&gt;

&lt;p&gt;Book shows the proof using indicator variables. Let’s talk about a different proof. Expected bucket size is n. Time to sort bucket is constant. This isn’t always right though - the expected time of sorting the bucket isn’t equal to the sorting time of expected bucket size.&lt;/p&gt;

&lt;p&gt;Why do I go to this class?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Classification</title>
   <link href="/eecs545/2015/01/21/eecs545/"/>
   <updated>2015-01-21T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/21/eecs545</id>
   <content type="html">&lt;h2 id=&quot;classification-problem&quot;&gt;Classification Problem&lt;/h2&gt;
&lt;p&gt;Given an input vector, assign it to one of K distinct classes. We can define 0-1 loss as the number of incorrect labels.&lt;/p&gt;

&lt;p&gt;Approaches:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nearest neighbor classifiers&lt;/li&gt;
  &lt;li&gt;Discriminant functions (SVM, Perceptron, etc)&lt;/li&gt;
  &lt;li&gt;Learn distributions
    &lt;ul&gt;
      &lt;li&gt;Discriminative models (Logistic Regression)&lt;/li&gt;
      &lt;li&gt;Generative models (HMM, Naive Bayes)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Exam Question:&lt;/strong&gt; Why is the posterior the quantity of interest?&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;
&lt;p&gt;Model class posterior using a sigmoid applied to a linear function of the feature vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; p(C_1 \mid \phi) = y(\phi) = \sigma(\mathbf{x}^T \phi(x)) &lt;/script&gt;

&lt;p&gt;Sigmoid is a smooth squashing function, logit function is the inverse of the sigmoid. This classifier generalizes to the &lt;em&gt;softmax&lt;/em&gt; function, which allows many classes and not just two. See slides for usual tricks on finding &lt;script type=&quot;math/tex&quot;&gt; P(t \mid w) &lt;/script&gt; and on gradient descent derivation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam Question:&lt;/strong&gt; Why is placing the decision boundary at 0.5 for the logistic classifier correct?&lt;/p&gt;

&lt;h2 id=&quot;generative-models&quot;&gt;Generative Models&lt;/h2&gt;
&lt;p&gt;Generative models learn the joint probabilities while discriminative models learn the conditional probabilities. In logistic regression, we make the assumption that the log odds are a linear function of x. &lt;/p&gt;

&lt;h2 id=&quot;gaussian-linear-discriminant-analysis&quot;&gt;Gaussian (Linear) Discriminant Analysis&lt;/h2&gt;
&lt;p&gt;Assume that the class conditional density is Gaussian. We make an assumption of what the Gaussians looks like, and then we estimate the parameters from the data. When we get a new data point, we sample from the distribution to determine the probability of each class. Fact: if we model the probabilities as Gaussians with the &lt;strong&gt;same covariance&lt;/strong&gt; matrix, then the optimal decision boundry is linear. &lt;/p&gt;

&lt;p&gt;Note to self: Look at derivation and do math yourself.&lt;/p&gt;

&lt;p&gt;Logistic regression is more flexible about data distribution, but GDA works well when the distribution follows the Gaussian assumption. In contrast, logistic regression requries costly iterative training procedures, but requires less parameters.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sequential Search Analysis</title>
   <link href="/eecs586/2015/01/14/eecs586/"/>
   <updated>2015-01-14T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/14/eecs586</id>
   <content type="html">&lt;h2 id=&quot;caching&quot;&gt;Caching&lt;/h2&gt;
&lt;p&gt;For move to front: What is the asymptotic number of expected keys encountered in a search?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; 1 + \sum_{i=1}^{n} p_i \sum_{j \dne i}\frac{p_j}{p_i+p_j} &lt;/script&gt;

&lt;p&gt;Where pi is the probability of request k_i, and the second term is the prob that kj is in front of ki.&lt;/p&gt;

&lt;p&gt;For transposition caching: We’ll assume n is more than 2 and that all probabilities are not the same. R.Rivset proved that transposition has a lower expected number of items examined than MTF does.&lt;/p&gt;

&lt;p&gt;In either case, we can represent the state space as a markov chain and take it to steady state to find expected number of items. There are two major methods of analyzing sequential search algorithms: probabilistically or via amortization. &lt;/p&gt;

&lt;p&gt;Amortized Analysis of Self-Organizing Sequential Search Heuristics:&lt;/p&gt;

&lt;p&gt;There is a sequence of requests, and the optimal ordering is order of decreasing m value. In this analysis we will not go to infinity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Linear Regression and Classification</title>
   <link href="/eecs545/2015/01/14/eecs545/"/>
   <updated>2015-01-14T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/14/eecs545</id>
   <content type="html">&lt;h2 id=&quot;probability-review&quot;&gt;Probability Review&lt;/h2&gt;
&lt;p&gt;Not many notes here. Review axioms. Review additional properties such as unions, law of total prob, Bayes’ rule. Likelihood functions, applications of Bayes’ rule:&lt;/p&gt;

&lt;p&gt;Posterior &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; likelihood x prior&lt;/p&gt;

&lt;p&gt;Remember that it’s easier to find the likelihood than it is to find the posterior. &lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood&quot;&gt;Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;Choose a parameter setting w that maximizes the likelihood function p(D|w). We’ll use log-likelihood so we can minimize the log (which has nice properties that let us sum instead of multiply.) There is also the Maximum a posteriori estimation, where we maximize p(w|D) instead of p(D|w). In this case the prior is important. These two different estimates will be the same when the prior is uniform.&lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-interpretation-of-linear-least-squares&quot;&gt;Maximum Likelihood interpretation of Linear Least Squares&lt;/h2&gt;
&lt;p&gt;Assume a stochastic model:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; t = y(x, w) + \epsilon &lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt; \epsilon \sim \mathcal{N}(0, \beta^{-1}) &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;this gives likelihood function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; p(t \shortmid x,w,\beta) = \mathcal{N}(t \shortmid y(x,w),\beta^{-1}) &lt;/script&gt;

&lt;p&gt;Minimizing the log likelihood will be exactly minimizing the sum of squares error function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam question:&lt;/strong&gt; Given a different stochastic model, derive the maximum likelihood solution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exam question:&lt;/strong&gt; L2 regularized least squares is the MAP estimate of w with a prior of &lt;script type=&quot;math/tex&quot;&gt; p(w) \propto e^{\frac{\lambda}{2} \parallel w \parallel^2} &lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;locally-weighted-linear-regression&quot;&gt;Locally Weighted Linear Regression&lt;/h2&gt;
&lt;p&gt;Weighted regression where the weights are dependent on x (query point), and you solve linear regression for each query point x. The wider the Gaussian, the less complex the model. Note that you’ll need to refit the regression for every single query that comes in.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title></title>
   <link href="/eecs586/2015/01/12/eecs586/"/>
   <updated>2015-01-12T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/12/eecs586</id>
   <content type="html">&lt;h2 id=&quot;back-to-sequential-search&quot;&gt;Back to Sequential Search&lt;/h2&gt;
&lt;p&gt;Remembering our example from last time, we talked about how making assumptions on sequential search was the only way to find various running times, especially average. What if all items don’t have the same probability? Let’s suppose there’s some probability distribution on key requests, and requests are independent. How do we choose items to look at?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sort keys by probability, decreasing order. Prove this is correct.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Proof (extremely questionable):&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Let O be the optimal ordering, O’ be ordering by probability order.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Suppose there is a case where one key has a higher probability and is farther in the search. If we switch these, we’ve decreased expected search time. Therefore, we never put the ordering out of sorted order in an optimal ordering.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;It is most likely, however, the the distribution will be uniform. Additionally, in many scenarios the item is not likely to be requested. Let’s look at an example:&lt;/p&gt;

&lt;p&gt;Zipf’s law makes statements about the probability of words in English, based on empirical research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;the&lt;/em&gt;: 6.18%&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I&lt;/em&gt;: 1.17%&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;you&lt;/em&gt;: 0.83%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pareto asked a similar question, but about distribution of income. They both found that the ith most likely word occurs &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{i}&lt;/script&gt; times as likely as the most likely word. Let’s look at randomly choosing A, B, or space on a keyboard. We can look at expected length of the word:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;len(0) = .33&lt;/li&gt;
  &lt;li&gt;len(1) = .66 * .33&lt;/li&gt;
  &lt;li&gt;len(i) = (.66)^i * .33&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; what Zipf looked at though. The sequence is not geometric because we must account for various transpositions of letters. See the CTools paper for full paper. The important observation of this is that &lt;strong&gt;in computer science, heavy tailed distributions are much more likely than in other disciplines.&lt;/strong&gt; This occurs because in CS many things are built up over time.&lt;/p&gt;

&lt;p&gt;The number of expected items examined will generally be calculable, but in cases where we don’t know the symbolic error we’ll need to learn the distribution. We’ll look at different caching schemes. Easy solution: back to front, just move cache around when new requests come in. Slightly better: count heuristic, keep track of counts. &lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Linear Regression</title>
   <link href="/eecs545/2015/01/12/eecs545/"/>
   <updated>2015-01-12T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/12/eecs545</id>
   <content type="html">&lt;h2 id=&quot;updated-grading-policy&quot;&gt;Updated Grading Policy&lt;/h2&gt;
&lt;p&gt;Now midterm and final, instead there’s a Kaggle based project. We can still do a project if we want. Talk to Kostas about this one to replace the Kaggle with research.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;We’ll use x for data, &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; for features, and y or t for targets. &lt;/p&gt;

&lt;p&gt;General formulation, learn the function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; y(x,\mathbf{w}) = \sum_{j=0}^M w_jx^j = w_0 + \sum_{j=1}^{M-1}w_j \phi_j(\mathbf{x}) &lt;/script&gt;

&lt;p&gt;This function is linear (why is this so?) Note that the basis functions do not need to be linear. The basis could be polynomia, or Guassian, or Sigmoidal, or whatever. We want to minimize some objective function, eg SSE. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch gradient descent&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \nabla E(w) = \sum_{n=1}^N (w^T \phi(x_n) - t_n)\phi(x_n) &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; w := w - \eta \nabla_w E(w) &lt;/script&gt;

&lt;p&gt;Also popular is &lt;strong&gt;stocastic gradient descent&lt;/strong&gt;, which is online. We also have the option of using the &lt;strong&gt;closed form solution&lt;/strong&gt;, which is only possible if we can do batch gradient descent. The closed form solution is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \hat w = (X^T X)^{-1} X^T y &lt;/script&gt;

&lt;p&gt;Note that X^T X is invertible. Also important is that we scale inversion on number of features, not number of datapoints.&lt;/p&gt;

&lt;h2 id=&quot;intro-to-regularization&quot;&gt;Intro to Regularization&lt;/h2&gt;
&lt;p&gt;How do we choose the degree of the polynomial for least squares? We can regularize, where we penalize for high amounts of “bend” or “magnitude” in the weights. Quick discussion on L2 vs L1 regularization and the sparsity that L1 provides.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Information</title>
   <link href="/eecs586/2015/01/07/eecs586-intro/"/>
   <updated>2015-01-07T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/07/eecs586-intro</id>
   <content type="html">&lt;h2 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; Quentin Stout&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Term:&lt;/strong&gt; Winter 2015&lt;/p&gt;

&lt;h3 id=&quot;school-description&quot;&gt;School Description&lt;/h3&gt;
&lt;p&gt;Design of algorithms for nonnumeric problems involving sorting, searching, scheduling, graph theory and geometry. Design techniques such as approximation, branch-and-bound, divide-and-conquer, dynamic programming, greed and randomization applied to polynomial and NP-hard problems. Analysis of time and space utilization.&lt;/p&gt;

&lt;h3 id=&quot;meets&quot;&gt;Meets&lt;/h3&gt;
&lt;p&gt;Monday/Wednesday 12:00pm - 1:30pm in EECS 1500&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs586/2015/01/07/eecs586/"/>
   <updated>2015-01-07T00:00:00-05:00</updated>
   <id>/eecs586/2015/01/07/eecs586</id>
   <content type="html">&lt;h2 id=&quot;course-logistics&quot;&gt;Course Logistics&lt;/h2&gt;
&lt;p&gt;This class is more about design of algorithms than analysis, but many times you’ll need to prove you can’t do better. Median grade is a high B+, grading is based on homework and a final. Everything must be typed!&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Algorithms are as important as the hardware. Mostly in this class we’ll determine bounds on algorithms, for example via the intergral test. &lt;/p&gt;

&lt;p&gt;Small o: f is an upper bound but not very good, generally little o isn’t very helpful.&lt;/p&gt;

&lt;h2 id=&quot;o-notation&quot;&gt;O-notation&lt;/h2&gt;
&lt;p&gt;There are a few ways to describe runtimes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Theta(n)&lt;/script&gt;: Order of, gives high and low bounds&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;: Order most of, gives high bound&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Omega(n)&lt;/script&gt;: Order at least, gives low bound&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;o(n)&lt;/script&gt;: Order less than, generally gives less information, think of as &amp;lt; instead of &amp;lt;=&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\omega(n)&lt;/script&gt;: Order greater than, same as small o&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bestworstavg-case&quot;&gt;Best/Worst/Avg Case&lt;/h2&gt;
&lt;p&gt;Sequential search: something is holding data, we can only go sequentially, trying to find a value.&lt;/p&gt;

&lt;p&gt;Suppose: &lt;/p&gt;

&lt;p&gt;A(1:n) of some key type (=,&amp;lt;,&amp;gt;)
x of same key type&lt;/p&gt;

&lt;p&gt;Is x in A?&lt;/p&gt;

&lt;h4 id=&quot;notation&quot;&gt;Notation&lt;/h4&gt;
&lt;p&gt;y cand z = if y false then false
            else z&lt;/p&gt;

&lt;p&gt;Suppose we just search straightforward. Loop and look. What’s the runtime? &lt;/p&gt;

&lt;p&gt;Best Case (Fastest)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; \Theta(1) &lt;/script&gt;
if x present and A(1)=x&lt;/p&gt;

&lt;p&gt;Worst Case (&lt;strong&gt;This is what we care about&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; \Theta(n) &lt;/script&gt;
if x absent or
A(n)=x and x doesn’t appear sooner&lt;/p&gt;

&lt;p&gt;Average Case&lt;/p&gt;

&lt;p&gt;We don’t know without knowing more about the system! We need probability distributions or other known facts. 
If we suppose that prob(i) is the prob that first occurence of x is at i, and we suppose x is present, then we can say expected num of items examined is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \sum_{i=1}^{n} i prob(i) &lt;/script&gt;

&lt;p&gt;Time = &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt; (Num of items examined)&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Information</title>
   <link href="/eecs545/2015/01/07/eecs545-intro/"/>
   <updated>2015-01-07T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/07/eecs545-intro</id>
   <content type="html">&lt;h2 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; Satinder Singh&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Term:&lt;/strong&gt; Winter 2015&lt;/p&gt;

&lt;h3 id=&quot;school-description&quot;&gt;School Description&lt;/h3&gt;
&lt;p&gt;Survey of recent research on learning in artificial intelligence systems. Topics include learning based on examples, instructions, analogy, discovery, experimentation, observation, problem-solving and explanation. The cognitive aspects of learning will also be studied.&lt;/p&gt;

&lt;h3 id=&quot;meets&quot;&gt;Meets&lt;/h3&gt;
&lt;p&gt;Monday/Wednesday 10:30am - 12:00pm in GG Brown room 1504&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs545/2015/01/07/eecs545/"/>
   <updated>2015-01-07T00:00:00-05:00</updated>
   <id>/eecs545/2015/01/07/eecs545</id>
   <content type="html">&lt;h2 id=&quot;class-logistics&quot;&gt;Class logistics&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Bi-weekly homework assignments, in whatever language, potentially Kaggle competitions&lt;/li&gt;
  &lt;li&gt;3 exams (fuck)&lt;/li&gt;
  &lt;li&gt;Use eecs545staff@umich.edu for course related emails&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;about-this-course&quot;&gt;About this course&lt;/h2&gt;
&lt;p&gt;Satinder’s research is reinforcement learning, so we can expect heavier treatment in that area. We will not follow a particular textbook. This class does not require 492, but it &lt;em&gt;does&lt;/em&gt; require linear algebra.&lt;/p&gt;

&lt;p&gt;Projects are possible in lieu of final! Must have group members and must pass interim reports.&lt;/p&gt;

&lt;h2 id=&quot;learning&quot;&gt;Learning&lt;/h2&gt;
&lt;p&gt;Representation is key! The choice of representation can help or hurt. We must also be aware of underlying assumptions that we make, because we can’t learn without assuming something.&lt;/p&gt;

&lt;p&gt;Given a set of inputs &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, .., x_n&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;x_i = (x_{i1}, x_{i2}, .., x_{id})&lt;/script&gt;, we want to learn a function that maps from &lt;script type=&quot;math/tex&quot;&gt;\vec x&lt;/script&gt; into classes.&lt;/p&gt;

&lt;p&gt;Important distinction: f is &lt;strong&gt;not&lt;/strong&gt; a decision boundary, but yields a decision boundary.&lt;/p&gt;

&lt;p&gt;Unsupervised vs Supervised learning, pretty straightforward at this point. Labels vs no labels. Etc.&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;This is Satinder’s shit, listen up kids. &lt;/p&gt;

&lt;p&gt;The goal of RL is to learn an &lt;em&gt;optimal policy&lt;/em&gt; from experience. We want to do this over time and maximize the sum of payoffs over time. How do we deal with partial observability, etc? Will cover POMDP, MDP, other RL parts.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - CloudBurst: Highly Sensitive Read Mapping with MapReduce</title>
   <link href="/eecs584/2014/12/02/eecs584/"/>
   <updated>2014-12-02T00:00:00-05:00</updated>
   <id>/eecs584/2014/12/02/eecs584</id>
   <content type="html">&lt;p&gt;In a surprising article for advanced databases, this paper presents a parallel DNA sequence alignment algorithm that can execute on MapReduce, providing a super-linear(!) speedup over serial alignment algorithms. DNA alignment is a computationally heavy task that’s required to compare genomes between different people or different animals. Because two people’s DNA is not exactly the same, lining up similar segments is a complex task. In some cases there may be errors from the sequencing, and other disparities that require a fuzzy algorithm to properly match sequences. The paper first describes the serial DNA matching algorithm used in most cases, and then expands on the current MapReduce paradigm as a refresher for the user. The algorithm is presented for MapReduce’s sequence alignment, and results are presented.&lt;/p&gt;

&lt;p&gt;In a traditional DNA sequence matching algorithm, the algorithm attempts to align “seeds”, which are segments of the sequence that match perfectly. These seeds are then grown outwards, relaxing the similarity requirements as it goes. The parallel version of this algorithm also uses the “seed-and-extend” method. In this case, the mappers check for seed growth points, and then the reduces grow the seeds, checking to see if there is an appropriate amount of similarity between sequences. Thus, intuitively, the algorithm parallelizes sequence alignment by starting with possible seed matches and then running many possible matches in parallel until only the correct result remains. Most fascinatingly, this algorithm provides super-linear performance gains over the serial version. Moving from 1 to 24 cores results in a 30-fold speed increase, and moving from 1 to 96 cores results in over a 100-fold increase. This suggests that the algorithm may perform better than the state of the art even on a dual core machine. I would have liked to see a comparison of the algorithm running on one machine to a serial version. It’s amazing that the overhead of MapReduce doesn’t contribute much, and that the results are so extraordinary. &lt;/p&gt;

&lt;p&gt;If this paper had a weakness, I would say that it doesn’t do a very good job of explaining the optimizations available to MapReduce. Perhaps using the various memory resident optimizations and locality aware features, the authors could achieve even better performance. While this would be beneficial for publishing, I think the simplicity of the technique as presented is quite powerful and will likely allow researchers to incorporate fast alignment with relatively little overhead and a small learning curve.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Science in an Exponential World</title>
   <link href="/eecs584/2014/12/01/eecs584/"/>
   <updated>2014-12-01T00:00:00-05:00</updated>
   <id>/eecs584/2014/12/01/eecs584</id>
   <content type="html">&lt;p&gt;In this short paper, the authors provide a quick look at the rapid development of big science and the need for databases to support scientific applications. They point out that while in the past scientists would keep a lab notebook of observations, modern scientific toolchains require complex data analysis and terabytes of actual storage. They (correctly in my opinion) point out that the problem is compouding, increasing at an exponential rate as scientists become more familiar with software and hardware simultaneously drops in cost. The strong technical point of this article is the idea that datacubes can provide the analytic power that scientists expect on an interactive timescale for larger sets of data. I agree this is likely true, but I think that in many cases scientists don’t want to commit to learning one of the many databases that’s available, and would rather learn a standard. I think if a standard is accepted that would be great for the science community. It seems that Matlab and more recently Python tend to be computational standards in science. We will see if a database steps up, and if Hadoop comes out on top due to its wide adoption and low cost.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Reivew - Requirements for Science Databases and SciDB</title>
   <link href="/eecs584/2014/11/30/eecs584/"/>
   <updated>2014-11-30T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/30/eecs584</id>
   <content type="html">&lt;p&gt;As scientific endeavors generate more information on larger scales, big projects generally turn to databases as a way to maintain their experimental data. In this paper, the authors introduce SciDB, a database designed to accommodate scientists and their “big science” endeavors. The authors of the paper met with various members of large scientific projects, including the Large Hadron Collider and the Large Synoptic Survey Telescope. Also present were members of the industry, who were included ostensibly to increase the scope of the project but likely provided the majority of the funding for the database. The paper discusses the various features a database for scientists would require. Because the various scientific disciplines require a wide variety of data representations, it’s difficult to include everyone’s feature requests. In the end, SciDB appears to be a Matlab style “matrix” storage system, that allows for various dimensions of arrays, including arrays that are jagged to support structures such as graphs and other exotic designs.&lt;/p&gt;

&lt;p&gt;While previous work has been done on scientific databases, it appears that all of the previous efforts failed in an attempt to be everything to everyone. Thus, some features of SciDB are left to the end user in hopes that they can support most people. Two important features of SciDB are quite novel, and powerful tools for the scientific community. The first is non-destructive edits, meaning that previous values are saved to allow scientists to see all experimental results. While other tools like BigTable have accommodated a time slice, this tool allows easier management by including an attribute titled “updatable” to arrays. Additionally, SciDB supports uncertainty in its queries. Thus, error bars (assuming a normal distribution) are computed and saved throughout the computation, allowing the user to keep track of precision easily and throughout the database chain. This is one of the concepts behind BlinkDB, although in Blink the error bars are specifically computed by the sampling interface and in SciDB they may be added by the user. Extending SciDB to include sampling may have been a better idea than extending BlinkDB to include matricies. &lt;/p&gt;

&lt;p&gt;The error bars also present a weakness in that they are parametric, and they assume that the underlying distribution is normal. It appears that the user is able to extend the error bar handling, which may be able to alleviate the problems. It’s also worth noting that this paper had no actual product at the time of release. Although SciDB now exists, it was bold of the authors to release a paper based on theory alone (although the paper is more a proposal than a research product). One more interesting observation is that, somewhat not-unexpectedly, Stonebraker again uses this paper as an opportunity to tell his audience that “Postgres supported features years ago”. While it does get tiresome to see him constantly push Postgres, it is nice to be reminded that object-relational databases exist and in many cases may provide a solution to a problem without building a new database.&lt;/p&gt;

&lt;p&gt;One more thing - the SciDB website treats SciDB like a product even though it’s open source, requring personal information to download. This is not the direction SciDB should be moving if it hopes to achieve widespread adoption.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Join Synopses for Approximate Query Answering</title>
   <link href="/eecs584/2014/11/29/eecs584/"/>
   <updated>2014-11-29T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/29/eecs584</id>
   <content type="html">&lt;p&gt;This paper is quite similar the BlinkDB, and as such I will gloss over some technical aspects even though this paper was published first. Again, creating a query system that can answer with approximate answers is a powerful solution to large data warehousing and data mining. By allowing interactive queries (meaning queries that run in near real time), the user is hidden from the complexities of dealing with massive amounts of data as long as they remember their answers aren’t perfect. In this paper the authors introduce join synopses for relational databases, and show that by using these join synopses a system can answer approximate queries with foreign key joins.&lt;/p&gt;

&lt;p&gt;The paper introduces the problems with joining random samples, and shows that by doing so the results of the join will be poor. This is because (1) the results of a sample before the join and of a sample after the join are not the same and (2) joining random samples will result in even smaller outputs which reduce confidence in the results. Like BlinkDB, this paper proposes a sampling method based on distinguished samples. Unlike BlinkDB, Aqua is designed to compute “join synopses”, which work to allow any arbitrary join. BlinkDB, on the other hand, samples from tables directly and does not specifically support joins. &lt;/p&gt;

&lt;p&gt;The authors also introduce a method of computing confidence bounds by extracting sub-samples from samples. To me this sounds just like bootstrapping, and I wonder why this terminology wasn’t used in this paper and how this is different from the bootstrapping that BlinkDB does. I suspect that this is simply a change in terminology that occurred over the years. While bootstrapping is not a new technique, it has gained popularity in recent years. I did like that this paper offered some theoretical insight into its techniques and provided proofs of the join’s correctness (although not fully here they do outline the basics and refer the reader to the full version of their paper). Because my project for this class involves modifying BlinkDB, I had trouble finding the actual mathematical underpinnings of the work (the references simply send the reader to a textbook). One obvious strength of the BlinkDB system is that its designed for distributed computing, while the Aqua system is overlaid on a commercial DBMS that is likely not designed to be shared-nothing. It remains to be seen as to whether or not the system can work atop a shared-nothing architecture such as Teradata. Part of this is because the paper is old enough that shared-nothing DBMS systems were not on the forefront of computing. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data</title>
   <link href="/eecs584/2014/11/28/eecs584/"/>
   <updated>2014-11-28T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/28/eecs584</id>
   <content type="html">&lt;p&gt;As data warehouses grow larger, the amount of time required to process queries begins to outpace the development of the hardware and software that runs the system. Ethernet links and hard-drives have limits on speed, and interactive queries become impossible in petabyte scale systems on commodity hardware. To remedy this, the authors of this paper propose a sampling based system. The core concept of BlinkDB is that by sampling the tables of a database, the system can return approximate answers to queries. These answers may not be exact, but should be close enough for many OLAP and data mining tasks. For example, if we wanted to calculate the average income of a town, we could sample the residents to get an approximate answer, which in many cases will be close enough. The paper also stresses the idea of error bounds for the returned results. This is important because a user wants to know how close their answer is to the true result. By returning error bounds the user can keep in mind the statistical consequences of their actions. &lt;/p&gt;

&lt;p&gt;BlinkDB is built atop two key ideas: a stratified sampling method and a dynamic sample selection strategy. The first important contribution is stratified sampling. Because the underlying samples may be inadequately distributed, BlinkDB maintains a set of stratified samples over the original data. Stratified sampling is the technique of sampling from different groups unequally in order to reduce bias. The authors do this because if some members of a subset are rare, we will be unable to construct accurate estimates over them if we sample indiscriminately. BlinkDB also introduces optimizations that allow the user to specify the latency or accuracy of a query in SQL. For example, a user can declare they would like results WITHIN 5 SECONDS or ERROR WITHIN 10% AT CONFIDENCE 95%. The paper outlines a method in which multiple samples are kept of the data, and the appropriate sample set is used to properly answer a query. Additionally, BlinkDB must keep an error latency profile of various queries in order to estimate the time cost of a query - the software uses both an error and latency profile for this. The system itself is built atop Spark and Hive, intercepting queries in the Shark driver to apply Hive UDAFs to the data. &lt;/p&gt;

&lt;p&gt;The strength of this paper lies in its stratified sampling method and the error latency profile used to estimate cost of a query. The results are promising, and sampling a database allows for interactive query sessions on databases that are otherwise too large. The system also is quite powerful because it’s built atop Spark, a memory resident extension to the distributed computation ecosystem Hadoop. The paper does have some weaknesses not addressed in this revision. One major weakness is that the aggregate functions must assume a normally distributed super-sample because they rely on closed-form estimators. Later versions of the paper introduce a bootstrapping method to deal with non-parametric datasets. Unrelated to the paper, the actual code released by the project is quite poor, with few features implemented and many discrepancies to the original work. This makes it impossible to duplicate the results given in the paper.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Complex Decisions and Learning</title>
   <link href="/eecs492/2014/11/25/eecs492/"/>
   <updated>2014-11-25T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/25/eecs492</id>
   <content type="html">&lt;h2 id=&quot;partially-observable-markov-decision-processes&quot;&gt;Partially Observable Markov Decision Processes&lt;/h2&gt;
&lt;p&gt;Aspects of a POMPDP:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transition model&lt;/li&gt;
  &lt;li&gt;Actions&lt;/li&gt;
  &lt;li&gt;Reward function&lt;/li&gt;
  &lt;li&gt;Sensor model (new to POMDP)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a POMDP, we don’t know the state of the work or the agent. In this case, we can make the gridworld a POMDP simply by removing the knowledge of our location. In a POMDP, a &lt;strong&gt;belief state&lt;/strong&gt; is a probability distribution over possible states. We want to determine the optimal action by mapping belief states to actions. Notice that this doesn’t rely on the actual state.&lt;/p&gt;

&lt;p&gt;How can we update our belief states? The probability of perceiving e, given action a is performed in belief state b:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(e \mid a,b) = \sum\limits_{s'} P(e \mid a,s',b)P(s' \mid a,b) &lt;/script&gt;

&lt;h2 id=&quot;decisions-with-multiple-agents&quot;&gt;Decisions with Multiple Agents&lt;/h2&gt;
&lt;p&gt;Draws from game theory in many cases. For example: prisoner’s dilemma. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dominant strategy: We always testify because it minimizes our losses deterministically, but this assumes that every outcome is equal.
    &lt;ul&gt;
      &lt;li&gt;Strongly or weakly dominates&lt;/li&gt;
      &lt;li&gt;Pareto optimal, pareto dominated&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Equalibriums exist in games, in this case if both testify that’s an equalibrium. John Nash proved that all games have at least &lt;em&gt;one&lt;/em&gt; equalibrium.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - Efficient Query Evaluation on Probabilistic Databases</title>
   <link href="/eecs584/2014/11/24/eecs584/"/>
   <updated>2014-11-24T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/24/eecs584</id>
   <content type="html">&lt;p&gt;Databases, especially for OLAP and business intelligence workloads, are well established but not very flexible for users. They rely on rigid schema and precise queries to retrieve results. On the other hand, information retrieval services such as Google are good at retrieving relevant information, but lack the structure of a database. For example, Google can correct spelling errors, allow for inaccurate queries, and can generally intuit what the user is asking for. This paper seeks to unify those opposing ideas by creating a probabilistic database that can rank the results in order of most relevant instead. This new paradigm would allow a user to query a database by asking less precisely what they want - for example a user could ask for “all good mystery novels from the 1990s” and receive a ranking of the results. By enabling these queries, an analyst could more easily analyze data from a large OLAP database, freed from the usual constraints of formulating just the perfect query to get the results he or she wants.&lt;/p&gt;

&lt;p&gt;The database described in this paper uses the idea of “possible worlds semantics”, which represents each possible database state along with its probabilities. The database assigns a probability to each tuple and uses these values to compute the probability of the tuples in the answer. Unfortunately, the probabilities computed in the naive way are wrong - the events are conditional on each other in unknown ways. Thus, the databases rewrites the query plan, searching until it finds one that is correct. This process is extremely powerful because it works on any SQL query with approximate predicates, and includes joins, nested sub-queries, aggregates, etc. The paper is quite mathematically rigorous, and provides a relational algebra for approximate operators, as well as formal proofs and theorems for much of the author’s work. One strength within this work is the section of query optimization, which provides transformation rules for this probabilistic algebra within a database. As a foundation for future projects, this work would appear quite influential.&lt;/p&gt;

&lt;p&gt;The paper also includes methods for dealing with the corner cases where a query has Sharp-P-complete complexity and cannot be solved in polynomial time. In these cases, the paper introduces probabilistic workarounds in the form of either error minimizing heuristic or Monte-Carlo simulation. The paper also (poorly) describes the prototype database that the authors implemented. The proposed database operates as a middleware to the RDBMS (in this case Microsoft SQL Server) and reformulates probabilistic queries into “extensional” SQL queries. This section seems to be a weakness of the paper, as both the experimental setup and results are fairly lacking compared to the rest of the journal article. Additionally, if we look at the results of runtime, the jump in time required to utilize this database is high - in some cases 1000 fold from safe plan to bare query. I would be interested to see a proper commercial implementation of this database to compare this paradigm to traditional RDBMS and information retrieval solutions. Obviously this is far outside the scope of the paper.&lt;/p&gt;

&lt;p&gt;One interesting idea would be extending the probabilistic database to fit other database designs. For example, supporting an object-relational model such as Postgres. Doing so would be enable approximate queries on a larger variety of data, such as geospatial or coordinate information.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Decision Analysis</title>
   <link href="/eecs492/2014/11/18/eecs492/"/>
   <updated>2014-11-18T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/18/eecs492</id>
   <content type="html">&lt;h2 id=&quot;some-more-stats&quot;&gt;Some more Stats&lt;/h2&gt;
&lt;p&gt;Covariance relates variance of multiple random variables. Remember that all the information of a Gaussian is contained in its exponential term. Covariance is a positive definite symmetric matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Central Limit Theorem&lt;/strong&gt; - The distribution of sum of N independent and identically distributed random variables approaches a normal distribution.&lt;/p&gt;

&lt;h2 id=&quot;decision-analysis&quot;&gt;Decision Analysis&lt;/h2&gt;
&lt;p&gt;Rational agents maximize utility - utility captures preferences. We need to formalize utility:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;P(Result=s’&lt;/td&gt;
      &lt;td&gt;a,e) is the probability of an outcome s’ given evidence e.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We also want to define a utility function U(s) that captures the agent’s preferences (this is a single number). Additionally we want the expected utility of an action, which we can define as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; EU(a \mid e) = \sum P(Result(a) = s' \mid a,\textbf{e})U(s') &lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - CryptDB: Processing Queries on an Encrypted Database</title>
   <link href="/eecs584/2014/11/17/eecs584/"/>
   <updated>2014-11-17T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/17/eecs584</id>
   <content type="html">&lt;p&gt;Privacy is an ever-increasing concern, especially in today’s always connected and cloud based world. Companies have databases with terabytes of consumer data, many of which are exploitable and unsafe. Even in cases where exploitation is not easy, there are always additional zero-day vulnerabilities (such as Heartbleed and Shell Shock) that present security threats and challenges. Thus, an encrypted database is a powerful guarantee that a user’s data will remain secure, even in the event of internal compromise. This paper introduces CryptDB, a database proxy layer and extension that secures existing databases. The ideas presented are designed to work over any commercial database, although the authors use MySQL.&lt;/p&gt;

&lt;p&gt;CryptDB’s primary contribution is the idea of multiple security layers to process queries and expose only as much information is required to get a result to the DBMS. Thus, the DBMS actually has no idea what data it is storing, relying on a secured proxy to handle interactions with the user. The authors use a large variety of existing security ciphers, providing completely random encryption, order-preserving encryption, homomorphic (function preserving), word search, and join encryption. Of special note, the OPE exposes some data to the client (up to half in the worst case). The homomorphic encryption is only additively homomorphic (fully homomorphic encryption is still in its infancy). The join encryption is specially designed for CryptDB and introduces a primitive called JOIN-ADJ (adjustable join) to support joining columns unknown apriori by the proxy. The proxy operates on a per-user basis and provides a separate session for each user so that a compromised key cannot expose the entire database. This design a strong because it also prevents a malicious user from exposing the database in a cloud system where users are not fully trusted. CryptDB is built atop mysql-proxy, and includes some UDFs to the underlying MySQL implementation.&lt;/p&gt;

&lt;p&gt;This raises the first point of vulnerability and weakness with the system - if the underlying operating system is the vulnerable system, the proxy is most likely going to be affected as well. The second weakness is that simply using the system will resolve more and more of the underlying information to the DBMS (as additional security layers are required) and will eventually expose almost all of the data eventually. This is because the order-preserving encryption leaks up to half of the data bits in the worst case. Thus, a workload could in theory expose the entire database with enough queries. The authors claim to be working on this problem and if they can fix it this will be a big improvement. Still though, eventually the lowest level of encryption will be used on all the data with enough workload.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - l-Diversity: Privacy Beyond k-Anonymity</title>
   <link href="/eecs584/2014/11/16/eecs584/"/>
   <updated>2014-11-16T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/16/eecs584</id>
   <content type="html">&lt;p&gt;As both industry and the sciences begin to leverage the vast amounts of data at their disposal, privacy becomes more important. As more data is published, it becomes more and more vital to anonymitize data that is released for study. The idea of k-Anonymity addressed this problem. K-Anonymity makes guarantees that each record is indistinguishable from at least k-1 other records with respect to certain attributes. The idea of k-anonymity is simple - for any item there must be k items that have the same field. While easy, this scheme presents problems.&lt;/p&gt;

&lt;p&gt;The authors explain these problems and introduce their own notion of anonymity called l-diversity. The first problem occurs when certain attributes are too homogeneous. In this case, it may be possible to infer data simply based on elimination - if we know all but one attribute and that attribute is not diverse, then we can simply infer the result. The second problem occurs when an attacker has external knowledge of the system. For example, if two attributes only occur together then we can make assumptions about identity even in k-anonymous tables. The paper doesn’t state, but implies, that this external knowledge need not be domain knowledge - with sufficient computational resources an attacker could use pattern recognition techniques to find data that occurs together.&lt;/p&gt;

&lt;p&gt;The key insight between l-diversity is that (1) the attributes cannot be too homogeneous or distinct and (2) the attributes cannot be inferred by l-1 pieces of damaging background knowledge. The authors do a good job of quantifying and formalizing these intuitions. The paper uses Bayes-Optimal to model background knowledge as a probability distribution. A table is l-diverse if there are at least l well-represented values for a sensitive attribute. The paper further defines several ways to formalize “well-diverse”. Entropy based diversity draws from information theory (remember that entropy is the “number of bits” required to represent a piece of data) to characterize diversity of an attribute. The authors also introduce a less-strict version they call recursive diversity. The paper finally offers algorithms and performance evaluation of l-diversity.&lt;/p&gt;

&lt;p&gt;While l-diversity is a powerful formalization of anonymization, the paper does have some weaknesses. It seems as though it would be difficult to calculate an l-diverse table for many sensitive fields because of how many constraints are present in the scheme, especially for entropy-based diversity. I do like that this paper provides such strong theoretical guarantees, and I find that a major strength of this paper. Still, I could see it being difficult to convince the general public to use this method because it is much more difficult to grasp than k-anonymity. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - RemusDB: Transparent High Availability for Database Systems</title>
   <link href="/eecs584/2014/11/15/eecs584/"/>
   <updated>2014-11-15T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/15/eecs584</id>
   <content type="html">&lt;p&gt;Highly available database systems are becoming more important and companies and services trade-off consistancy for availability. This paper presents RemusDB, a software layer designed to introduce high availability in commodity database systems. It does this via virtual machine, placing the host database on a VM and replicating the VM to provide failover. This design allows any database to be used as the underlying system, freeing database designers from the complexities of high availiability. &lt;/p&gt;

&lt;p&gt;The Remus system is an established failover system for virtual machines that run on the Xen hypervisor. Remus checkpoints the VM at various times, capturing the complete state of the device. On the event of a failure, the client is forwarded to the new system, which has an exact (or near exact) copy of the old VM. Thus, the client sees no interruption of service and continues on like nothing happened. While a powerful tool, Remus presents problems when running a database system. Specifically, large amounts of memory must be maintained. Additionally, RemusDB reduces latency added to the client-server communication path by the failover mechanisms. The primary insight of this paper is the idea of compressing checkpoints to transfer less data during replication. Because databases generally maximize memory usage to provide faster performance via caching, this idea is important for database workloads. RemusDB also selectively ignores certain parts of memory that can be reconstructed by the backup machine, again reducing memory transfer. In theory this would make the failover slower, but the authors don’t mention this at all.&lt;/p&gt;

&lt;p&gt;The paper also changes the underlying network code to work around Remus’ network buffering. Remus holds packets until a checkpoint is completed, potentially disrupting database service. While required for RemusDB to work, this part of the paper seems to me by far the weakest. The authors actually change the underlying database as well as the linux kernel itself to allow the DBMS to selectivly decide which packets can be buffered until the next checkpoint. While I understand the motivation behind this, it to me seems to defeat the entire purpose of RemusDB, which is to lie atop the hypervisor to provide availability. The authors claim that any DBMS should work with the system, but with this modification we will require a new “RemusDB approved” version of the DBMS to actually implement any of the systems. &lt;/p&gt;

&lt;p&gt;One final thing to note is that consistancy is still mostly achieved even though we have a highly available system. RemusDB is 2-safe, meaning that updates will not be lost in the event of a failover. This is possible because transaction commits are not acknowledged until both systems have recorded the update. One interesting discussion would be how this impacts the CAP theorem, as we must trade availability for consistency in some form.  &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Relational Cloud: A Database-as-a-Service for the Cloud</title>
   <link href="/eecs584/2014/11/14/eecs584/"/>
   <updated>2014-11-14T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/14/eecs584</id>
   <content type="html">&lt;p&gt;As more webservices are moved to the cloud, companies find themselves considering databases as a cloud service. In theory it makes sense - much of the cost of a database is on startup and administration. License fees and competent database administrators are expensive, but generally don’t scale as the database does. Although on paper DBaaS makes sense, there are problems for both the database owners and the users. Database performance can be expensive in this model, requiring more money from the cloud provider. Additionally, companies don’t trust random companies with their sensitive data. This paper introduces the Relational Cloud, a database designed as a service that attempts to alleviate some of these concerns. Note that at the time of this writing the DBaaS was just gaining traction, with Amazon and Microsoft’s offerings just taking off the ground. At the time of this review, there are many competing DBaaS providers, including multiple paradigms and types of databases.&lt;/p&gt;

&lt;p&gt;Relational cloud presents a few novel ideas. The first is that the database attempts to partition databases by machine, in order to better group tenants that can coexist on a machine. The second, related to this, is the use of a graph-based data partitioning algorithm to scale out complex workloads. The final contribution is an encryption layer that can operate on encrypted data and secure the user’s information. This last contribution is an extremely powerful idea, and is later extended into a whole separate series of papers (CryptDB). I think the strongest contribution of this paper is in CryptDB, as privacy concerns are more and more relevant not only for corporations but also individuals. I believe that in the future we will see privacy in cloud services becoming a strong selling point, and the ability to operate on encrypted data is quite powerful.&lt;/p&gt;

&lt;p&gt;To partition data, Relational Cloud uses a strategy based on workload. By looking at previous workloads, the analyzer attempts to partition the database in a way that minimizes the number of distributed transaction. This allows better performance, and because the operation is done at the database level we can avoid expensive virtual machines to contain the databases. Relational cloud uses an engine called Kairos to allocate resources across physical machines. Kairos uses non-linear optimization to minimize the number of machines required to support a workload, as well as balance load across machines. I think this paper should have included more information about Kairos, but the authors instead decided to publish a separate paper. For encrypted data transactions, the database supports several levels of encryption. The authors call this “adjustable security”, meaning that they can choose how secure a tuple is based on what is required. Weaker encryption guarantees are made for tuples that must support operations, such as sorting and inequality checks. Each row is encrypted into an “onion”, meaning that each value in the table is wrapped in layers of increasing encryption. &lt;/p&gt;

&lt;p&gt;For me, the biggest weakness of this paper is the number of offshoots it requires for understanding. The paper contains two powerful concepts, but offloads each of them into a separate paper. While the motivations behind this may be reasonable (each is a large subject), it makes it much harder to understand the system. I also would have liked a few more results, for example the performance of encryption on non-OTLP loads. I assume this analysis is contained within the other offshoot papers.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Continuous Probability</title>
   <link href="/eecs492/2014/11/13/eecs492/"/>
   <updated>2014-11-13T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/13/eecs492</id>
   <content type="html">&lt;h2 id=&quot;more-bayesian-networks&quot;&gt;More Bayesian Networks&lt;/h2&gt;
&lt;p&gt;Important types of networks:&lt;/p&gt;

&lt;h4 id=&quot;singly-connected-network&quot;&gt;Singly Connected Network&lt;/h4&gt;
&lt;p&gt;At most one undirected path between any node pair. Space and time complexity is linear in size of network.&lt;/p&gt;

&lt;h4 id=&quot;multiply-connected-network&quot;&gt;Multiply Connected Network&lt;/h4&gt;
&lt;p&gt;Multiple undirected paths between nodes. Space and time complexity is exponential in size. If we identify a set of variables that will render a network singly connected we can split apart the network to create a singly connected network. We could also transform a network into a “polytree” where we join individual nodes to form cluster nodes. &lt;/p&gt;

&lt;h2 id=&quot;approximate-inference&quot;&gt;Approximate Inference&lt;/h2&gt;
&lt;p&gt;Many times the network grows out of control, so we have to estimate. Our goal is to generate samples from a known distribution. &lt;strong&gt;Direct sampling&lt;/strong&gt; is the easiest way to do this. We just sample many times and eventually get a close approximation to our real network. If our distribution is hard to sample, we can use &lt;strong&gt;rejection sampling&lt;/strong&gt; to compute conditional probability. This method rejects samples that don’t match the evidence. Instead of calculating the full joint probability, we only calculate the probability for specific relations. In rejection sampling, as your conditionals become more rare, accuracy rapidly falls. &lt;/p&gt;

&lt;p&gt;Is there a better way? Yes, we can use &lt;strong&gt;liklihood weighting&lt;/strong&gt;. In this method, we only want to generate samples consistent with the evidence. We’ll fix the values for evidence and sample only non-evidence variables. &lt;/p&gt;

&lt;h2 id=&quot;continuous-probability-distributions&quot;&gt;Continuous Probability Distributions&lt;/h2&gt;
&lt;p&gt;Basics of continuous probability here: integrating under curve is 1, probability at exactly one point is zero, etc etc. In continuous probability, we use &lt;strong&gt;expectation&lt;/strong&gt; to discuss a variable’s value. &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; E[x] = \int_{- \infty}^{\infty} xP(x) dx &lt;/script&gt;

&lt;p&gt;Expected value is average, but average only thinks of uniform distribution. Basic properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; E[\alpha] = \alpha &lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; E[\alpha x] = \alpha E[x] &lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; E[\alpha + x] = \alpha + E[x] &lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt; E[x+y] = E[x] + E[y] &lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Expected value is a linear operator which is just peachy. However, expected values are not unique, we want a measure of how spread the data is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
 \begin{align*}
\sigma^2 &amp;= E[(x-E[x])^2] \\
         &amp;= E[x^2] - 2x(E[x]) + (E[x])^2 \\
         &amp;= E[x^2] - (E[x])^2
   \end{align*}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;Also introducing a Guassian distribution. These are nice because we know the values based on variance and mean. It’s also its own conjugate prior (it’s the exponential distribution of the statistics world). Of course we can have multiple dimensions of random variables, which we can talk about via correlation. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Inference</title>
   <link href="/eecs492/2014/11/11/eecs492/"/>
   <updated>2014-11-11T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/11/eecs492</id>
   <content type="html">&lt;h2 id=&quot;types-of-graph-structures&quot;&gt;Types of Graph Structures&lt;/h2&gt;
&lt;p&gt;Last time we talked about tail to tail models. This time we discuss “head to tail” relationships. In head to tail we see a-&amp;gt;c-&amp;gt;b. We can use this knowledge to show conditional independence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(a,b \mid c) = \frac{P(a,b,c)}{P(c)} = p(b \mid c)p(a \mid c) &lt;/script&gt;

&lt;p&gt;Which shows conditional independence between a and b.&lt;/p&gt;

&lt;h2 id=&quot;inference-by-enumeration&quot;&gt;Inference by enumeration&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(X \mid e) = \alpha P(X,e) = \alpha \sum\limits_y P(X,e,y) &lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;\alpha = \frac{1}{P(e)}&lt;/script&gt;. We sum over the hidden variables so introduce them into our equation.&lt;/p&gt;

&lt;p&gt;Computationally this enumeration is hard because we must sum over so many variables. &lt;/p&gt;

&lt;h2 id=&quot;variable-elimination&quot;&gt;Variable Elimination&lt;/h2&gt;
&lt;p&gt;Given a bunch of components we want to evaluate from the right to left side so we don’t waste calculation. This is online.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Get Another Label? Improving Data Quality and Data Mining Using Multiple, Noisy Labelers</title>
   <link href="/eecs584/2014/11/10/eecs584/"/>
   <updated>2014-11-10T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/10/eecs584</id>
   <content type="html">&lt;p&gt;While crowdsourcing is a powerful tool for researchers who need a human intelligence for their data, it can be quite expensive to hire individuals for a large data set. This paper explores the cost model of data labeling, and looks at the most effective way to partition funding when accuracy is required. In cases of unlabeled or partially labeled data, gaining a few key labels is quite important. However, because of the cost of those labels, most systems attempt to label only the most important points. This compounds problems when a label coming in is wrong - falsely labeling support vectors in an SVM will reduce accuracy dramatically.&lt;/p&gt;

&lt;p&gt;The paper first looks at the idea of repeated labeling. By asking many people to label one data point and taking a vote, most inaccuracy can be removed from the new points. The paper asserts that the probability of a label appearing in the system changes how well the system responds to hand labeled data. In cases where the label is more uncertain, labeling data multiple times improves accuracy dramatically. The paper also discusses the idea of preserving uncertainty in labeling via majority vote. Because some classifiers can use the information about label uncertainty in their decision or training functions, by saving the results of the vote we can increase accuracy (ie a label where all voters agree is a more powerful label than one where one side wins by a few votes). &lt;/p&gt;

&lt;p&gt;I like the paper’s discussion of selective repeated-labeling. The intuition is that while repeated labeling can improve accuracy, if we can allocated our total label requests towards less confident samples, we can improve quality without requiring more human requests. That is, we will take some of the repeated labels away from points we’re certain about, and move them towards points we aren’t. The paper presents a method of estimating label uncertainty that avoids natural purity measures such as entropy. The paper concludes with a fairly comprehensive results section where the authors use mostly synthetic data to test their hypothesis. I wish the authors had used real data instead of introducing noise to existing data benchmarks, as the practically of the system is hard to assess under those conditions.&lt;/p&gt;

&lt;p&gt;One weakness of the paper was the assumption of a homogenous environment. Although the authors do relax some assumptions throughout the paper, they fail to account for the possibilities of a real world crowd. The authors mention the failings, specifically that a higher reward may correspond to a better label (so paying more might be a better option than simply repeating many times) and that label difficulty is not static. This paper does provide a strong framework for further exploration into crowd-based reinforcement learning. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - CrowdDb: Answering Queries with Crowdsourcing</title>
   <link href="/eecs584/2014/11/09/eecs584/"/>
   <updated>2014-11-09T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/09/eecs584</id>
   <content type="html">&lt;p&gt;While strides in machine learning have vastly improved the ability of machines to emulate human cognition, there are still many tasks which humans excel or where building the appropriate system would be far too costly. With the connectivity of the internet, new models for using human intelligence have become popular, now called “crowdsourcing”. Crowdsourcing refers to breaking a task into many smaller subproblems, each of which are easily solvable by a human but not a machine. For example, recognizing the object in a photo or the numbers on a house are both easy for humans and generally hard for machines. Amazon’s Mechanical Turk has become a popular platform for researchers and companies who require human intelligence in otherwise automated systems. CrowdDB attempts to provide a database abstraction over mechanical turk to allow otherwise difficult tasks to be automated without a programmer to create the tasks in question.&lt;/p&gt;

&lt;p&gt;CrowdDB introduces an extension to SQL (thankfully not named CrowdQL) which is designed to allow programmers to specify which tasks in the database should be allocated to humans. The paper introduces several tasks that are dispatchable to Amazon, specifically comparison, ordering, and field completion. I think that this is a smart and interesting way to provide interaction with the crowd, as the language separates the crowd sourcing from the database itself. In theory any other crowdsourcing method could be used, including one that is done mainly by machine. The paper also introduces a generic user interface building utility that can automatically generate user interfaces for the crowd based on schema from queries. The authors herald their UI builder as a major contribution, but I think that this is less significant than the other parts of the paper. Automatic UI development has been done before and the user interface itself provides nothing new. The interfaces built are simply form versions of the queries presented. &lt;/p&gt;

&lt;p&gt;This paper shows major weakness by not properly taking accuracy of the crowd into account. The crowd can be made up of people who deliberately want to undermine the task at hand, and the database takes crowd responses at face value. While the authors do address this as a weakness, I think that for any real production system this needs to be addressed, likely in a whole other paper. It was interesting to see the human variability on trust that must be accounted for. The workers can choose who to work for, and undermining their trust will result in poorer results. The need to respond to this automatically would be fantastic.&lt;/p&gt;

&lt;p&gt;While I like the idea of a crowd sourced database, it seems to me that the idea of crowdsourcing has been shoehorned into SQL more than it probably needs to be. I can appreciate the requirement for an abstraction of dispatching tasks to humans, but it almost seems as if SQL is a bad choice of first class citizen for the task. The rigid schema of databases clashes with the vast flexibility of the crowd, and I feel it would be better to see a more general model of crowd dispatch that would then be accessed via SQL, in addition to any other languages that could allow more expression.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Networks</title>
   <link href="/eecs492/2014/11/06/eecs492/"/>
   <updated>2014-11-06T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/06/eecs492</id>
   <content type="html">&lt;h2 id=&quot;more-probability&quot;&gt;More Probability&lt;/h2&gt;

&lt;h3 id=&quot;marginal-distributions&quot;&gt;Marginal Distributions&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; P(Y) = \sum\limits_{z \in Z} P(Y, z) &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We can sum over parts of the over distribution to determine specific attributes. &lt;/p&gt;

&lt;h3 id=&quot;independence&quot;&gt;Independence&lt;/h3&gt;
&lt;p&gt;Independent variables mean that the occurence one event tells you nothing about the occurence of another.&lt;/p&gt;

&lt;h3 id=&quot;bayes-theorem&quot;&gt;Bayes Theorem&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; P(a|b) = \frac{P(b|a)P(a)}{P(b)} &lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;conditional-independence&quot;&gt;Conditional Independence&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; P(X,Y|Z) = P(X|Z)P(Y|Z) &lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;bayesian-networks&quot;&gt;Bayesian Networks&lt;/h2&gt;
&lt;p&gt;In a Bayesian network we assume conditional independence to create our distribution. By using the product rule we’ll be able to build a model of probabilities and we can infer results from the network.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Uncertainty</title>
   <link href="/eecs492/2014/11/04/eecs492/"/>
   <updated>2014-11-04T00:00:00-05:00</updated>
   <id>/eecs492/2014/11/04/eecs492</id>
   <content type="html">&lt;h2 id=&quot;some-final-planning&quot;&gt;Some Final Planning&lt;/h2&gt;

&lt;h3 id=&quot;contingent-planning&quot;&gt;Contingent Planning&lt;/h3&gt;
&lt;p&gt;If we don’t know about a conditional before hand we need to gather information before planning more actions. This leads to continuous replanning where we can’t plan everything beforehand. This is generally how things work in the real world and with real time. In this case we want &lt;strong&gt;online replanning&lt;/strong&gt; to change our action plan as new information comes in. This requires &lt;strong&gt;execution monitoring&lt;/strong&gt; in the real world. &lt;/p&gt;

&lt;h3 id=&quot;multi-agent-planning&quot;&gt;Multi-agent Planning&lt;/h3&gt;
&lt;p&gt;Having multiple agents whose actions depend on each other introduces complexity. There are a few types of multi-actor setting, but the most complicated is &lt;strong&gt;multiagent&lt;/strong&gt; which creates a shared nothing system. Our focus is on multiple simultaneous actions. The way we do this is to decouple actions from actors as much as possible. We want to be &lt;strong&gt;loosely coupled&lt;/strong&gt;. One common way to reconcile joint solutions is to use social laws and communication. An example is in sports - play on your side of the court and communicate when things become questionable (“I got it!”).&lt;/p&gt;

&lt;h2 id=&quot;probability-yay&quot;&gt;Probability (yay)&lt;/h2&gt;
&lt;p&gt;Describes uncertainty. Cool.&lt;/p&gt;

&lt;p&gt;We talk about probability in AI in terms of “possible worlds”, aka the &lt;strong&gt;sample space&lt;/strong&gt;. &lt;/p&gt;

&lt;h3 id=&quot;some-really-chill-axioms&quot;&gt;Some really chill axioms:&lt;/h3&gt;

&lt;p&gt;Probability expressed from 0 to 1:
&lt;script type=&quot;math/tex&quot;&gt; 0 \leq P(\omega) \leq 1 &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Probability sums to 1:
&lt;script type=&quot;math/tex&quot;&gt; \sum\limits_{\omega \in \Omega} P(\omega) = 1 &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Probabilities add for mutually exclusive events:
&lt;script type=&quot;math/tex&quot;&gt; P(a \vee b) = P(a) + P(b) &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Probabilities add but must subtract intersect for non-mutually exclusive events:
&lt;script type=&quot;math/tex&quot;&gt; P(a \vee b) = P(a) + P(b) - P(a \wedge b) &lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;types-of-probabilities&quot;&gt;Types of Probabilities:&lt;/h3&gt;
&lt;p&gt;Priors: Unconditional chances
&lt;script type=&quot;math/tex&quot;&gt;P(X=x)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Conditional Probabilities: 
&lt;script type=&quot;math/tex&quot;&gt;P(a|b) = \frac{P(a \wedge b)}{P(b)}&lt;/script&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - Generic Schema Matching with Cupid</title>
   <link href="/eecs584/2014/11/03/eecs584/"/>
   <updated>2014-11-03T00:00:00-05:00</updated>
   <id>/eecs584/2014/11/03/eecs584</id>
   <content type="html">&lt;p&gt;While the previous paper told us the hassle that schema matching brought the developer, this paper actually presents a working solution to schema matching, developed by Microsoft. As outlined in the survey paper, schema matching is a pervasive and particularly difficult problem that is best performed by domain expert humans, even if the work is tedious and seemingly brain-dead. This paper describes Cupid, a comprehensive schema matching tool that, much like many of the new databases we study in this class, builds its solution on the partially complete work of others instead of designing a fully novel solution. In this sense we can see it almost like the Gamma project in that it took many incomplete or non-commercial ideas and implemented them into one design. &lt;/p&gt;

&lt;p&gt;The paper owes so much of its design to previous schema matchers that it first outlines the various types of matching techniques in use today, followed by the common products available and how they use those techniques. I actually really like this system and wish more papers did this, it presented a very good overview of the previous work in the field. Cupid considers itself most similar to DIKE and MOMIS in terms of general schema matching, and thus compares itself to these packages. Both of these packages are schema matchers that use both element and structure level information. Cupid also seems to mostly take this approach, and so it seems that, like in machine learning, the kitchen sink technique works best in the real world (see Netflix algorithm, etc). The paper outlines the various techniques it uses for both linguistic and structure matching. To deal with similar schema names, Cupid uses a measure of word similarity along with a database of similar words and homonyms to try and match schema that use different but similar labels. For structure matching, Cupid uses a fairly complex system based on structural similarity. This similarity matching uses both the linguistic similarity described previously, as well as the local structure of the schema to make inferences about matching elements. While the first section describes a tree schema, the paper later expands the tree into a graphical model to allow schema to express non-hierarchical possibilities. This map acts as a directed graph, allowing three different types of nodes that can describe one to one, one to many, and inheritance relationships. While no doubt this model is powerful, it introduces a considerable amount of complexity. The paper finally outlines other features and compares Cupid to other schema matchers.&lt;/p&gt;

&lt;p&gt;One weakness of this paper is possibly owed to its release date (2001), but its linguistic model seems quite limited. The paper uses a thesaurus to determine abbreviations and similar words, but these are only useful in an academic sense. A better solution would be to create a translation mapping using real world data, such as something you would use in the Google “did you mean” algorithm. This would allow for the variability of human writing, as well as provide a constantly evolving solution for slang and new words or abbreviations. I’m also surprised that the similarity of the tree is not computed in a probabilistic sense. The authors structure identification is inherently a deterministic process, and thus is likely to make more errors compared to a system that could be run multiple times with different parameters (until convergence) to allow for higher accuracy. I do think the biggest strength is still probably in the structure mapping technique, however, and I like the idea of using a directed graph for a schema model, as this creates a very general framework.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Why Your Data Wont Mix: Semantic Heterogeneity</title>
   <link href="/eecs584/2014/11/01/eecs584/"/>
   <updated>2014-11-01T00:00:00-04:00</updated>
   <id>/eecs584/2014/11/01/eecs584</id>
   <content type="html">&lt;p&gt;Data is ubiquitous in any company or organization, but in many cases incorporating data from different sources presents a problem. Even within one organization, different data sources are designed using different schemas. Trying to reconcile all those slightly different schemas is currently an arduous process that must be done by hand. A human user must use their own domain knowledge to match schemas and determine which fields match and how to deal with missing or incomplete fields. This paper, which is more of a survey paper than anything else, offers a look at the problems that data mixing presents, and explains why the problem is so hard. The author also examines potential approaches that could work in the future, as well as problems he forsees moving forward.&lt;/p&gt;

&lt;p&gt;Schema matching is a hard problem, but an automated solution offers many benefits. The most general and largest benefit is that as data becomes more fluid, the amount of analysis and calculation that is possible in a fixed amount of development time grows. The author presents specific instances where data matching becomes useful, for example indexing the deep web. The deep web refers to any dynamically generated content behind search forms, and a search engine that could access and index these databases would provide magnitudes more information in some queries. To do this, the search engine would need a way to interpret the many different forms available from various websites, and this is not currently possible. The author argues that the domain knowledge required combined with technical expertise makes it hard to find people to match by hand. He then examines several state of the art data matching systems, and points out that each system has flaws and a single method is fragile and unlikely to succeed. While some systems attempt to combine solutions, the paper argues for machine learning based techniques that learn from a training set how to match schema. Such a system, the author argues, would be much more stable and likely to offer an automated solution that can handle real world data. &lt;/p&gt;

&lt;p&gt;One strength of this paper is the looking forward section where the author gives his outlook of the future and the problems facing semantic heterogeneity. He correctly predicts that the size of data in the near future will make many of the techniques in the state of the art intractable. He also points out that managing a &lt;em&gt;dataspace&lt;/em&gt; as opposed to a database is going to be a problem. A dataspace is comprised of participants and relationships, and as technology becomes more and more ubiquitous in our lives we can expect this problem of dataspaces to grow just as the paper predicts. One can imagine that the data for a human of the future would contain boundless dimensions, ranging from health data to smart devices to internet connected appliances. &lt;/p&gt;

&lt;p&gt;The strength of this paper is also a weakness in some respects - the author doesn’t really offer any ideas on how to face the problems he presents. The paper claims that the future roadblocks are just seeing research, but clearly did not expect the amount of heterogeneous data that would be available in only a few years. I also think the discussion about machine learning is lacking - it reads as a cursory glance but we’re never enlightened with how exactly machine learning could be applied to the problem of schema matching. I assume that a thorough treatment is available in other resources.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Real World Search</title>
   <link href="/eecs492/2014/10/30/eecs492/"/>
   <updated>2014-10-30T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/30/eecs492</id>
   <content type="html">&lt;h2 id=&quot;continue-less-real-planning&quot;&gt;Continue Less Real Planning&lt;/h2&gt;
&lt;p&gt;Defining the search problem:&lt;/p&gt;

&lt;p&gt;Initial state, actions avail, resulting state. This is backwards search as applied to a planning graph instead of a search tree. In GraphPlan finding a solution is intractable in the worst case, so we need heuristics to guide action selection. Also, this tells us nothing about timing.&lt;/p&gt;

&lt;h2 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h2&gt;
&lt;p&gt;Up until now we’ve been assuming everything takes one time step. How do we solve scheduling? We’ll use the &lt;strong&gt;critical path method&lt;/strong&gt;. In this way, a critical path is one that has the earliest start and end times for each action. Think critical path from business scheduling.&lt;/p&gt;

&lt;p&gt;The path goes through a graph, a partial order plan, and a linearly ordered sequence of actions. For each action off the critical path, we have some slack time. We want to calculate early start and late start for each action. In this new model we can represent variables in our sentences. &lt;/p&gt;

&lt;p&gt;Partially observable environments add complexity: during planning we need a model of sensors. The agent has to reason in real time while executing the plan. We’lll introduce a thing called ‘percept schema’ that augments the percepts from PDDL.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Graph Planning</title>
   <link href="/eecs492/2014/10/28/eecs492/"/>
   <updated>2014-10-28T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/28/eecs492</id>
   <content type="html">&lt;h2 id=&quot;backwards-search&quot;&gt;Backwards Search&lt;/h2&gt;
&lt;p&gt;In backwards search we do “regression planning” where we generate predecessors starting from the goal state. We aren’t searching in isolation though, we are moving toward a target state. The problem is handling interactions between goal propositions.&lt;/p&gt;

&lt;p&gt;Formally - If the domain can be expressed in PDDL, we can do regression search. Given a ground goal description g, ground action a:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; g' = (g - ADD(a)) \cup PRECOND(a) &lt;/script&gt;

&lt;h2 id=&quot;heuristics-for-planning&quot;&gt;Heuristics for Planning&lt;/h2&gt;
&lt;p&gt;We want to develop admissible heuristics for planning, and then we can use A&lt;script type=&quot;math/tex&quot;&gt;^\ast&lt;/script&gt;. We’ll:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Treat the search as a graph&lt;/li&gt;
  &lt;li&gt;Nodes are states&lt;/li&gt;
  &lt;li&gt;Edges are actions&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Planning graphs only work with propositional logic. The planning graph is state followed by action followed by state etc. Thus the graph is organized into layers, S0, A0, S1, A1, etc. Note that we need to describe mutually exclusive actions/states.  &lt;/p&gt;

&lt;p&gt;Details:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Planning graphs are NOT search, they are a heuristic. As a result, you &lt;strong&gt;cannot definitively answer whether a state is reachable&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;We CAN estimate how many steps to S&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we progress through the planning graph, we’ll alternate between &lt;script type=&quot;math/tex&quot;&gt;S_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; until two consecutive levels are indentical. As we progress, mutex links monotonically decrease. &lt;/p&gt;

&lt;h2 id=&quot;mutexes&quot;&gt;Mutexes&lt;/h2&gt;
&lt;p&gt;There are several mutex types:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inconsistent Effects - One action negates the other&lt;/li&gt;
  &lt;li&gt;Interferences - Sequential constraint&lt;/li&gt;
  &lt;li&gt;Competing Needs - One precondition of an action is mutually exclusive with the precondition of another&lt;/li&gt;
  &lt;li&gt;Inconsistent Support - Two literals at the same level that are a negation of each other&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Authoritative Sources in a Hyperlinked Environment</title>
   <link href="/eecs584/2014/10/27/eecs584/"/>
   <updated>2014-10-27T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/27/eecs584</id>
   <content type="html">&lt;p&gt;In the late 90s, the World Wide Web was quickly becoming a global internet, and research in characterizing the network’s structure was coming to a head. While Google is generally credited for the creation of authority based search algorithm, on the other coast Professor Jon Kleinberg was developing an algorithm that formed the basis for Google and provided complex algorithmic representation of webpages. This paper outlines Kleinberg’s “HITS” algorithm (which is actually never called HITS in the publication), which represents pages by both authority as well as a “hub score” which estimates how value that page is to others.&lt;/p&gt;

&lt;p&gt;The issue of searching the web was the primary motivation of this paper, like many others at the time. This paper, unlike the PageRank paper, develops in depth mathematical description of its algorithm. The author represents the internet as a directed graph, and runs an iterated algorithm to develop its two scores. First, it tries to determine authority by inspecting how many hubs point to it. Second, it updates its hub score to be the sum of the authority scores it points to. The authors provide mathematical proof (based on eigenvector analysis) that the algorithm indeed converges. This is a contrast to the Google PageRank paper, which does not provide much mathematical rigor. &lt;/p&gt;

&lt;p&gt;One major weakness of this technique is that the rankings themselves are dependent on the search terms, meaning that a new ranking must be computed for each query that is given to the system. This makes the HITS algorithm weak for a search engine as the processing power required would be immense for large loads, especially back in the time period it was developed. It does, however, only use “relevant” documents which is good for increasing computation speed and is a strength as it doesn’t require the entire database to compute. One could potentially see this being useful for a highly distributed algorithm - by partitioning data into disjoint subsets we would ideally be able to run many queries in parallel.&lt;/p&gt;

&lt;p&gt;I think the most interesting part of this paper isn’t the algorithm itself, but the historical perspective one can view it with. This algorithm, besides the obvious issue of being a per-query algorithm, is basically what Google ended up creating. Not to disparage Google’s accomplishments, but most of the ideas here were used and tweaked by Google. This paper shows that history really is written by the winner, as it’s conceivable that had Dr Kleinberg been an entrepreneur that Google would never have gained traction. In some ways the algorithm is superior - for example it computes both hub and authority rankings which would help in ranking and search. The paper especially is far superior, offering proofs of convergence as well as a comprehensive discussion of use cases and modifications possible. Fortunately for Google, however, this paper was never commercialized and PageRank ended up gaining fame as the “first” authority based search algorithm.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - The PageRank Citation Ranking: Bringing Order to the Web</title>
   <link href="/eecs584/2014/10/26/eecs584/"/>
   <updated>2014-10-26T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/26/eecs584</id>
   <content type="html">&lt;p&gt;In the early days of the web, search engines simply indexed pages by how much of your text matched the page. Results were inconsistent, and website designers could fool the systems by simply placing a bunch of hidden text somewhere on the page. In addition, results were never listed by what the user was truly looking for - searching a common word could bring results from any number of random webpages. The PageRank algorithm attempts to provide a ranking algorithm based on authority instead of frequency. The intuition is that important web pages are likely linked to by (a) lots of webpages and (b) other important webpages. This realization is the underlying of Google’s first search algorithm, and spawned the multinational monopoly that exists today.&lt;/p&gt;

&lt;p&gt;The PageRank algorithm uses the underlying structure of hyperlinks to determine page rank instead of page content. The more pages that link to your page, the higher your rank will be. Additionally, the authors chose to model the system as a random internet surfer, and introduced a damping factor (matrix E) to make the algorithm more resilient to infinite loops and cyclic graphs. While many details of the algorithm are excluded, the final form lends itself to matrix operations, and thus is computable in a reasonable amount of time. Even at the time of this papers release (1998), the algorithm could index millions of pages effectively. The authors provide some implementation details such as how to allocate memory for each page and how to remove dangling links to reduce memory consumption.&lt;/p&gt;

&lt;p&gt;This paper is very strong in explaining its major points - the authors offer lots of simple explanations for how their system works, while also including some mathematical background. It’s worth noting, however, that the final implementation of the algorithm ends up being full of warts - the non-conformity of the web (especially in an era before stronger compliance and open source parsers) means that the authors have to introduce ugly heuristics to create a working system. For example, the authors exclude everything from a cgi-bin folder, which is a sloppy way of excluding dynamic pages. One thing I really like about this paper is the discussion of the personalized E matrix, and how personalized search can be created very easily. I tend to think of personalized results as a recent development, but even during the genesis of Google the authors had created a model for it. From a historical perspective this paper is interesting and I think it’s good to read as a history lesson in the field. At the time, it introduced a novel concept of page ranking, and has become so influential that most people who were unaware of early search are surprised that the systems in place were ever &lt;strong&gt;not&lt;/strong&gt; authority based.&lt;/p&gt;

&lt;p&gt;I would say this paper is fairly weak in its actual mathematical background, and reads like a paper written by engineers and not scientists (which isn’t a bad thing). While it’s quite readable, the paper provides few theoretical guarantees or proofs, simply giving us a few equations. Part of this is the simplicity of the algorithm, but I would have liked to see a discussion provide more mathematical background. Even the wikipedia article for PageRank handles the math at a higher level than this paper.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Dynamo: Amazons Highly Available Key-value Store</title>
   <link href="/eecs584/2014/10/25/eecs584/"/>
   <updated>2014-10-25T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/25/eecs584</id>
   <content type="html">&lt;p&gt;Typical database packages attempt to have something for everybody, and as a result leave room for improvement in most use cases, even if that room is fairly small. For most companies, the ease of use and flexibility that a modern DMBS provides is incentive enough to offset the performance loss. However, for large companies like Google and Amazon, building an in house solution to extract the last five percent of performance is generally cheaper in the long run. Thus is the motivation behind Dynamo, a key-value store built by Amazon that trades consistency for availability and response time. &lt;/p&gt;

&lt;p&gt;Dynamo is on the surface quite simple - the application exposes a get and put method to the user, indexed by key. The complexity of the system, however, lies on a few factors:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Highly write available, the system prioritizes writes over reads&lt;/li&gt;
  &lt;li&gt;Eventually consistent, the system makes no promises about the state of data at a current time&lt;/li&gt;
  &lt;li&gt;Distributed and fault tolerent, the system is designed to tolerate entire datacenter outages&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Amazon would lose customer loyalty if their website failed to work even one percent of the time, and thus for them it’s worth spending the money on a system that will rarely ever fail. The biggest innovation to achieving this goal is the idea of the ring node in a peer to peer system. Dynamo uses a consistent partitioning algorithm to store data over a set of nodes (ideally located in separate data centers). In a sense this is a bit like Google’s GFS, but Dynamo differs by offering a peer to peer system with no master. The system also has a powerful versioning tool that can detect when versions of objects conflict over time. This allows the user to resolve these conflicts using logic they define. While this is claimed to be a strength of the system, it is also a weakness. By leaving conflict resolution mainly in the hands of the application programmer, Dynamo places heavy trust in its users. &lt;/p&gt;

&lt;p&gt;One strength of Dynamo is that the system remains flexible for Amazon’s internal usage. By offering tunable knobs, the system allows users to configure the database to their workload. For example, Dynamo can be used for mainly read queries by increasing the value of W and setting R to be one. There is a weakness to the system hinted at by the paper, and that is that Dynamo is not actually fully automated. Adding additional nodes requires administrator intervention, and this would become quite tedious in cases with thousands of nodes. I suspect that Amazon’s cloud offering of Dynamo shields the user from most of these issues, and that by now Amazon has improved the system to automatically add and release nodes from the database.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Planning Day 2</title>
   <link href="/eecs492/2014/10/21/eecs492/"/>
   <updated>2014-10-21T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/21/eecs492</id>
   <content type="html">&lt;p&gt;Methods we’ve discussed have some limitations, such as domain-specific heuristics. Planning represents an alternative that tends to use graph heuristics. Logic, for example, is not stateful and cannot handle temporal assumptions or changes in structure. Logic assumes everything happens at the same time, planning does not.&lt;/p&gt;

&lt;p&gt;An action divides time into a “before” and “after”. We’ll use a language called PDDL to express action schema. Note that PDDL is a highly restricted subset of first order logic.&lt;/p&gt;

&lt;p&gt;State is represented by a set of fluents, and those fluents must be &lt;em&gt;ground&lt;/em&gt; and &lt;em&gt;functionless&lt;/em&gt;. The action schema defines preconditions and the effects of a given action. This allows us to represent state.&lt;/p&gt;

&lt;p&gt;Action(Fly(p, from, to))
    Precond: &lt;script type=&quot;math/tex&quot;&gt;At(p, from) \wedge Plane(p) \wedge Airport(from) \wedge Airport(to)&lt;/script&gt;
    Effect: &lt;script type=&quot;math/tex&quot;&gt; \neg At(p, from) \wedge At(p, to) &lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;forward-search&quot;&gt;Forward Search&lt;/h3&gt;

&lt;p&gt;In this case, forward search is called progression planning. Basically we just move through the search space, with each constraint and action happening at any state change step. We can use any graph search algorithm for this, and any complete graph search algorithm will be complete for forward planning. Obviously this is quite inefficient, as there are many irrelevant actions. We need either: good heuristics OR backward search.&lt;/p&gt;

&lt;h3 id=&quot;backward-search&quot;&gt;Backward Search&lt;/h3&gt;

&lt;p&gt;This is what people actually use. Also called &lt;strong&gt;regression&lt;/strong&gt; planning, it generates predecessors starting from the goal state. We are regressing through potential goals trying to not invalidate any states.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - A Comparison of Approaches to Large-Scale Data Analysis</title>
   <link href="/eecs584/2014/10/20/eecs584/"/>
   <updated>2014-10-20T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/20/eecs584</id>
   <content type="html">&lt;p&gt;When Google announced MapReduce in 2004, it was widely viewed as a paradigm shift in distributed data analysis. The architecture was quickly developed into an open source deriviative, designed to run atop the HDFS, itself modeled after Google’s GFS. This open source combination of software, along with other packages, became Hadoop and positioned itself as the spearhead in the data science and big data movement. In this paper (somehow not surprisingly featuring Michael Stonebreaker), the authors argue that MapReduce is not the silver bullet to data warehouse analysis and that parallel DBMSs are more established and outperform MapReduce in most tasks, while offering ease of use that MapReduce lacks.&lt;/p&gt;

&lt;p&gt;MapReduce works by running a set of mapper and reducer tasks over a large set of data, distributing the computation in such a way to make restarts easy in the case of hardware failure. This paper highlights the weaknesses of MapReduce, including but not limited to its runtime performance. The authors equate MapReduce’s requirement to program in Java to the domain specific and hardware specific language of the failed CODASYL, and make a similar argument against the system that several of the authors did years ago against graph based models. This is a reasonable complaint, and at the time this was a strength of the paper. Since publication, it has been accepted that a higher level of abstraction would be required, and SQL to MapReduce translators such as Hive were developed. Additionally, the authors argue against the requirement of placing intermediate data on disk, and point out that parallel DBMSs don’t have this requirement. Again, modern development prevails, and Spark/Shark are designed to do exactly this. It’s clear that this paper had some influence on those systems, as the authors have direct influence on the Berkeley database group that developed Spark/Shark. &lt;/p&gt;

&lt;p&gt;The paper also presents several comparisons of MapReduce performance against DBMS-X (almost certainly Teradata), and Vertica (another Stonebraker enterprise). Across all benchmarks, the parallel DBMSs appear to be superior. The authors do note that MapReduce has better failsafe mechanisms than the databases tested, but assert that the performance penalty is not worthwhile. I like that the authors make claims as to why the parallel RDBMSs outperform MapReduce. However, many of the performance reasons are the result of such a mature field. Indexing and internal optimizations developed over years and years of industry have made the RDBMS very powerful at extracting as much performance as possible out of the machines it is installed on. While the authors do make valid claims, it’s important to remember how young this new architecture is. I think one weakness of this paper is that it fails to recognize how important MapReduce’s “many low power machines” architecture is to it’s design. The other database systems cannot even be installed on thousands of machines, and in some cases it may be significantly cheaper to buy many commodity machines (especially for large companies like Google who do so at massive scale). This is overlooked, however, and should be addressed by the authors. Overall this paper is a good reality check for developers who get easily excited over shiny new technology - parallel DBMSs have existed since the Gamma project and for mission critical and time critical applications it is likely worth paying the extra for the much faster system.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - MapReduce: Simplified Data Processing on Large Clusters</title>
   <link href="/eecs584/2014/10/18/eecs584/"/>
   <updated>2014-10-18T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/18/eecs584</id>
   <content type="html">&lt;p&gt;Distributed data processing has gained heavy traction as a result of internet scale data warehouses. In 2004, Google introduced MapReduce, a programming model and associated implementation for processing and generating large data sets. MapReduce has been extremely influential in the data processing space, and has become a de facto standard for distributed data analysis, running on Hadoop or similar system. MapReduce earns its name from its programming model, which draws from the functional programming paradigm of mapping a function to data and reducing that data to an aggregate value. Because these operations are atomic, and in most cases referentially transparent, the analysis can easily be parallelized and made fault tolerant.&lt;/p&gt;

&lt;p&gt;This paper outlines the details of Google’s MapReduce system and provides examples of when the application is appropriate. MapReduce requires a user to write two functions, a mapping function that applies a function to data and emits key value pairs, and a reducing function that takes key value pairs and reduces them to one output. The intermediate result is stored on the machine, allowing machines to be either a mapper or reducer. This algorithm also allows for machine failure - in the case that a node fails to respond to the master, the node’s jobs are simply placed on other machines. &lt;/p&gt;

&lt;p&gt;This paper’s strength lies in the simplicity of the system. I suspect that many people who use MapReduce on a daily basis have never read the paper, and likely do not have significant background with parallel computation. By applying a simple API to a complex problem, Google has allowed data computation on a large scale. Thus, I claim the primary contribution is the idea of using the map-reduce paradigm itself. There are other strengths in this paper, including the idea of backup tasks to reduce stragglers and the methods of splitting data to be used by the workers. I also liked that the paper includes an example program in appendix A, as this highlights the practicality and simplicity of the system. &lt;/p&gt;

&lt;p&gt;To achieve such power, the designers of MapReduce had to make several significant tradeoffs. The largest weakness of this system is that the programmer must be able to express their idea in the form of a map-reduce query. This is difficult for tasks that cannot run in isolation (for example training an SVM). Additionally, because MapReduce is a large system, real time processing is impossible. The system is best used for batch processes, and has trouble handling streaming data. One problem that has been addressed since the introduction of MapReduce is that the user must program the system themselves. This is analogous to IMS or CODASYL where the user had to define functions to query the database. Systems to translate SQL or other domain specific languages into MapReduce jobs are now quite popular, for example Hive or Pig. Finally, while MapReduce allows large data sets to be easily processed by storing intermediate results on disk, this is a weakness because each part of the job requires additional IO writes. By placing the results in memory significant speedups could be achieved. This has been explored in Apache’s Spark and in-memory Hadoop.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Biostatistics</title>
   <link href="/eecs598/2014/10/16/eecs598/"/>
   <updated>2014-10-16T00:00:00-04:00</updated>
   <id>/eecs598/2014/10/16/eecs598</id>
   <content type="html">&lt;p&gt;How do we model expected outcomes? We can predict either binary or continuously.&lt;/p&gt;

&lt;h2 id=&quot;binary-methods&quot;&gt;Binary Methods&lt;/h2&gt;
&lt;p&gt;A - True Positive
B - False Negative
C - False Negative
D - True Negative&lt;/p&gt;

&lt;h4 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h4&gt;
&lt;p&gt;Proportion of cases predicted correctly. Rarely used in real systems.
&lt;script type=&quot;math/tex&quot;&gt; \frac{A+D}{A+B+C+D} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;recall&quot;&gt;Recall&lt;/h4&gt;
&lt;p&gt;Sensitivity, or how well we predict true positives compared to false negatives
&lt;script type=&quot;math/tex&quot;&gt; \frac{A}{A+B} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;specificity&quot;&gt;Specificity&lt;/h4&gt;
&lt;p&gt;How well we distingish between negatives and false positives
&lt;script type=&quot;math/tex&quot;&gt; \frac{D}{C+D} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;precision&quot;&gt;Precision&lt;/h4&gt;
&lt;p&gt;Positive predictive value
&lt;script type=&quot;math/tex&quot;&gt; \frac{A}{A+C} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;negative-predictive-power&quot;&gt;Negative predictive power&lt;/h4&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt; \frac{D}{D+B} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;relative-risk&quot;&gt;Relative Risk&lt;/h4&gt;
&lt;p&gt;How likely an outcome is relative to the alternative
&lt;script type=&quot;math/tex&quot;&gt; \frac{A/(A+C)}{B/(B+D)} &lt;/script&gt;&lt;/p&gt;

&lt;h4 id=&quot;odds-ratio&quot;&gt;Odds ratio&lt;/h4&gt;
&lt;p&gt;Odds of actual predictions, does not change based on positive or negative outcomes
&lt;script type=&quot;math/tex&quot;&gt; \frac{A/C}{B/D} &lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;continuous-methods&quot;&gt;Continuous Methods&lt;/h2&gt;
&lt;p&gt;Gives a somewhat better model&lt;/p&gt;

&lt;h4 id=&quot;auc---operating-curve&quot;&gt;AUC - Operating Curve&lt;/h4&gt;
&lt;p&gt;Lets us see the values at different decision thresholds, generally widely used.&lt;/p&gt;

&lt;h4 id=&quot;hosmer-lemeshow-test&quot;&gt;Hosmer-Lemeshow Test&lt;/h4&gt;
&lt;p&gt;Check to see if observed outcomes and predicted outcomes match, but can be fooled by larger samples.
&lt;script type=&quot;math/tex&quot;&gt; \sum\limits_{i=1}^n \frac{(O_i - E_i)^2}{E_i(1-E_i/n_i)} &lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;comparing-two-systems&quot;&gt;Comparing Two Systems&lt;/h2&gt;
&lt;p&gt;How can we compare two systems to each other? Well we could just use the metrics we’ve already discussed. We have other systems though:&lt;/p&gt;

&lt;h4 id=&quot;net-reclassification-improvement-nri&quot;&gt;Net Reclassification Improvement (NRI)&lt;/h4&gt;
&lt;p&gt;Focuses on binary predictors, and looks at how system B treats observations differently. This gives us a sense of how many patients will be affected by improvements.&lt;/p&gt;

&lt;h4 id=&quot;integrated-discrimination-improvement-idi&quot;&gt;Integrated Discrimination Improvement (IDI)&lt;/h4&gt;
&lt;p&gt;Compares differences in discrimination slopes, aka differences of mean of probabilities for outcome vs non-outcome.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Planning</title>
   <link href="/eecs492/2014/10/16/eecs492/"/>
   <updated>2014-10-16T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/16/eecs492</id>
   <content type="html">&lt;p&gt;This lecture is actually more about first order logic.&lt;/p&gt;

&lt;h2 id=&quot;inference-in-fol&quot;&gt;Inference in FOL&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Universal instantiation&lt;/strong&gt; is the idea that we can infer any sentence obtained by substituting a &lt;em&gt;ground term&lt;/em&gt; for the variable. If we replace a variable by a single new constant it’s called &lt;strong&gt;existential instantiation&lt;/strong&gt; and we replace a variable with a single constant. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First order inference&lt;/strong&gt; allows us to substitute names for variables, and we can apply this many times over many sentences. This brings us to &lt;strong&gt;unification&lt;/strong&gt;, which allows us to find the values of variables to make a sentence logically consistent. &lt;/p&gt;

&lt;p&gt;How do we resolve first order logical statements? We can look at chaining again. Let’s look first at &lt;strong&gt;forward chaining&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;In forward chaining we want to apply logic iteratively until we reach a conclusion. However, this is quite inefficient, if intuitive. We recheck each rule each iteration which is a huge waste. There’s a version that’s efficient, but we won’t discuss it.&lt;/p&gt;

&lt;p&gt;Obviously the other option of this is &lt;strong&gt;backward chaining&lt;/strong&gt;, which is basically just using depth first search. &lt;/p&gt;

&lt;h2 id=&quot;resolution&quot;&gt;Resolution&lt;/h2&gt;
&lt;p&gt;Resolution requires conjunctive normal form! &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Dremel: Interactive Analysis of Web-Scale Datasets</title>
   <link href="/eecs584/2014/10/12/eecs584/"/>
   <updated>2014-10-12T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/12/eecs584</id>
   <content type="html">&lt;p&gt;Web scale data analysis is time consuming even with modern technologies. While Map-Reduce offers a flexible model for analyzing large scale data, it is still generally not near interactive speed and requires writing complex Java procedures to run. Dremel is a new take on petabyte-scale data analysis, offering a column storage based, interactive SQL approach to read only data. This new system is quite a bit different from well known SQL based interfaces to Map-Reduce such as Hive or Pig, because instead of simply translating SQL to MapReduce jobs it creates a new architecture in order to deliver results faster.&lt;/p&gt;

&lt;p&gt;The primary contribution of this paper is the introduction of a columnar based storage system for nested data, aka data with repeatable and optional fields. Dremel does this by assigning two values to the column data - a repeated field that specifies which reptition the value represents, and a definition level to specify how many fields are actually defined. This layout is quite clever, and allows Dremel to store data in a column format and reap the benefits that come with columns for aggregate queries. The paper also introduces a new query language for the Dremel system, as well as providing a finite state method of reassembling the data structures from columns. Because Dremel is a shared nothing system, it uses a tree architecture to execute queries across many machines, with each eventually moving through the root server. Dremel’s performance seems incredible from the data the paper presents, processing queries an order of magnitude or two faster than MapReduce alone. &lt;/p&gt;

&lt;p&gt;With any high performance system comes trade offs, and Dremel is no exception. Data in Dremel is read only, which is a massive limitation for any system that must fufill both storage and OLAP requirements. In order to actually execute data queries so quickly, the user must load up a separate cluster just to do analysis. I like that the paper provides algorithms for each of its components, but it does a poor job of explaining exactly how the underlying column storage works. Because we are never writing to the system, we alleviate most of the traditional database concerns. However, this aspect of the system still likely has pitfalls and gotchas, and an overview would have been nice. I would also be interested to see how data is actually stored on the system and split between nodes, as the actual process of loading data onto the cluster is completely skipped. This is likely because the Dremel team feels no need to introduce a proprietary piece of technology into the paper. Once again, papers from a commercial system lack details that would likely be required for implementation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Big Table: A Distributed Storage System for Structured Data</title>
   <link href="/eecs584/2014/10/11/eecs584/"/>
   <updated>2014-10-11T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/11/eecs584</id>
   <content type="html">&lt;p&gt;With the rise of monopoly internet companies, a few behemoths generate data on scales unexpected by researchers. BigTable, the brainchild of the Google research team, is an example of a technology developed purely for application. With a large research budget and petabyte scale data, Google designed BigTable to work across hundreds to thousands of commodity machines. BigTable is a non-relational database that uses a three dimensional indexing system to store data - two dimensions are string keys, and the third is a time based index. This structure allows BigTable to naturally fit the data usage patterns of many products in the Google domain, where data changes across time on a massive scale (ie Street View, Google Earth, Web Indexing, etc). &lt;/p&gt;

&lt;p&gt;The largest contribution of this paper is the data model, which introduces time indexing as a first class citizen of the database. It’s important for the reader to note that BigTable is &lt;em&gt;not&lt;/em&gt; a database in the conventional sense, and is instead a sparse, multidimensional, sorted map. In most cases, the client is designed in such a way that commonly accessed information is stored nearby. For example, webpages can be stored as com.domain.subdomain, and by doing so will keep subdomains nearby on disk. BigTable is built upon several other Google services, specifically the Google File System, the Chubby distributed lock system, and the Google SSTable file format. The system consists of thousands of “tablets”, hundreds of which are stored on each tablet server. Like the Google File System, in most cases the client will communicate directly with these tablet servers, talking to the master server only when it needs an update on cached tablet server information. Tablets distribute disk data across machines, and are stored via the underlying SSTable file format. Google also introduces performance enhancements, such as bloom filters for tablet membership and compression inside of SSTables using a novel compression algorithm designed to trade CPU time for compression efficiency. &lt;/p&gt;

&lt;p&gt;BigTable shares many similarities with C-Store, and the authors mention this directly. One important inspiration from C-store is the use of a hybrid read/write system to write data to disk. BigTable uses a “memtable” to write data to directly, and then transfers that memory based table to disk over time. One major strength of BigTable is its robustness. Servers of any type can fail and the system will recover quickly. By using many small tablets, the remaining machines can easily take a small part of one machine’s work. Additionally, the commit-log implementation is quite smart - the system uses one massive commit log per tablet server, but on recovery sorts the log to allow sequential read by each machine. &lt;/p&gt;

&lt;p&gt;The commit-log is likely a major weakness of the system as well - the tablet server writes its own commit log, and it seems as though if a system crashed and was unrecoverable that its data would be unrecoverable as well. The GFS provides K-safety for the data itself, so this situation will not completely lose data. Additionally, the method of using multiple threads to write the log seems like it wouldn’t fix write issues in many cases. If one thread is writing slowly, there is likely an underlying cause and switching to another thread for writes will only delay the onset of the larger problem. One final weakness of this system is that it relies on the multidimensional structure it provides, as well as the client to ensure that data access is efficient. For example, if the user had decided to store subdomain.domain.com instead of the other way around, access would be slow for nearby subdomains. Thus, this system is only truly useful for someone with technical knowledge, and would not be as useful for non-technical business analysts. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Data Analysis II</title>
   <link href="/eecs598/2014/10/09/eecs598/"/>
   <updated>2014-10-09T00:00:00-04:00</updated>
   <id>/eecs598/2014/10/09/eecs598</id>
   <content type="html">&lt;h2 id=&quot;clustering&quot;&gt;Clustering&lt;/h2&gt;

&lt;h3 id=&quot;k-means&quot;&gt;K-Means&lt;/h3&gt;
&lt;p&gt;We’ll randomly assign some centroids to data, assign points to them, and iteratively cluster. Good because it’s simple, but choosing the starting point can be an issue. Density must be “bloblike” and we have to choose starting parameters to begin with. The process is stochastic and so not reproducible. &lt;/p&gt;

&lt;h3 id=&quot;hierarchical-clustering&quot;&gt;Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;We start with each point as a cluster and we merge clusters repeatedly. If we merge the closest clusters repeatedly we’ll eventually end up with a tree of clusters. Once we have a cluster, how do we decide which method to compare the distances between them? We can use minimum, maximum, avg, or centroid.&lt;/p&gt;

&lt;h3 id=&quot;fuzzy-clustering&quot;&gt;Fuzzy Clustering&lt;/h3&gt;
&lt;p&gt;Basically K-means but we assign membership to multiple sets and allow each point to “maybe” be in a cluster.&lt;/p&gt;

&lt;h2 id=&quot;feature-selection&quot;&gt;Feature Selection&lt;/h2&gt;
&lt;p&gt;We want to rank features for how well they do in our algorithms. How do we choose the features? We can use correlation, try each one, etc. Correlation only lets us look at one feature at a time but in multiple dimensions we might be able to classify. In most cases we’ll use forward or backward feature selection.&lt;/p&gt;

&lt;h3 id=&quot;principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/h3&gt;
&lt;p&gt;We try and extract uncorrelated features by creating a set of basis functions in the original feature space. We want to maximize the variance for each component with respect to the original data.&lt;/p&gt;

&lt;h3 id=&quot;multidimensional-scaling&quot;&gt;Multidimensional Scaling&lt;/h3&gt;
&lt;p&gt;We want to capture the distance between points in our lower dimensional representation. In MDS we only require a notion of how different points are. This is considered a type of embedding.&lt;/p&gt;

&lt;h3 id=&quot;manifolds&quot;&gt;Manifolds&lt;/h3&gt;
&lt;p&gt;What do we do when the distribution of values follows a structured layout, aka a manifold. In this case the Euclidian distance no longer applies. Many times we’ll just embed a manifold into Euclidian space. A common way to do this is via isomap, where we look at the distance over a graph of the points in manifold space.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>First Order Logic Day 2</title>
   <link href="/eecs492/2014/10/09/eecs492/"/>
   <updated>2014-10-09T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/09/eecs492</id>
   <content type="html">&lt;h2 id=&quot;the-move-to-first-order-logic&quot;&gt;The Move to First Order Logic&lt;/h2&gt;
&lt;p&gt;We want to move away from propositional logic. Propositional logic is good because it’s unambiguous but too specific - we want to allow variables in the logic. &lt;/p&gt;

&lt;p&gt;In first order logic we have objects (rooms, dogs, etc) and relations. Relations test a property of one or more objects, and with &lt;strong&gt;no arguments&lt;/strong&gt; these are equivalent to propositional logic. &lt;/p&gt;

&lt;p&gt;Relations - properties that are links between variables
Function - input a tuple, output a value&lt;/p&gt;

&lt;p&gt;First order logic is the same as propositional logic with variables. Obviously first order logic is more complicated, but it’s in fact &lt;em&gt;much&lt;/em&gt; more so. There are new variables in FOL called qualifier variables:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\forall&lt;/script&gt;&lt;/strong&gt; - Says “for all”, and is equivalent to a big conjunction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt;&lt;/strong&gt; - Says “there exists”, equivalent to a big disjunction.&lt;/p&gt;

&lt;p&gt;We introduce restrictions to reduce complication:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unique-names assumption - Two names cannot be the same object&lt;/li&gt;
  &lt;li&gt;Closed-world assumption - Any sentences unknown to be true are false&lt;/li&gt;
  &lt;li&gt;Domain closure - There are no more domain objects than the ones specified&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These assumptions allow us to more concisely specify logic. &lt;/p&gt;

&lt;h2 id=&quot;quantifiers&quot;&gt;Quantifiers&lt;/h2&gt;
&lt;p&gt;We’ll use quantifiers using an ASK/TELL interface. We can tell explicit facts or we can tell via variables. We can ask for statements, or we can ask for variables, aka &lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt; Person(x) &lt;script type=&quot;math/tex&quot;&gt;\Rightarrow&lt;/script&gt; {x/Robert} and {x\John}.&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;
&lt;p&gt;Universal instantiation lets us infer any sentence by substituting a ground term for the variable. Aka:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\forall \nu \alpha}{Subst(\{\nu/\kappa\},\alpha)}&lt;/script&gt;

&lt;p&gt;The same thing applies for &lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt;. Basically we reduce a higher order function by partially applying an argument.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>First Order Logic</title>
   <link href="/eecs492/2014/10/07/eecs492/"/>
   <updated>2014-10-07T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/07/eecs492</id>
   <content type="html">&lt;h2 id=&quot;finish-prepositional-logic&quot;&gt;Finish Prepositional Logic&lt;/h2&gt;
&lt;p&gt;A set of entailed sentences can only increase as sentences are added. Conclusions from inference rules are never defeated by further inference, thus the search will never need to backtrack. This means we don’t adapt well because we can only become more specific and we’ll think the entire history is true. &lt;strong&gt;Everything we know now will remain true forever&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;What does it mean to be complete? We can reach any goal in the search space. We do this by resolution, which is a complete inference algorithm when coupled with any complete search algorithm. We can use unit resolution to split up our knowledge base:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac{(c \vee a_1 \vee a_2 ... \vee a_n) \wedge (~c \vee b_1 \vee b_2 ... \vee b_k)}{a_1 \vee a_2 ... \vee a_n \vee b_1 \vee b_2 ... \vee b_k} &lt;/script&gt;

&lt;p&gt;To actually use this in an algorithm we must put this into conjunctive normal form, aka each sentance is a set of literals combined with or. We can use resolution refutation to try and prove the opposite of a statement. Eg, if we get a sentence &lt;script type=&quot;math/tex&quot;&gt;P_{1,2}&lt;/script&gt; we will try and prove &lt;script type=&quot;math/tex&quot;&gt;\neg P_{1,2}&lt;/script&gt;. &lt;/p&gt;

&lt;p&gt;Resolution is powerful, but we may not always need the full strength (and thus full computational complexity). We can introduce restrictions on it by assuming either horn or definite clauses: forward or backward chaining. &lt;/p&gt;

&lt;h2 id=&quot;chaining&quot;&gt;Chaining&lt;/h2&gt;
&lt;p&gt;Forward chaining maintains for each rule a count of unsatisfied premises. We fill a queue with known facts, and go through the queue, inferring rules and reducing the count associated, adding new rules back in.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - The Gamma Database Project</title>
   <link href="/eecs584/2014/10/05/eecs584/"/>
   <updated>2014-10-05T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/05/eecs584</id>
   <content type="html">&lt;p&gt;This paper details the Gamma Database, a groundbreaking DBMS designed for shared nothing machines. Unlike the other attempts at shared nothing databases at the time of writing (which was the late 80s), the Gamma project was designed to scale linearly with the number of nodes added. This was a revolutionary achievement, as previous shared nothing machines were not entirely decentralized and still ran into single-node bottlenecks if the number of machines was too high. Gamma did this by using novel algorithms for horizontal partitioning and parallel hash algorithms.&lt;/p&gt;

&lt;p&gt;The Gamma paper is lengthy and so in the interests of brevity I’ll gloss over details. By the time this paper is released, the Gamma database project is actually on version 2.0. The new architecture uses a 32 processor hypercube from Intel with enough RAM to provide adequate buffering. The authors had to develop their own operating system to support their database, an impressive feat in itself. Gamma stores its data in horizontal partitions, which allows the system to exploit all the bandwidth of the disk subsystems. One thing I find interesting is that the authors were willing to admit their mistake in declustering all relations across all nodes. They advocate another system to decluster, based on “heat” of the relation (in COPE88). Generally authors do not like admitting that they are wrong in academic writings, and I think it’s commendable these authors are willing to do so.&lt;/p&gt;

&lt;p&gt;One strength of this paper is the authors proposal to use hash-based parallel algorithms to scale up query processing. By decentralizing data page processing throughout the system, Gamma avoids a potential bottleneck that its predecessor would frequently encounter. The paper outlines their multiprocessor join (and select) algorithms later in the document. The intuition behind the join operators attempts to combine relations into disjoint buckets via hashing. These buckets hold all tuples with the same value attribute. The authors outline sort-merge, Grace, Simple, and Hybrid algorithms for parallel joins. Each of these algorithms has already been published in a paper, so this proposal simple acted to aggregate the results into a real system. While on the surface this seems like a weakness of the paper, without an architecture these ideas would possibly have languished in obscurity. &lt;/p&gt;

&lt;p&gt;The paper also outlines the locking mechanism used by Gamma, which in my eyes is the weakest and most outdated part of the paper. The centralized deadlock detection algorithm proposed introduces a bottleneck into the system, and seems quite heavy duty to me. I think part of the reason for this is the time the paper was written in - seek and write times were much longer and conflicts were likely to occur. The log algorithm also appears slow on recovery, as all data must be forwarded to each recovering node. &lt;/p&gt;

&lt;p&gt;I do like that the system employs “chained declustering” to ensure availability in the event of a node failure. This thought is a predecessor of K-safety and is a requirement in a modern system. The authors haven’t actually implemented the chained declustering at the time of publication, however, so any benefit in real systems is speculation (as compared to interleaved declustering). The paper does a thorough evaluation of the Gamma project, although there is no comparison to a commercial DMBS such as Teradata, possibly because the new database is so different the authors feel comparison would not make sense.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - C-Store: A Column-oriented DBMS</title>
   <link href="/eecs584/2014/10/04/eecs584/"/>
   <updated>2014-10-04T00:00:00-04:00</updated>
   <id>/eecs584/2014/10/04/eecs584</id>
   <content type="html">&lt;p&gt;In what seems to be a common trend, this paper introduces yet another radical database architecture reimagining, again spearheaded by the prolific Michael Stonebraker. C-Store is a shared nothing column oriented database, designed for OLAP workloads with a focus on high read throughput and low read latency. This is in contrast to traditional DBMSs which are instead optimized for write efficiency. Written in 2005, this paper borrows heavily from other ideas in the database space, but is the first design to present a comprehensive architecture for a column store system that leverages modern computational systems such as shared nothing designs.&lt;/p&gt;

&lt;p&gt;The core of C-store is the use of a column oriented data model, which eschews traditional row indexing in order to provide better read access in cases where a user is accessing a small subset of columns over a large quantity of tuples. C-Store accomplishes this by using projections, which are sorted subsets of attributes of a table. The database may contain any number of projections, but each column in must be present in at least one. By using column based projections, C-Store offers faster read performance for queries that read data mainly from columns, as many ad-hoc queries tend to do in analytic processing. For example, it’s more likely for a client to request the first names of all clients than it is to request a client by name when doing data analysis. The paper gives an overview of other parts of the system, including a look at the snapshot isolation supported by the DBMS and the performance enhancements such as column compression and column-oriented optimizers and executors. Components of interest are the hybrid architecture for reading and writing, which consists of two separate storage utilities - one for reading and one for writing. The other interesting discussion is that of k-safety and the implementation of a shared nothing architecture. The paper ends with performance analysis (which is eye opening if taken at face value) as well as a look at systems that inspired C-Store.&lt;/p&gt;

&lt;h3 id=&quot;strengths&quot;&gt;Strengths:&lt;/h3&gt;
&lt;p&gt;C-Store’s use of column based projections offers an innovative solution to column based storage. While offering the obvious advantages of fast read access of OLAP workloads, the key insight of these projections is the use of overlap to facilitate k-safety. Because each projection (which is in essence a restricted materialized view) contains data that is non-dependent on the other projections, a given column can be represented multiple ways. By doing this the database can contain redundant columns and allow K nodes to fail while staying operational. &lt;/p&gt;

&lt;p&gt;The use of read stores and write stores is another clever innovation. C-Store compromises between read performance and writability by offering a small column store for writes to take place in. This store can be thought of as a staging area, or buffer for writes. Stored in memory, this buffer is eventually written to the database. By offering only snapshot isolation, the potential for lock contention and isolation failure is reduced with this eventually consistent system. This hybrid is especially useful because the column store requires many operations to do small updates.&lt;/p&gt;

&lt;h3 id=&quot;weaknesses&quot;&gt;Weaknesses:&lt;/h3&gt;
&lt;p&gt;As with many shared nothing systems, the problem of how to distribute redundant data across nodes is a hard one. Determining the physical layout in C-Store requires the database administrator to maintain K-safety while also keeping performance optimal. This is devastating for a real world system, and an automated tool would be required for this system to function in a real world scenario. Another glaring weakness is the inability to operate efficiently on workloads unforeseen by the installer. If there is a scenario where we must do a large write, we will be horribly inefficient. In the same way, executing and transactional processing workload will cause the system to slow dramatically.  &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Propositional Logic and Inference</title>
   <link href="/eecs492/2014/10/02/eecs492/"/>
   <updated>2014-10-02T00:00:00-04:00</updated>
   <id>/eecs492/2014/10/02/eecs492</id>
   <content type="html">&lt;h2 id=&quot;knowledge-based-agents&quot;&gt;Knowledge Based Agents&lt;/h2&gt;
&lt;p&gt;Knowledge based agents reason over the knowledge base to come up with the best action. The hard thing is how to encode our knowledge in the machine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Entailment&lt;/strong&gt; - Relation between sentences where you can say one follows logically from another. If &lt;script type=&quot;math/tex&quot;&gt;KB \mid= \alpha&lt;/script&gt; we say that the knowledge base &lt;em&gt;entails&lt;/em&gt; alpha. &lt;/p&gt;

&lt;p&gt;The basic way of checking for relationships is model checking, which just enumerates all models where &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is true.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Truth preserving - Only derive entailed sentences&lt;/li&gt;
  &lt;li&gt;Complete - Can derive &lt;strong&gt;all&lt;/strong&gt; sentences that are entailed&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sentences&quot;&gt;Sentences&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sentence is either an AtomicSequence or a ComplexSentence&lt;/li&gt;
  &lt;li&gt;Atomic sentence is either True, False, or a Symbol&lt;/li&gt;
  &lt;li&gt;Symbol is P, Q, R, etc&lt;/li&gt;
  &lt;li&gt;A complex sentence includes logical operators&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also need semantics that determine the interpretation of symbols. Review logical prepositions here, aka NOT, AND, OR, IMPLIES, EQUALITY.&lt;/p&gt;

&lt;p&gt;For the wumpus world, we can directly encode rules and develop a complete knowledge base. This method of model checking works but is infeasible. &lt;strong&gt;Every known inference algorithm for propositional logic has worst case exponential complexity.&lt;/strong&gt; 
## Inference
Inference is the process by which sentences are derived from others. For a given inference method we want it to be sound (no false conclusions) and complete (no missing conclusions). So what inference methods can we implement?&lt;/p&gt;

&lt;h3 id=&quot;model-checking&quot;&gt;Model Checking&lt;/h3&gt;
&lt;p&gt;Generic inference, intuitive but not implementable. We literally just try everything and check that alpha is valid. We want to use a &lt;em&gt;proof based&lt;/em&gt; method instead to eliminate model checking.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Logical Agents</title>
   <link href="/eecs492/2014/09/30/eecs492/"/>
   <updated>2014-09-30T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/30/eecs492</id>
   <content type="html">&lt;h2 id=&quot;finish-up-constraints&quot;&gt;Finish up constraints&lt;/h2&gt;

&lt;h3 id=&quot;forward-checking&quot;&gt;Forward Checking&lt;/h3&gt;
&lt;p&gt;We can reduce our search by forward checking - whenever a variable X is assigned we examine each unassigned Y and delete Y that doesn’t fit constraints. Forward checking is good, but we don’t actually look ahead to all of our constraints; FC is almost a greedy search.&lt;/p&gt;

&lt;p&gt;If we maintain arc consistency, we’ll be more stringent and search space is even smaller. &lt;/p&gt;

&lt;h3 id=&quot;backjumping&quot;&gt;Backjumping&lt;/h3&gt;
&lt;p&gt;The way we knew in the 4queen problem in notes on where to restart is because we knew where to backjump. To do this we’ll create a conflict set containing the past conflicts.&lt;/p&gt;

&lt;h2 id=&quot;logic&quot;&gt;Logic&lt;/h2&gt;
&lt;p&gt;We want to be able to reason about the world in a way that lets us derive new facts about the world without full knowledge. In the logical framework we’ll have a knowledge base, and we want to augment and query it somehow. As an example of logic, we’ll introduce wumpus world:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;An agent runs through the maze, avoiding the wumpus and pits, while looking for gold. Additionally, the agent can shoot the wumpus (but only has one arrow).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this world the agent can perceive things in adjacent squares to objects. World is discrete, static, single-agent, sequential, partially observable (arrow and wumpus states are not given by world). We’ll cover more next week. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Fast Algorithms for Mining Association Rules</title>
   <link href="/eecs584/2014/09/28/eecs584/"/>
   <updated>2014-09-28T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/28/eecs584</id>
   <content type="html">&lt;p&gt;Discovering associations in data allows businesses and scientists to form critical insights. In a business, association between buying habits of customers can create customized catalogues or online experiences. In a biology lab, associations between different chemicals or proteins can signify interesting research directions. Traditional association mining tends to be quite slow as it requires multiple passes over the data which is superlinear in time. This paper introduces &lt;em&gt;Apriori&lt;/em&gt; mining, and two derivative algorithms, &lt;em&gt;Apriori-Tid&lt;/em&gt; and &lt;em&gt;AprioriHybrid&lt;/em&gt; that reduces the number of passes over a database required to count frequent itemsets.&lt;/p&gt;

&lt;p&gt;The primary strength of this paper is the algorithm’s ability to use previous sets (apriori information) to quickly find larger sets. All of the algorithms introduced in the paper generate candidate itemsets by using only the itemsets found to be large in previous passes. Because any subset of a large itemset must be large, the algorithm can join smaller sets and prune the search space by removing subsets that are not large. The paper also introduces the Tid and Hybrid algorithms to improve performance when disk access is expensive or to reduce passes by sometimes using Tid and sometimes using the usual algorithm. &lt;/p&gt;

&lt;p&gt;The authors include the precise algorithms for their contribution, as well as a substantial look at the performance of the apriori algorithm versus the competing best methods. Not only does apriori execute faster, but it also scales better with database size, which is even more important in today’s world of big data sets. I like that the authors show the linearity of their algorithm via experimental evidence. While not as rigorous as a formal proof, it would be enough to make me confident choosing their algorithm in a product.&lt;/p&gt;

&lt;p&gt;Association mining has inherent weaknesses that would require additional extensions to address. Common associations may not qualify as interesting, especially when associations are a result of the domain (for example, if a store requires two items be bought together). Additionally, data must be categorical, although this limitation can be fixed via binning. One final weakness that comes to mind is that many times there are associated quantities with items (such as amount of gas purchased). This algorithm would fail to incorporate this value, and simply repeating the item in the dataset will not work for an item with such fine granularity.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals</title>
   <link href="/eecs584/2014/09/27/eecs584/"/>
   <updated>2014-09-27T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/27/eecs584</id>
   <content type="html">&lt;p&gt;Traditional SQL statements for business intelligence tend to require data aggregation in the query results. This is generally done with GROUP BY, which allows the user to organize the returned data by certain column values. This presents a problem when attempting to “roll-up” data, or aggregate data over multiple levels. Note that unless otherwise stated, this review will be written from the perspective of the time of writing. The current SQL standard now supports datacubes, a testament to the paper’s influence. &lt;/p&gt;

&lt;p&gt;This paper’s primary contribution is the “CUBE” operator, which allows a user to aggregate table results over multiple dimensions, all the way to the largest possible view. The authors provide intuition to the results of this operation via the “ALL” keyword which acts as a placeholder in tuples where the actual value of the row is in a different column. For example, the final row in a 3d data cube query might be “ALL ALL ALL 941”, where 941 represents a sum over all the other columns, and the ALL values are inconsequential (it makes no sense to have a value for an aggregation of categorical data, for example). The paper also introduces the “ROLLUP” operator, which allows a roll-up report by producing super-aggregates instead of a full cube. The paper also provides the algebra of the operators it discusses (GROUP BY, ROLLUP, and CUBE), as well as providing a syntax proposal for them.&lt;/p&gt;

&lt;p&gt;One interesting thing to note is that the CUBE is the spanning case for the ROLLUP and GROUP BY operators; that is that ROLLUP and GROUP BY are the degenerate cases. The paper ends with a discussion of the ALL value, including the decision to note actually include it in the recommendation, and a look at other details of implementation. These details include decorations (columns dependent on grouping columns not in the GROUP BY), snowflake queries (a schema with many aggregation granularities), and details on how to compute ROLLUPs and CUBEs.&lt;/p&gt;

&lt;p&gt;Strengths:&lt;/p&gt;

&lt;p&gt;This paper acts as the seminal paper for data cubes, and as a result has had much influence on RDBMS design in the years after its publication. Including the implementation details is especially important, and the look at various classes of aggregate functions (distributive, algebraic, holistic) would be extremely helpful to me were I to implement this functionality myself. From a readability standpoint the paper does a good job of including examples and details.&lt;/p&gt;

&lt;p&gt;Weaknesses:&lt;/p&gt;

&lt;p&gt;This paper was written in 1997, when data analytics was still in its infancy. For larger datasets, like those common in modern companies, the data cube presents a massive amount of data to the user. While good for small queries, data cubes likely present too much information to be useful. Additionally, the paper fails to adequately discuss the expansion of the cube to higher dimensions, although it is assumed the cube is actually a hypercube. I expect, however, that very high dimensional cubes come with additional challenges such as how to aggregate the data in a meaningful way. Lastly, the cubes as they are are not flexible, requiring the aggregate functions defined by the DBMS. The authors assert that they attempted to allow the user to specify aggregate functions, but found it too troublesome. In modern business analytics, this assumption is probably untrue.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Data Analysis I</title>
   <link href="/eecs598/2014/09/25/eecs598/"/>
   <updated>2014-09-25T00:00:00-04:00</updated>
   <id>/eecs598/2014/09/25/eecs598</id>
   <content type="html">&lt;h2 id=&quot;classification&quot;&gt;Classification&lt;/h2&gt;
&lt;p&gt;Predicting outcomes&lt;/p&gt;

&lt;h3 id=&quot;nearest-neighbors&quot;&gt;Nearest Neighbors&lt;/h3&gt;
&lt;p&gt;Simple model, look at what happened to similar data. We can use a few metrics to determine “distance”, but in the end it’s all the same idea. Note we should normalize the data because otherwise large distances screw with us.&lt;/p&gt;

&lt;h3 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h3&gt;
&lt;p&gt;Another basic model, attempts to split data based on features. We use entropy to quantify the amount of disorder in a set, aka the amount of information required to represent a node. The feature space of the decision tree is partitions aligned with the axis, which is hard for diagonals or etc.&lt;/p&gt;

&lt;h3 id=&quot;bayesian-learning&quot;&gt;Bayesian Learning&lt;/h3&gt;
&lt;p&gt;We want to represent things probabilistically and choose our hypothesis with the max probability. We’ll use Baye’s rule to determine probabilities of each possible classification. Because we decompose into separate P’s, we make a big assumption about independence, but this may not be true. On the other hand, we actually get a probability out.&lt;/p&gt;

&lt;h3 id=&quot;svm-support-vector-machine&quot;&gt;SVM (Support Vector Machine)&lt;/h3&gt;
&lt;p&gt;We want to separate the data in the &lt;em&gt;best way&lt;/em&gt; possible, aka provide the maximum separating hyperplane. We find the “support vectors” that support the hyperplane, we only need these points to classify. We can add in a kernel function to extrapolate to higher dimensions. The big SVM advantage is that you avoid overfitting my maximizing the hyperplane.&lt;/p&gt;

&lt;h2 id=&quot;regression&quot;&gt;Regression&lt;/h2&gt;
&lt;p&gt;Predicting values&lt;/p&gt;

&lt;h3 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;Good for when there’s a range over which things matter. At the extremes things don’t matter as much. This is good for many medical applications. We’ll solve this via maximum likelihood estimation. We want to find \beta values for our probability function.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - H-Store&#58 A High-Performance, Distributed Main Memory Transaction Processing System</title>
   <link href="/eecs584/2014/09/25/eecs584/"/>
   <updated>2014-09-25T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/25/eecs584</id>
   <content type="html">&lt;p&gt;In this demo paper from 2008, the authors demonstrate H-Store, a shared nothing main memory database system. In their previous paper on OTLP and main memory databases, the authors lay the groundwork for this database and describe some of the problems of traditional DMBSes and potential new architectures to be considered. Specifically, the authors argue that extending traditional structures such as locks and buffer managers to main memory databases is detrimental, and that an entirely new architecture could be constructed to better take advantage of the capabilities inherent in an MMDB. &lt;/p&gt;

&lt;p&gt;In H-Store, the authors completely eliminate locking and latching by distributing the filesystem throughout multiple sites, and storing disjoint sets of data among them. Because each H-Store site is single threaded and operates on its own memory, the system is able to remove locks entirely while maintaining parallelism via multiple sites per CPU and multiple CPUs per node. This architecture also allows H-Store to easily redistribute processing load across the cluster - incoming transactions are handled at each site and then, if possible, rerouted to their proper sites.&lt;/p&gt;

&lt;p&gt;Another important architecture decision of H-Store was the separation of deployment and runtime. During system deployment, the database administrator must predefine transactions in the form of &lt;em&gt;stored procedures&lt;/em&gt;. By precompiling transactions, H-Store is able to improve performance (by allowing direct access to memory), as well as eliminate locking (by reasoning about data layout at dispatch time). In addition, after construction, the database is optimized to the parameters at deployment and will not fare well when adding new procedures (although this is possible). This separation of deploy and runtime is a point of strength and weakness in H-Store, and allows the database to trade performance for flexibility. &lt;/p&gt;

&lt;p&gt;H-Store uses redundancy to operate without any disk storage at all. This presents a potential problem that goes un-addressed in this paper - what happens if power fails to a data center. The way H-Store is described here, all data will be permanently lost because there is no disk storage at all. Redundancy assumes individual sites or nodes may fail, but that in the end there will be enough of the cluster up to recover the lost data. For a production system, this would be catastrophic, and a real-world deployment of H-Store would certainly require checkpointing or snapshotting to disk. Another weakness of this system is its deploy-time capability. Not only is everything required at deploy time, but there is no automatic database layout design, which leaves work to the administrator. This is a weakness of the system, although the authors do acknowledge that the limitation exists and there is active work to remedy the problem.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Constraint Satisfaction</title>
   <link href="/eecs492/2014/09/25/eecs492/"/>
   <updated>2014-09-25T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/25/eecs492</id>
   <content type="html">&lt;p&gt;How do we introduce constraints to solve problems where the variables are linked. A few definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; - a set of variables &lt;script type=&quot;math/tex&quot;&gt;\lbrace X_1, ..., X_n \rbrace&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; - set of domains &lt;script type=&quot;math/tex&quot;&gt;\lbrace D_1, ..., D_n \rbrace&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; - a set of constraints&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A few more definitions in a constrained search problem, describes how variables are assigned.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consistent assignment - violates no constraints&lt;/li&gt;
  &lt;li&gt;Partial assignment - assigns values to some variables&lt;/li&gt;
  &lt;li&gt;Complete assignment - assigns values to all variables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can create a constraint graph that represents variables and constraints. Once we introduce constraints, we vastly decrease the search space. &lt;strong&gt;Constraints describe relationships&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;types-of-constraints&quot;&gt;Types of constraints&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Precedence constraints - things must be done in order&lt;/li&gt;
  &lt;li&gt;Disjunctive constraints - limited resources affect order&lt;/li&gt;
  &lt;li&gt;Time constraints - domain of others shrinks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consistency&quot;&gt;Consistency&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Node consistency - All values in domain satisfy unary constraints&lt;/li&gt;
  &lt;li&gt;Arc consistency - Consistency between the domain and range&lt;/li&gt;
  &lt;li&gt;Path consistency - Uses implicit constraints inferred by triples of variables&lt;/li&gt;
  &lt;li&gt;K-consistency - Generalization of consistency with more relationships&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of multiple constraints, we can propagate constraints throughout the network. For arc consistency, we can use AC3. Path consistency lacks enforcement of unary constraints, and for this reason it’s &lt;strong&gt;not&lt;/strong&gt; arc consistent. &lt;/p&gt;

&lt;h3 id=&quot;solving-csps&quot;&gt;Solving CSPs&lt;/h3&gt;
&lt;p&gt;The simplest approach is uninformed search, but the search space is huge. Additionally, ordering doesn’t matter. We need to combine search and consistency, we’ll use &lt;em&gt;backtracking&lt;/em&gt; to use search once we’ve reduced the space. The key is that consistency depends only on assigned variables, eg order doesn’t matter. At a basic level this is like depth first search. In backtracking we use a heuristic to choose what to expand first (minimum remaining value).  &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Main Memory Database Systems - An Overview</title>
   <link href="/eecs584/2014/09/24/eecs584/"/>
   <updated>2014-09-24T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/24/eecs584</id>
   <content type="html">&lt;p&gt;Main memory database systems offer notable benefits for small databases such as online transaction processing systems. These benefits include faster queries, higher throughput, and simplified system architecture. Written in 1992, this paper has powerful and forward looking insights about the architecture decisions that need to go into a main memory database system. This paper is ahead of its time, as many of the concepts could not be applied for years after publication - main memory was falling in price but fitting large customer databases in memory for a reasonable price was still almost a decade away. Indeed, it wasn’t until recently that modern juggernauts such as Microsoft SQL and Oracle started supporting memory resident systems. This paper also correctly predicted non-volatile main memory years before it had come to market.&lt;/p&gt;

&lt;p&gt;This paper acts almost as a survey to main memory database systems, and looks at each part of a traditional DMBS and how it relates to a MMDB. The recurring observation made by the authors is that the disk is no longer the bottleneck of the system. This has several implications. First, contention on disk is almost eliminated, and as a result we can use larger lock granule sizes. Additionally, because main memory no longer favors sequential access, we can utilize different index structures and almost completely remove buffer management. Although beneficial, MMDBs do present some problems. Most critically, because memory is volatile, there must exist some persistent storage in case of power failure or crashes. Logging can be done in group commits to a disk, or using some sort of “stable main memory”, which was a hypothetical concept when this paper was published. The database also flushes the entire system to disk occasionally, in the form of snapshots or checkpoints. The problem of how to store data on disk is also briefly mentioned, although the author’s main point here is simply that there is no analogue to migration in a traditional DBMS. In the last section of this paper, the authors examine several main memory database systems available at the time of writing. However, being such an old paper (in terms of technology), much of this section is outdated.&lt;/p&gt;

&lt;p&gt;This paper is quite ahead of its time, but it still has a few weaknesses. First, some of the technology discussed did not exist in 1992 at the time of publication. Non-volatile memory is just now coming out. While this is okay from a theoretical perspective, the paper doesn’t point out that this technology is undeveloped. I am a bit surprised that the authors didn’t foresee the use of shared nothing systems in this paper - the idea of shared nothing wasn’t unheard of at the time of writing and the authors likely were aware that extremely high density main memory would be expensive, even in the future. Finally, the authors advocate T-trees for memory resident database systems, but research has shown that T-trees may not actually offer noticeable benefits to a DBMS. The authors were likely simply using the results of T-tree research.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Search Implementation</title>
   <link href="/eecs492/2014/09/24/eecs492/"/>
   <updated>2014-09-24T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/24/eecs492</id>
   <content type="html">&lt;h2 id=&quot;generic-best-first&quot;&gt;Generic Best First&lt;/h2&gt;
&lt;p&gt;Start with a queue, pop the most promising node according to f. If n is a goal node, return it otherwise add successors. The choice of f changes the search algorithm.&lt;/p&gt;

&lt;p&gt;We’ll say each node has a few attributes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Depth&lt;/li&gt;
  &lt;li&gt;g - Actual path cost from root to here&lt;/li&gt;
  &lt;li&gt;h - Estimated cost from this node to goal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So how to do choose f?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BFS: f = depth&lt;/li&gt;
  &lt;li&gt;DFS: f = 0-depth&lt;/li&gt;
  &lt;li&gt;UCS: f = g&lt;/li&gt;
  &lt;li&gt;GBFS: f = h&lt;/li&gt;
  &lt;li&gt;A-star: f = g+h&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - OTLP Through the Looking Glass and What We Found There</title>
   <link href="/eecs584/2014/09/23/eecs584/"/>
   <updated>2014-09-23T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/23/eecs584</id>
   <content type="html">&lt;p&gt;Online transaction processing is an important subset of general database management systems that generally deals with business or commercial transactions. OTLP systems require high throughput and low latency, as most serve many users and make frequent updates. However, because the number of customers and their transaction information does not scale with Moore’s law, OTLP databases are relatively small by today’s standards. Written in 2008, this paper argues that it’s time for a complete overhaul of OTLP database systems. The authors prove their point by completely removing most of the functionality of the Shore OTLP database to show how much code is wasted.&lt;/p&gt;

&lt;p&gt;A key point in this paper is the use of main memory databases. In contrast to a traditional DBMS which lives on disk, a main memory database keeps all data in memory. This reduces or eliminates many problems with disk based DBMS systems, specifically rendering most of the disk access optimizations obsolete. Many of the architecture observations the authors make in this paper are implemented in their H-Store database system. The paper discusses the idea of a cluster of shared nothing machines that are memory resident and single threaded. Each CPU core acts as its own transaction manager, with its own memory space to completely eliminate locking. Instead of logging to disk, these systems would simply migrate data to available sites when there is a failure, thus maintaining availability without disk overhead.&lt;/p&gt;

&lt;p&gt;The authors attempt to demonstrate the amount of CPU instruction that could be saved by eliminating and optimizing Shore. They remove sections one at a time, including locking, latching, multithreading, buffer management, and bookkeeping. At the end, they have a bare minimum database system that is both simpler and much faster. The resulting database system can be seen as a proof of concept for the performance of the H-Store system that the lab later develops. This paper lays the groundwork for H-Store and presents concrete evidence that much of the machinery of a modern DMBS is outdated and performance of memory-resident DBMSes can be improved by adapting a new architecture. In the final section of the paper the authors concretely state their vision for a new OTLP engine, and most of these features appear in H-Store.&lt;/p&gt;

&lt;p&gt;This paper fails to address catastrophic failure events in main memory DBMSes, for example if the power goes out in a database center. I assume that a form of checkpointing or snapshotting would take place in any production main memory database, but this paper fails to address this. Additionally, even by removing most of the code from Shore, the authors fail to consider pre-compiled transactions in their speed comparisons or recommendations. A main memory, distributed DBMS must trade flexibility for speed and consistency. This tradeoff occurs in H-Store, and indeed must take place to remove locking and latching. However, this paper removes locking and latching without mentioning the flexibility trade offs that must take place in order to produce a production system similar to the one they are advocating.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Adversarial Search</title>
   <link href="/eecs492/2014/09/23/eecs492/"/>
   <updated>2014-09-23T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/23/eecs492</id>
   <content type="html">&lt;h2 id=&quot;two-player-games&quot;&gt;Two Player Games&lt;/h2&gt;
&lt;p&gt;For now we’ll deal with one other agent only, in fully observable deterministic environments. How do we characterize the other agent? We say the opponent is exactly opposed to us. We’ll say it’s a &lt;em&gt;zero sum game&lt;/em&gt;, meaning the rewards don’t change game to game.&lt;/p&gt;

&lt;p&gt;These problems are hard, because the search trees are generally massive and exponential. How do we deal with this?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New approaches must define the optimal move differently.&lt;/li&gt;
  &lt;li&gt;We need to prune our search tree.&lt;/li&gt;
  &lt;li&gt;Likely need a Heuristic evaluation function.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;minimax&quot;&gt;Minimax&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Max ALWAYS goes first&lt;/strong&gt; (this will be on the exam). Also note there are rewards &lt;em&gt;and&lt;/em&gt; penalties for the winner and loser.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Terminal test - Returns true when the game is over.&lt;/li&gt;
  &lt;li&gt;Utility - Defines a numeric value for a game that ends in a terminal state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Game tree now models both players explicitly. &lt;/p&gt;

&lt;h4 id=&quot;example---tic-tac-toe&quot;&gt;Example - Tic Tac Toe&lt;/h4&gt;
&lt;p&gt;We’ll say a win is +1 and a loss is -1, a tie is 0. &lt;/p&gt;

&lt;h2 id=&quot;decisions-in-minimax&quot;&gt;Decisions in Minimax&lt;/h2&gt;
&lt;p&gt;We will assume the opponent is playing optimally. Then we re-run minimax next turn.&lt;/p&gt;

&lt;h2 id=&quot;alpha-beta-pruning&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha-\beta&lt;/script&gt; Pruning&lt;/h2&gt;
&lt;p&gt;Minimax is inefficient, has &lt;script type=&quot;math/tex&quot;&gt;O(b^m)&lt;/script&gt; time complexity. We’ll prune the tree by ignoring the moves &lt;em&gt;min&lt;/em&gt; will ignore. Pruning like this lets us remove entire subtrees in many cases. This depth first search depends heavily on the ordering of the tree. Alpha-beta pruning can generally double our search depth!&lt;/p&gt;

&lt;h2 id=&quot;evaluating-in-real-time&quot;&gt;Evaluating in Real Time&lt;/h2&gt;
&lt;p&gt;With deep trees we can’t even get to the end. We’ll need to introduce an evaluation function to score a mid-level node. The heuristic should be strongly correlated with a good outcome. Introducing this adds uncertainty, however. Uncertainty doesn’t change anything about the environment, but changes the computation. &lt;/p&gt;

&lt;p&gt;What if we evaluate the function via features?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(s) = \sum \limits_{i=1}^n w_i f_i(s)&lt;/script&gt;

&lt;p&gt;This misses dependencies however. We can use a heuristic in place of final state once we hit a certain depth. &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Evaluation Function - Guess of utility function&lt;/li&gt;
  &lt;li&gt;Utility Function - Final state of a game&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stochastic-games&quot;&gt;Stochastic Games&lt;/h2&gt;
&lt;p&gt;We can extend games to include random elements but this increases complexity by a large margin. We’ll add &lt;em&gt;chance nodes&lt;/em&gt; and calculate expected values. This is called &lt;strong&gt;expectiminimax&lt;/strong&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - On Optimistic Methods of Concurrency Control</title>
   <link href="/eecs584/2014/09/21/eecs584/"/>
   <updated>2014-09-21T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/21/eecs584</id>
   <content type="html">&lt;p&gt;Traditional database systems require stringent concurrency controls the ensure that there is no data loss or corruption when multiple agents are accessing the system. In most modern DBMSes, this is managed through the use of locks, which provide exclusive access to some subset of the database. While this approach manages concurrency while preventing corruption, it is inherently a heavy overhead system, and locks introduce complexity at both runtime and during system design. &lt;/p&gt;

&lt;p&gt;This paper proposes a new approach to concurrency, which it refers to as optimistic methods of concurrency control. The new method is motivated by the observation that in most cases, locks aren’t actually required to prevent corruption. Only in a small number of cases will locks actually be utilized, and the remainder of the time they simply create overhead and performance issues. Thus, the new system does away with locking entirely, writing and reading as if the database is being accessed only by a single system at a time and then using a strong validation algorithm to discard and retry operations that would result in an inconsistent state. The validation phase of the new method checks to see if the operations performed are serializable, and if they are not it assumes corruption occurred and aborts the transaction. It accomplishes this validation through the use of transaction numbers, which are assigned at the end of the read phase to maximize system throughput.&lt;/p&gt;

&lt;p&gt;The authors make other considerations, such as what to do when a validation repeatedly fails. This represents the worst case scenario for the performance of the system, and after a set number of tries the transaction is given exclusive access to the entire database, forcing it to complete. The paper examines the “critical section” of transactions, and examines cases of parallel operations (such as multiple CPUs and parallel validation). Finally, the authors do a rigorous analysis of how their method works on B-trees, including calculating the probability that a calculation requires a restart. I really like that the authors include this calculation, although the authors find a good case for their calculations. The optimistic approach seems like a good idea when domain knowledge tells you that you will likely not have many conflicts, but clearly performance will degrade significantly if this assumption isn’t true. As the authors assert, a technique combining locking and optimism would be ideal to support cases not on either extreme of the spectrum (which most real world cases would be). It would also be interesting to examine lock-free methods to supporting the other ANSI SQL isolation levels besides simply serializability, such as READ COMMITTED or UNCOMMITTED. It’s unclear whether or not the authors implemented the idea they proposed, but if they did I would have liked to see real world comparisons of access performance on both real and synthetic data.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - A Critique of ANSI SQL Isolation Levels</title>
   <link href="/eecs584/2014/09/20/eecs584/"/>
   <updated>2014-09-20T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/20/eecs584</id>
   <content type="html">&lt;p&gt;The ANSI SQL standard defines isolation levels in databases in terms of what is allowed to occur in the database. In this influential 1995 paper, the authors argue that this type of specification fails to properly describe isolation levels, and will allow certain corner cases to occur anyway. In the ANSI standard, phenomena are expressed in a written, English form that allows room for interpretation. This paper claims that isolation, instead of being expressed in terms of phenomena, should rather be expressed in terms of locking levels. The authors also characterize locking in a rigorous and more mathematically formal way that removes ambiguity from isolation levels. The mathematical formalism introduced to SQL isolation is an important contribution introduced by this paper, and terminology introduced here such as “loose” vs “strong” phenomena interpretation creates a framework that allows the authors to offer criticisms of the ANSI SQL standard.&lt;/p&gt;

&lt;p&gt;The mathematical tools introduced in this paper allow the authors to introduce additional phenomena types: cursor stability and snapshot isolation. Of particular importance is snapshot isolation, which is widely deployed in modern database systems. Snapshot isolation offers better performance than serializable isolation while offering most of the same protections against anomalies. The paper shows, however, that snapshot isolation is &lt;em&gt;not&lt;/em&gt; the serializable. This fact implies a failing of the ANSI SQL isolation levels - snapshot isolation exhibits no prohibited phenomena as defined by the ANSI standard, yet is not serializable and can still allow errors. The paper also offers a short description of other multi-version systems and compares them to snapshot isolation.&lt;/p&gt;

&lt;p&gt;I like that this paper offers such a comprehensive analysis of isolation levels and that it offers precise definitions where ANSI failed. I would be interested to see how database definitions such as ANSI have adapted in the face of new evidence. A cursory Google search indicated that although this paper has been well cited and well received, there has not been an overhaul in the ANSI isolation level standards. I also think that the “first-committer-wins” feature seems like an easy but limiting solution to preventing lost updates in the snapshot transaction system. In a system with heavy latency or transaction costs, it seems that the DBMS could possibly resolve conflicts automatically. This would require some sort of merging algorithm, which is beyond the scope of this paper and would have tradeoffs in lost accuracy. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Optimization of Sequence Queries in Database Systems</title>
   <link href="/eecs584/2014/09/19/eecs584/"/>
   <updated>2014-09-19T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/19/eecs584</id>
   <content type="html">&lt;p&gt;Time series data has important applications in finance, biology, business, and many other disciplines. Additionally, time series analysis has gained traction in the machine learning and data mining community as a method of non-linear system analysis. This paper, written in 2001 at UCLA, offers a look at how to deal with time series data in the context of a database query. Especially in cases where large amounts of data are stored over time (eg stock market), efficient and easy queries on the database should be possible. For traditional DBMSes and standard SQL, it is difficult to concisely and efficiently create time series queries. &lt;/p&gt;

&lt;p&gt;This paper begins by proposing an extension to the SQL language, called SQL-TS. This superset of SQL includes several constructs to deal with time series data. Of most importance is the FROM .. AS clause that will preserve time series ordering in query requests. This clause can contain star (wildcard) terms to allow for an variable amount of data returned while maintaining proper ordering. This is a powerful construct in terms of expressiveness, and to me is the major syntactical contribution of this paper. SQL-TS also includes CLUSTER BY and SEQUENCE BY clauses to simplify return ordering.&lt;/p&gt;

&lt;p&gt;The most important contribution of this paper is the actual implementation of an efficient search method in time series databases, modeled after the Knuth-Morris-Pratt string search algorithm. The KMP algorithm uses data available in the search string itself to prevent backtracking. Starting from the beginning, the algorithm compares corresponding characters until matching fails. However, if the matching fails after &lt;em&gt;i&lt;/em&gt; matches, the algorithm remembers the previous comparisons to potentially skip comparing up to &lt;em&gt;i&lt;/em&gt; characters of the string its searching through. The paper introduces a heavily generalized method of the KMP string search that they call Optimized Pattern Search. This search generalizes KMP to support general predicates and more general objects. Importantly, the new algorithm also includes support for wildcard queries, which is a powerful feature of the SQL-TS language. The algorithm cleverly captures all pairwise relations between pattern elements by computing “precondition logic matricies”, and uses shifts within these matricies to define pattern matches.&lt;/p&gt;

&lt;p&gt;The experimental results in this paper are quite weak. The authors use a single experiment to test their algorithm. One of the primary strengths of the KMP algorithm is that is offers a good worst case runtime. General, random string searches will likely perform just as well with a naive search when compared to KMP. However, in corner cases, such as long repeating elements, the naive version will degrade immensely. The tests performed here are on real data only, and with complex queries that may or may not represent real world loads. There is also no mention of space complexity for this method, or a comparison of this methods space requirements to a naive pattern match (which, admittedly, would not support most functions of OPS). Lastly, I would be interested to see this algorithm of pattern matching compared to other work in the field. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Local Search</title>
   <link href="/eecs492/2014/09/18/eecs492/"/>
   <updated>2014-09-18T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/18/eecs492</id>
   <content type="html">&lt;h2 id=&quot;recap&quot;&gt;Recap&lt;/h2&gt;
&lt;p&gt;We’ll do a short recap. Maybe, but not likely short.&lt;/p&gt;

&lt;h3 id=&quot;informed-search&quot;&gt;Informed Search&lt;/h3&gt;
&lt;p&gt;Informed searches use data about the goal that we have that’s not part of the problem definition. For example, &lt;strong&gt;A-star&lt;/strong&gt; search uses a heuristic function to work towards a goal, and applies the node cost as &lt;em&gt;path cost + heuristic cost&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Quick proof: Prove A-star is optimal. This will likely be on the exam.&lt;/p&gt;

&lt;p&gt;First prove A-star is non-decreasing cost
Then argue about the exploration of the path cost&lt;/p&gt;

&lt;p&gt;There are also memory bound heuristic searchs. &lt;strong&gt;Iterative deepening&lt;/strong&gt; A-star is like iterative deepening but cutoff is now f_cost and not the depth. We can also use &lt;strong&gt;recursive best first search&lt;/strong&gt; to mimic best first search but cutting memory by only storing one child.&lt;/p&gt;

&lt;p&gt;Heuristics can be designed in several ways, one way is to relax the problem. For example, we can solve only part of the problem and create a pattern database with solutions to all the possible subproblems.&lt;/p&gt;

&lt;h2 id=&quot;local-search&quot;&gt;Local Search&lt;/h2&gt;
&lt;p&gt;Previously we explored the search space in a systematic way. When a goal is found the path is the solution. In many cases we actually don’t care about the path, the final product is important. During a local search, we evaluate and modify the current state.&lt;/p&gt;

&lt;p&gt;Example - 8 queen problem. We only care that this is solved, not the path to the solution. &lt;/p&gt;

&lt;p&gt;In local search, we only maintain the current node, and we don’t retain path. This allows us to solve both massive search space problems as well as pure optimization problems. Complete if we reach the goal always, optimal if we always in the local maximum.&lt;/p&gt;

&lt;h3 id=&quot;hill-climbing&quot;&gt;Hill Climbing&lt;/h3&gt;
&lt;p&gt;We just move in the direction of the gradient and try and find the nearest peak. This search is not globally optimal unless the objective function is convex. How can we hill climb with the 8 queen problem?&lt;/p&gt;

&lt;p&gt;We’ll define a heuristic that keeps track of how many queens conflict, and just go until we’ve hit zero, headed towards the minimum. In some cases we hit the local maximum though, and this isn’t the solution. Classic optimization problem, but how to solve? We can offer stochastic solutions to hill climbing, which generally require many runs. Unfortunately this doesn’t offer a worst case guarantee.&lt;/p&gt;

&lt;h3 id=&quot;simulated-annealing&quot;&gt;Simulated Annealing&lt;/h3&gt;
&lt;p&gt;Allows a transition from one non-local state to another. Probability of this transition falls over time. We pick a random move, if it improves we accept it. If not, we either ignore it, or accept it with probability P. &lt;/p&gt;

&lt;h3 id=&quot;local-beam-search&quot;&gt;Local Beam Search&lt;/h3&gt;
&lt;p&gt;We track k states instead of only one. Generate k initial states and generate successors for each state, saving only the best. This is basically a best first search that occurs in local space instead of global.&lt;/p&gt;

&lt;h3 id=&quot;genetic-algorithm&quot;&gt;Genetic Algorithm&lt;/h3&gt;
&lt;p&gt;We start with randomly generated states and rate each. Then we combine parents and randomly mutate some of them. The process repeats as we proceed, discarding agents with low fitness and keeping better ones. In genetic algorithms, the fitness function is quite important, similar to the heuristic in informed search. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - The Design of an Acquisitional Query Processor For Sensor Networks</title>
   <link href="/eecs584/2014/09/17/eecs584/"/>
   <updated>2014-09-17T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/17/eecs584</id>
   <content type="html">&lt;p&gt;Sensors for long term, low power monitoring present different challenges than traditional database workloads. This paper addresses common problems encountered when creating and deploying these networks, and offers solutions by the way of simple SQL extensions as well as low level query processing. Written in 2003, the issues discussed in this work are even more relevant nowadays - sensors are becoming even cheaper and as the “Internet of Things” looms on the horizon we must confront the reality of large scale and low power ad-hoc networks becoming commonplace. While the techniques in this document pertain mainly to scientific monitoring, they can be applied to home automation, building management, commercial operations, and an endless variety of other uses.&lt;/p&gt;

&lt;p&gt;From the writing, this paper appears to consider itself among the first to truly consider a sensor network as a separate entity. Previous works attempt to offer more general solutions such as saving power on transmitting or processing. The novel idea explored in this paper is the idea that a sensor network can control its rate of sampling, and that many times the sampling rate is flexible with respect to other, more important goals (such as network longevity). The authors introduce the query language for their acquisitional query processor, which they call acquisitional query language. The AQL is basically an extension of SQL that allows specification of sampling rates, as well as windowing and view support (which the paper calls materialization points”. The database in the paper, TinyDB, also supports event based queries that trigger based on external events. This is critical to maintaining long battery life (because polling is so expensive) and already supported via interrupts in most hardware. The query processor has a general idea of the power used for each query, and can optimize based on expected power usage both for queries and to ensure longevity of the system (ie set a sample rate based on lifetime).&lt;/p&gt;

&lt;p&gt;The paper also explores ways to connect the sensor network, which is done via semantic routing tree. By storing metadata and knowing their parents properties, nodes can conserve power by avoiding unnecessary processing. I really like that this paper addresses these issues because it shows the authors are working on a very specific problem and not just trying to conserve power in a general network system. The sensor network can also adapt to heavy load by changing its sampling rate or choosing which tuples to drop in a congested network. While a short section, this part of the paper is a novel solution to network congestion and power reduction. By reducing or eliminating less useful data, the network can eliminate backlog and congestion while maintaining a good picture of the final data. &lt;/p&gt;

&lt;p&gt;While this paper did a good job of addressing the requirements of an ad-hoc sensor network, I think it could do better at implementation in some cases. Dropping data on a packet by packet basis is done crudely - data compression could make a big impact on the quality of the data the network receives. The authors do mention this fact and concede that the low processing power of the network prohibits expensive operations such as Fourier analysis. The authors also make assumptions about the data sent via sensor networks, assuming most of it is real valued and univariate. This paper works very well in scientific monitoring domains, but fails to extend to cases where higher dimensional data or textual data is required. While shortsighted, this is most likely because the authors had a specific application in mind and possibly failed to see the explosion of sensors and low energy devices that would occur in the future.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Informed Search</title>
   <link href="/eecs492/2014/09/16/eecs492/"/>
   <updated>2014-09-16T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/16/eecs492</id>
   <content type="html">&lt;h2 id=&quot;review-from-last-time&quot;&gt;Review from Last Time&lt;/h2&gt;
&lt;p&gt;Last time we looked at breadth first search and uniform cost search, searches that are generally complete and optimal (UCS is optimal), but are quite time and especially space complex. Important difference to remember:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breadth First&lt;/strong&gt; - Goal test at generation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Uniform Cost&lt;/strong&gt; - Goal test at expansion&lt;/p&gt;

&lt;p&gt;We also looked at depth first search, which is not optimal but trades off for decreased space complexity. The DFS is complete only if used as a graph search, because in a tree search we could get stuck in a loop. Also not complete in infinite search spaces, obviously. If we limit the depth of DFS we can fix the problem with unbounded trees.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;DFS-limited&lt;/strong&gt; - Not complete, not optimal, but time complexity, while still exponential, is now &lt;script type=&quot;math/tex&quot;&gt;O(b^L)&lt;/script&gt; where L is the max depth. Same thing with space complexity, now &lt;script type=&quot;math/tex&quot;&gt;O(bL)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Iterative Deepening&lt;/strong&gt; - Use DFS-limited but instead of fully exploring, expand the depth limit.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bidirectional Search&lt;/strong&gt; - Search from both the goal and the start, saves space by searching from the goal.&lt;/p&gt;

&lt;h2 id=&quot;informed-search&quot;&gt;Informed Search&lt;/h2&gt;
&lt;p&gt;We consider “best first search”, where we evaluate nodes to expand based on an evaluation function. We exploit an f(n) to hopefully find the goal faster. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Heuristics&lt;/strong&gt; - Uses information not in the problem definition, where h(goal) = 0. This function is critical to a good search. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Greedy Best-First Search&lt;/strong&gt; - Expands the node closest to goal. Not necessarily optimal.&lt;/p&gt;

&lt;h2 id=&quot;a-star-search&quot;&gt;A-star Search&lt;/h2&gt;
&lt;p&gt;Combines the path-cost as well as a heuristic that looks at straight-line distance to the goal.&lt;/p&gt;

&lt;p&gt;Short time out for admissibility. An admissible heuristic never overestimates the cost to reach the goal, eg &lt;script type=&quot;math/tex&quot;&gt;h(n)&lt;/script&gt; &amp;lt; minimum achievable cost from n to goal. Consistency requires monotonic heuristic; it’s a stronger version of admissibility that requires consistency from each next node also.&lt;/p&gt;

&lt;p&gt;Tree-search is optimal if heuristic is admissible, and graph-search is optimal if consistent. The proof is important but this lecture was piss poor at explaining it.&lt;/p&gt;

&lt;p&gt;If the heuristic is admissible, A-star is complete, optimally, and optimally efficient.&lt;/p&gt;

&lt;p&gt;If we are &lt;em&gt;memory bound&lt;/em&gt;, we can use either iterative deepening for A-star, or recursive best first search. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Automated Selection of Materialized Views and Indexes for SQL Databases</title>
   <link href="/eecs584/2014/09/14/eecs584/"/>
   <updated>2014-09-14T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/14/eecs584</id>
   <content type="html">&lt;p&gt;Modern relational and object-relational database systems support materialized views, which provide a more flexible way to encapsulate database structure. These views can offer significant performance gains when properly configured, because unlike indexes they can span multiple tables and have selections or GROUP BY over multiple columns. This paper, written by the Microsoft SQL research team, introduces a solution to automatically select both views and indexes based on “training data” provided by a database administrator. Unlike previous work, this paper focuses on combining index and view selection, as well as providing novel work at selecting materialized views.&lt;/p&gt;

&lt;p&gt;The architecture of index and view selection is based on the Microsoft SQL server, and at times reuses portions of that codebase, specifically the query cost estimator and more generally the query optimizer. Because the search space of possible materialized view candidates is quite large, the authors seek to find interesting table subsets via a cost metric that considers the total cost of all queries in the workload and attempts to normalize that by the size of the tables in Q and T, where Q are tables referenced in the query and T is the table-subset. This is a requirement because some tables may be so small that building a view on them would be a waste of time - queries on the table are fast already. The authors also use the Microsoft SQL optimizer to prune the possible views, and then merge the views to try and include views that will answer multiple queries and thus be the most space efficient representations.&lt;/p&gt;

&lt;p&gt;Unlike previous works, this paper examines both indexes and materialized views jointly when considering the retained options. This allows the optimizer to choose combinations of views and indexes that may be pruned by examining only one type of structure.&lt;/p&gt;

&lt;p&gt;In contrast to many academic research papers, the techniques in this work have already been incorporated into a professional database tool. This allows the authors to create an impressive set of benchmarks and performance statistics. The authors provide convincing evidence that their methods are superior and robust, testing their finished product on several synthetic and real world datasets. An argument could be made that their datasets were not large enough for real world data, but because the competing techniques are so inefficient, the tuning wizard created in this paper is sometimes the only possible option. I also like that the authors offer not only possible future directions, but also point to specific papers when discussing open problems remaining in their area.&lt;/p&gt;

&lt;p&gt;While a strong paper, I would have liked to see an overview of the Greedy(m,k) algorithm referenced several times throughout the paper. The authors did provide reference material, but based on the importance of the algorithm it would be beneficial to provide at least a superficial overview. I also think it’s important to remember that because this is a paper that relates directly to a product, the authors have implicit bias towards their own solution. There is little discussion of competing commercial solutions such as Oracle or DB/2; although the paper mentions briefly that the competition has automated tuning available, it never compares its techniques directly.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - An Overview of Query Optimization in Relational Systems</title>
   <link href="/eecs584/2014/09/13/eecs584/"/>
   <updated>2014-09-13T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/13/eecs584</id>
   <content type="html">&lt;p&gt;The declarative SQL language is currently the lingua franca of modern relational, and occasionally non-relational, database systems. Converting such a high level syntax to commands in the DBMS is a difficult task as there are generally multiple representations for a given high level query, and each representation has different and sometimes highly varying performance. This paper seeks to present the basics of query optimization in relation databases. As the paper was written and published by Microsoft, we can assume there are implicit biases towards Microsoft SQL server, but the paper does a good job of addressing historical precedents.&lt;/p&gt;

&lt;p&gt;The paper begins by providing a discussion of the System-R optimization framework, which could be considered the seminal work in query optimization, as well as SQL in general. System-R was developed by IBM as the first implementation of SQL, and thus their dynamic programming based query optimizer acted as an example for subsequent query optimization attempts. System-R provides powerful but limited query optimization for join ordering, and most implementations of SQL today use a variant of the dynamic programming idea for join ordering optimization. Unfortunately, current databases support optimization for other operators. For example, select-project-join queries may be flattened or outer joins may be reordered. The optimizer may attempt to collapse multi-block queries into a single block via multiple different methods. &lt;/p&gt;

&lt;p&gt;The author provides an overview of cost estimation, and explains how most cost optimization relies on database statistics collected over the life the system, such as histograms or correlation tables. This section is hand waved over, and very little actual information about cost estimation is given. This is likely because cost estimation is worth multiple papers alone, and the author felt he could not adequately present most of the material. The paper presents short looks at two query optimization projects - the query graph model based Starburst, and the goal driven Volcano/Cascades system. Finally, there is a short look at modern problems such as parallelism and user-defined functionality (like, for example, PostgreSQL). I really appreciated that the author included these looks forward as it allows the paper to stay fairly modern even years later. I also felt the paper was strong in examples, although the way these examples were presented had room for improvement. The examples were all in technical terms and understanding them took a significant amount of time. However, for the example with a visual aid I was able to understand the core concept quickly. More graphs and visuals would have greatly improved the accessibility of this paper, which is important in a survey paper like this.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper Review - The Google File System</title>
   <link href="/eecs584/2014/09/12/eecs584/"/>
   <updated>2014-09-12T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/12/eecs584</id>
   <content type="html">&lt;p&gt;This paper details the Google File System, a distributed, fault tolerant file system in use by Google to handle their massive amounts of data on commodity hardware. The GFS uses a master-node configuration, running many small “chunkservers” on commodity Linux machines and a master server to coordinate operations between those nodes. The file system runs in user space (ie is not a kernel process) and &lt;em&gt;expects&lt;/em&gt; some nodes to be in a failure state at some time. This expectation allows for scalability and a highly fault tolerant system. &lt;/p&gt;

&lt;p&gt;Clients communicate first with the master server, which holds in memory a hash of each file path to the corresponding chunk location (or locations as the data is duplicated on multiple machines at all times). The master server returns the server(s) location(s) and the client does the majority of its communication with the chunkserver directly. This maximizes system throughput, as each node uses its own Ethernet connection and there is no bottleneck at the master server for many simultaneous clients. This high throughput generally comes at the expense of latency. The chunkservers also supports fast, simultaneous writes to a single file via “record appends”, which guarantee only that the data will be written, and not in what order.&lt;/p&gt;

&lt;p&gt;The master servers themselves (plural because even though one master runs at a time there are duplicates to prevent data loss) manage the network and chunk layout. The master holds metadata about each chunk, and requires each chunkserver to periodically check in with a “heartbeat” message. The master allows “snapshots” to be made by creating a second pointer to a chunk and requiring the chunkserver to duplicate that chunk at next access. Additionally, the master manages garbage collection by simply marking items as “deleted” and occasionally running a garbage collector to lazily delete the items. &lt;/p&gt;

&lt;p&gt;This paper is an interesting read because this is the paper written for a commercial system that was designed with specific industry goals in mind. As a consequence of being designed by Google, the GFS has very specific performance goals in mind. Google knows that most of their data will be written or read as a stream by many producers and consumers, and thus made tradeoffs to support this. The filesystem is not optimized for many small writes, although it would have been interesting if the paper examined these possibilities in their conclusions. Additionally, Google requires high throughput and dependency at the expense of hardware. Thus, the high level design emphasizes that hardware is cheap and downtime is expensive. For a home or small business user, this design is totally impractical because it requires so many machines (even the small test clusters use 16 nodes and 3 masters). &lt;/p&gt;

&lt;p&gt;The paper briefly mentions diagnostic tools that the GFS uses, such as RPC logs and other diagnostic logs, but does not disclose them or provide details. It would have been nice if the paper went into further detail, as diagnostics are important in production ready systems. I suspect that these logs are partially industry secrets, and Google feels no need to disclose their proprietary diagnostic systems in the paper. This weakness is a general weakness of all industry papers – the authors cannot and will not tell us certain things about their system because doing so would compromise the company’s proprietary data. Even in this paper, we are given very little information about the benchmark system’s actual function, as the production server and especially research server details are likely shrouded in secrecy by the company. &lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Data Representation</title>
   <link href="/eecs598/2014/09/11/eecs598/"/>
   <updated>2014-09-11T00:00:00-04:00</updated>
   <id>/eecs598/2014/09/11/eecs598</id>
   <content type="html">&lt;h2 id=&quot;time-vs-frequency-domain&quot;&gt;Time vs Frequency Domain&lt;/h2&gt;
&lt;p&gt;We can represent signals in the time or frequency domain. Time can be okay, but many biological processes are repeating and so we can use the frequencies instead for analysis. Basic idea behind fourier analysis, won’t bother going into details here. Read an undergrad textbook.&lt;/p&gt;

&lt;p&gt;It’s hard to model signals as time and space localized, however. For example, we can do a STFT and that will give us a look, but our window determines resolution and we can never have it all.&lt;/p&gt;

&lt;h2 id=&quot;non-linear-approaches&quot;&gt;Non-linear approaches&lt;/h2&gt;
&lt;p&gt;A few non-linear approaches are interested:&lt;/p&gt;

&lt;h4 id=&quot;poincare-plot-analysis&quot;&gt;Poincare plot analysis&lt;/h4&gt;
&lt;p&gt;Plot the signal as a function of itself for various time points. This lets us see variability over time.&lt;/p&gt;

&lt;h4 id=&quot;entropy&quot;&gt;Entropy&lt;/h4&gt;
&lt;p&gt;Lets us quantify how regular or disordered a set A with categories a is. How do we extend to real signals? Approximate entropy. We basically window our signal and look for similar windows within the time series.&lt;/p&gt;

&lt;h4 id=&quot;lyapunov-exponents&quot;&gt;Lyapunov Exponents&lt;/h4&gt;
&lt;p&gt;Measures the rate at which closely spaced trajectories diverge. We want to see how signals relate.&lt;/p&gt;

&lt;h4 id=&quot;detrended-fluctuation-analysis&quot;&gt;Detrended Fluctuation Analysis&lt;/h4&gt;
&lt;p&gt;Useful for revealing the extent of long-range correlation in a time series. Usually we remove the mean, divide the signal into windows, and then study by windows.&lt;/p&gt;

&lt;h2 id=&quot;template-matching&quot;&gt;Template Matching&lt;/h2&gt;
&lt;p&gt;We want to somehow match templates to the signal.&lt;/p&gt;

&lt;h4 id=&quot;cross-correlation&quot;&gt;Cross-correlation&lt;/h4&gt;
&lt;p&gt;Convolve two signals. Problematic because time variations change cross-correlation. We generally will use some time warping to compare. Chih-chun uses this in his morphological variability papers.&lt;/p&gt;

&lt;h4 id=&quot;alignment-techniques&quot;&gt;Alignment-techniques&lt;/h4&gt;
&lt;p&gt;Instead of just correlating, we’ll use time warping. Good because it’s simple and generally gives good performance. However, it’s computationally expensive.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - RAID, High-Performance, Reliable Secondary Storage</title>
   <link href="/eecs584/2014/09/11/eecs584/"/>
   <updated>2014-09-11T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/11/eecs584</id>
   <content type="html">&lt;p&gt;Mechanical disk drives are by far the most common point of failure in modern computer systems. As a result, disk redundancy is of upmost importance in any database system. Even if the individual disks have an acceptable median time to failure (MTTF), the large data storage requirements of any current database requires additional drives, and thus decreases the MTTF by magnitudes of time. RAID, or Redundant Array of Inexpensive Disks, provides standard configurations to aggregate multiple disks into one logical volume. This paper provides an overview of the various RAID configuration options (referred to as &lt;em&gt;levels&lt;/em&gt;), as well as performance and implementation considerations (although some of this was outside the scope of this reading).&lt;/p&gt;

&lt;p&gt;The RAID standard specifies techniques used to partition data and store/restore data, but does not specify exact implementations. These configuration options are split into seven &lt;em&gt;levels&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Level 0 – This level simply uses multiple disks as one large volume, with no redundancy across disks.&lt;/li&gt;
  &lt;li&gt;Level 1 – A basic data redundancy, level 1 creates two copies of data, one on each disk.&lt;/li&gt;
  &lt;li&gt;Level 2 – Level 2 uses error correcting codes as used in main memory hardware. This reduces the disks required to store redundancy, but multiple recovery disks are required to identify failures.&lt;/li&gt;
  &lt;li&gt;Level 3 – Uses bit interleaved parity and stores parity information on extra disks. This allows the volume to use only one parity disk, but for writes the volume must write to that additional disk and thus the volume can only service a single write request at once.&lt;/li&gt;
  &lt;li&gt;Level 4 – Block interleaved parity is similar to level 3 but parity is written in blocks called striping units. Again the parity disk easily becomes a bottleneck.&lt;/li&gt;
  &lt;li&gt;Level 5 – Block interleaved distributed parity is similar to level 4 but instead interleaves parity blocks throughout the regular data. This removes the parity disk as a bottleneck and allows fast small read, large read and write access. The distributed parity is a weakness, however, for small writes that require syncs across multiple volumes.&lt;/li&gt;
  &lt;li&gt;Level 6 – The final level uses Reed-Soloman codes to allow for additional disk failure over level 5. This level offers improved redundancy at the expense of space, and compounds the small write problem of level 5 by using more volumes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper further visits reliability of the different RAID levels, as well as offering a look at differing failure modes and adjusting calculations for more reasonable failure conditions. Besides double disk failure, the paper examines system crashes and uncorrectable bit error for both level 5 and level 6 disk arrays. As expected, level 6 provides strong protection against failure, with even the catastrophic system crash having a 10 year probability of less than 8%. The level 5 RAID is reasonably well protected against double disk failure and system crashes, but has an almost 25% chance to fail due to bit error within 10 years. The final section of our reading concerns physical layout concerns and practical issues in the RAID system.&lt;/p&gt;

&lt;p&gt;I felt this paper did a good job of addressing the RAID specification in a technical but accessible manner. I especially liked that it offered more practical considerations, such as the orthogonal RAID section, while still offering theoretical conclusions. While the paper did briefly discuss and hint that only RAID level 5 and 6 were widely used in large systems, I felt it failed to offer much discussion about why this was the case, or offer counter examples of when lower levels would be used in real-world scenarios. It also would have been interesting to see tangible examples of when varying RAID levels would be used in industrial applications. Last, I would have been interested in a discussion about implementation, especially operating system support. This paper is old enough, however, that such standards may not have evolved yet.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Uninformed Search</title>
   <link href="/eecs492/2014/09/11/eecs492/"/>
   <updated>2014-09-11T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/11/eecs492</id>
   <content type="html">&lt;h2 id=&quot;how-do-we-search&quot;&gt;How do we search?&lt;/h2&gt;
&lt;p&gt;Create a tree with nodes, we’ll define nodes as either:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Root node - Initial state&lt;/li&gt;
  &lt;li&gt;Fringe node - A node along a path in the graph&lt;/li&gt;
  &lt;li&gt;Leaf node - A node without children&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note, there is a difference between a state and a node. A node represents a state, but the state can be represented in different nodes. The path cost can be simple or complex.&lt;/p&gt;

&lt;p&gt;Our node strategy dictates which node to expand to in any particular search situation. Candidates are nodes at the &lt;strong&gt;frontier&lt;/strong&gt;, which we maintain in some kind of queue.&lt;/p&gt;

&lt;h2 id=&quot;types-of-search&quot;&gt;Types of Search&lt;/h2&gt;

&lt;h3 id=&quot;tree-search&quot;&gt;Tree Search&lt;/h3&gt;
&lt;p&gt;We’ll assume the state space is a tree. The frontier is a queue of nodes (or partial paths). This search is basically how we’ll look for a leaf of a tree data structure. The tree is built as we go along, and how we do the actual search is specific to depth or breadth first search.&lt;/p&gt;

&lt;p&gt;What’s the issue with this? If we have a cyclic graph, we can end up looping forever looking for a goal. What if we keep track of visited states?&lt;/p&gt;

&lt;h3 id=&quot;graph-search&quot;&gt;Graph Search&lt;/h3&gt;
&lt;p&gt;A more general search where the state space is a graph. The explored set holds nodes already explored, and when we search we only add nodes to the frontier if they are unexplored. &lt;/p&gt;

&lt;h2 id=&quot;infrastructure-for-search&quot;&gt;Infrastructure for Search&lt;/h2&gt;
&lt;p&gt;Each child node has a state, a parent, and a current path cost to get to that node. Usually we’ll use a type of queue to store the data for our search, and this queue will change types depending on the search. &lt;/p&gt;

&lt;p&gt;How can we measure performance?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Completeness - Can an algorithm find a solution&lt;/li&gt;
  &lt;li&gt;Optimality - Can the algorithm find the optimal solution, usually in terms of path cost&lt;/li&gt;
  &lt;li&gt;Time complexity - How long does it take to search&lt;/li&gt;
  &lt;li&gt;Space complexity - How much memory is needed to search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generally, uninformed search is limited in complexity.&lt;/p&gt;

&lt;h2 id=&quot;uninformed-search&quot;&gt;Uninformed Search&lt;/h2&gt;
&lt;p&gt;The simplest form of search, uses only information available to the algorithm. The distinguishing factor is the &lt;strong&gt;search order&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;breadth-first-search&quot;&gt;Breadth-first search&lt;/h3&gt;
&lt;p&gt;We’ll look for the solution at each layer, generally we’ll find the shallowest solution first. For this search we use a FIFO queue. We test when the node is generated not expanded. This search discards any new path to a state we’ve already explored. The algorithm is complete, and possibly optimal if there is a non-decreasing path cost as we move down nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt; - &lt;script type=&quot;math/tex&quot;&gt;O(b^d)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Space Complexity&lt;/strong&gt; - &lt;script type=&quot;math/tex&quot;&gt;O(b^d)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Both are exponential, and breadth-first will almost always run out of space first.&lt;/p&gt;

&lt;h3 id=&quot;uniform-cost-search&quot;&gt;Uniform-Cost Search&lt;/h3&gt;
&lt;p&gt;When step costs are equal the BFS is optimal. If not, we can use UCS instead, where we care about path cost instead of hops. Uniform-Cost search expands node n with the lowest path cost. Now we use a priority queue for our data structure. The goal is tested for when a node is selected for expansion. All nodes at cost less than c are expanded first. The UCS is optimal and complete (given step cost above zero). For complexity, C-star is optimal solution and &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is step size.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt; - &lt;script type=&quot;math/tex&quot;&gt;O(b^{1+[C^* / \epsilon]})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Space Complexity&lt;/strong&gt; - &lt;script type=&quot;math/tex&quot;&gt;O(b^{1+[C^* / \epsilon]})&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;depth-first-search&quot;&gt;Depth First Search&lt;/h3&gt;
&lt;p&gt;We’ll search from top to bottom first, using a LIFO queue. Although DFS is complete as a graph search, it’s not optimal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Time Complexity&lt;/strong&gt; - Still &lt;script type=&quot;math/tex&quot;&gt;O(b^m)&lt;/script&gt; where m is the maximum depth. This is even worse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Space Complexity&lt;/strong&gt; - In graph search there is no advantage of BFS, but in tree search we can remove a node from memory after it’s been explored. Storage requirement is now &lt;script type=&quot;math/tex&quot;&gt;O(bm)&lt;/script&gt; which is &lt;em&gt;linear&lt;/em&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Problem Solving and Search</title>
   <link href="/eecs492/2014/09/09/eecs492/"/>
   <updated>2014-09-09T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/09/eecs492</id>
   <content type="html">&lt;h2 id=&quot;agents&quot;&gt;Agents&lt;/h2&gt;
&lt;p&gt;Simple reflex agents based only on world at this moment.&lt;/p&gt;

&lt;h3 id=&quot;types-of-agents&quot;&gt;Types of Agents&lt;/h3&gt;

&lt;h4 id=&quot;model-based-reflex-agent&quot;&gt;Model based reflex agent&lt;/h4&gt;
&lt;p&gt;Now we want to use a model to estimate state from the percepts. We don’t need to see the whole world state. Agents in this model will update their state according to a model (make a best guess at true world state) and then match a rule to execute an action.&lt;/p&gt;

&lt;h4 id=&quot;goal-based-agent&quot;&gt;Goal based agent&lt;/h4&gt;
&lt;p&gt;The agent wants to reach a goal, so it looks for desirable states. Now we need to consider what will happen if we take an action and if we want the result.&lt;/p&gt;

&lt;h4 id=&quot;utility-based-agent&quot;&gt;Utility based agent&lt;/h4&gt;
&lt;p&gt;Uses a performance measure to differentiate world states. This uses a utility function, which is an internalization of the agent’s performance. An agent like this can deal with conflicting goals.&lt;/p&gt;

&lt;h4 id=&quot;learning-agent&quot;&gt;Learning agent&lt;/h4&gt;
&lt;p&gt;An agent that learns over time and becomes better as it explores an unknown environment. This model contains a critic to pass information back to the learning element.&lt;/p&gt;

&lt;h3 id=&quot;evaluating-agent-designs&quot;&gt;Evaluating Agent Designs&lt;/h3&gt;
&lt;p&gt;We need to somehow find optimality with respect to PEAS characterization. This can be either via trials or proof.&lt;/p&gt;

&lt;h2 id=&quot;search&quot;&gt;Search&lt;/h2&gt;
&lt;p&gt;We want to represent a path to our goals. In many cases we can represent all possible states as a space, eg the search space. For example, we can represent the search state as a physical map. Right now we’re finding our path through the world, and assuming the world won’t change. How do we do search?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Characterize the World&lt;/li&gt;
  &lt;li&gt;Formulate a Goal&lt;/li&gt;
  &lt;li&gt;Formulate the Problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Search&lt;/strong&gt;: The process of looking for a sequence of actions that records a goal.&lt;/p&gt;

&lt;p&gt;Even for seemingly non-graph problems we can define a state space graph for each state. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Self-selecting, Self-tuning, Incrementally Optimized Indexes</title>
   <link href="/eecs584/2014/09/08/eecs584/"/>
   <updated>2014-09-08T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/08/eecs584</id>
   <content type="html">&lt;p&gt;Our first truly modern technical paper, Graefe and Kuno’s 2010 publication on adaptive indexes provides an efficient technique for automatic index generation and maintenance. &lt;/p&gt;

&lt;p&gt;As modern databases continue to grow to massive scales, access becomes more and more time consuming. Even an O(n) search complexity afforded by a simple scan is too slow when a table can contain millions of entries. Data structures known as indexes can solve this problem by creating a second structure for each column that allows faster access, for example a B-tree. This speed comes at an expense however - the index must be constructed for each column and may occasionally need to be optimized. &lt;/p&gt;

&lt;p&gt;Solutions exist to automatically reindex columns or monitor performance, but this requires a large operation whenever one of these reindexes is deemed necessary. Additionally, many of these tools require a highly trained and potentially expensive database administrator for the system. A proposed solution is to build the index on the fly, creating and improving index structure based on access to that column. This paper builds on previous work, a technique known as “Database Cracking”, but instead of partitioning at each query the algorithm performs a merge step. Database cracking allows for an adaptive index, but is limiting in cases where the entire database index will not fit into local memory. Adaptive filtering is specifically targeted towards architectures where the index resides of disk, as is the case for extremely large databases.&lt;/p&gt;

&lt;p&gt;Adaptive merging starts by creating sorted partitions from the raw data, sorted via quicksort and represented by a partitioned B-tree. The partition size is determined by the amount of physical memory available. Then incoming query ranges search each (sorted) partition and combine all results into a new “merge” partition. Additional queries select more data from the partitions and are merged into the final partition. Eventually all data will be contained in this final partition and the result will be a fully optimized index structure. Because the underlying data structure is the well-studied B-tree, traditional locking and logging structures can be used. Updates such as inserts and deletions can proceed traditionally or using optimized algorithms for small transactions.&lt;/p&gt;

&lt;p&gt;Performance compared to database cracking appears good, and the algorithm is especially useful in its primary use case which is for large, block access disk devices. The overhead of primary partition creation is heavy for adaptive indexing, but if the quicksort operation can fit on primary memory, the resulting structure is very efficient compared to its partition-based sibling. Unlike database cracking, adaptive indexing’s performance varies based on query range size - smaller query ranges are much less efficient. For systems where stability is important, cracking may be a better option. I would have also liked to see the tests performed on other types of data: the tests in the paper were done with permutations of consecutive integers. It would have been good to see, for example, performance on a database with many similar values or low cardinality.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - R-Trees</title>
   <link href="/eecs584/2014/09/07/eecs584-2/"/>
   <updated>2014-09-07T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/07/eecs584-2</id>
   <content type="html">&lt;p&gt;This paper is the seminal paper for the R-Tree, published in 1984 by Antonin Guttman at Berkeley. At the time of writing, current indexing structures could not easily support spatial data such as CAD or mapping data. One dimensional data structures fail on multiple dimensions, and multi-dimensional methods generally fail for practical considerations (ie memory or time complexity) or cannot support ranges of data. The R-tree represents objects by intervals in multiple dimensions.&lt;/p&gt;

&lt;p&gt;The R-tree is similar to a B-tree, but instead of each leaf containing a single value the leaves of an R-tree contain an n-dimensional rectangle which is the bounding box of the spatial object the node indexes. Each parent node contains the children of that node, as well as the bounding box over all of the children. We can search through the tree by recursively searching for children which intersect the query bounding box S. At each leaf node of the qualifying children, we again check for intersection with S and if the leaf qualifies we add the leaf to the set of qualifying records. &lt;/p&gt;

&lt;p&gt;Insertion proceeds similar to a B-tree, except that splitting a node requires a heuristic process to limit to search space of the optimal case. The paper introduces a “quadratic search” as well as a “linear search” and compares the two - linear search exhibits faster inserts but similar resulting search performance. Deletion proceeds by removing the leaf, and if the resulting node is too small by re-inserting the orphan leaves into the next higher tier. &lt;/p&gt;

&lt;p&gt;Guttman’s R-tree is impressive, and its utilization in modern indexing systems demonstrates its power. The algorithm presentation is particularly concise and easy to read. Nonetheless, the paper exhibits a few weaknesses. One common criticism of R-trees is that while searching multiple subtrees under a node may be visited. This means that worst-case performance is impossible to guarantee. Guttman does assert this fact, and additional research after publication attempts to add theoretical bounds. In practice, however, the R-tree is fast and efficient, and so the more complex descendants are rarely used.&lt;/p&gt;

&lt;p&gt;It would have been beneficial for Guttman to compare his new structure to the other data structures he mentions in his introduction. While he dismisses most of them for one limitation or another, I would like to see some sort of demonstration that, for example, corner stitching is indeed inefficient for random searches of large data. I also thought his graphs seemed quite rough, owing to a small sample size. I would have liked to see both a larger sample size in his final plots, as well as using different input data instead of just one example. It’s likely that the hardware at the time of writing was slow and that the included results alone took a long time to generate. &lt;/p&gt;

&lt;p&gt;One final subject that would have been beneficial to touch on are the practical considerations for such an index structure. In a full DBMS, the index structures must support proper locking and logging. Stonebraker claims in “Anatomy of a Database System” that the locking methods used on B+-Trees do not apply to R-trees. Although a full discussion would be out of the scope of the paper there is no mention of this issue even in passing.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Operating System Support for Database Management</title>
   <link href="/eecs584/2014/09/07/eecs584/"/>
   <updated>2014-09-07T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/07/eecs584</id>
   <content type="html">&lt;p&gt;Another Stonebraker paper, although published considerably earlier than the previous readings, this publication seeks to examining operating system services in the context of database management systems. The paper addresses several problems in operating system facilities when applied to database management. Generally, Stonebraker asserts that operating systems offer too much abstraction over important low level facilities, and that while the operating system offers good general purpose performance and usability, it lacks fine grain control that can be devastating in a high performance database management system. The publication is structured in several parts, and this writeup will discuss potential weaknesses of the paper in each.&lt;/p&gt;

&lt;h3 id=&quot;buffer-pool-management&quot;&gt;Buffer Pool Management&lt;/h3&gt;
&lt;p&gt;Modern operating systems offer a “buffer pool” which acts as a main memory cache for the filesystem. This cache improves performance by both writing to disk in large blocks (less disk writes) and caching frequently accessed disk memory (less disk reads). While beneficial in general computing, the algorithms used (eg LRU) lose performance when faced with typical database tasks. Additionally, the operating system offers no services to assist in crash recovery. To remedy these problems, most DBMSes use their own user space buffer pool, duplicating a significant part of the OS.&lt;/p&gt;

&lt;h3 id=&quot;the-file-system&quot;&gt;The File System&lt;/h3&gt;
&lt;p&gt;Stonebraker argues that character array filesystem representation is not useful to a DBMS, and prefers records management. However, it’s unclear what modern systems implement or how much performance would improve. It’s implied that current DBMSes simply build a records management atop a character array system. The author decrys the extra overhead of using three separate trees for a database filesystem, but simply states “the extra overhead … is probably substantial” without giving much analysis of the situation.&lt;/p&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;
&lt;p&gt;In this section the author presents several options for system architecture, especially the idea of process-per-user vs a server model. This section is quite outdated, but brings up important historical failings that architects had to deal with. Of course, modern operating system support for multithreading is quite good, and in most cases the overhead of thread context switching is low. In the author’s later paper “Anatomy of a Database System”, he presents more modern views of the performance problem and explains that process-per-user is generally now a legacy option. Stonebraker presents a solution to the performance issue, but he fails to recognize that market forces will eventually result in better threading support at the OS level.&lt;/p&gt;

&lt;h3 id=&quot;consistancy-control&quot;&gt;Consistancy Control&lt;/h3&gt;
&lt;p&gt;Operating systems fail to provide appropriate support for fine grain locks and crash recovery. Again, this problem is solved in most DBMSes by providing the support in user space, duplicating functionality. &lt;/p&gt;

&lt;h3 id=&quot;paged-virtual-memory&quot;&gt;Paged Virtual Memory&lt;/h3&gt;
&lt;p&gt;Stonebraker asserts that a large file will have a large page table, resulting in page faults for both the file and the table itself. He offers a solution to chunk files in address space, which would force the DBMS to provide significant bookkeeping. He lacks a solution for dealing with larger files, and offhandedly mentions that page file buffering is also a problem. This section feels unexplored, possibly because Stonebraker recognizes that main memory will increase in capacity quickly (his cited example of a 100K page table not fitting in RAM is laughable today). Still, it would have been beneficial for him to offer references to research on the subject if it existed at the time.&lt;/p&gt;

&lt;p&gt;This paper succeeds at its intentions and offers a good overview of OS facilities and the weaknesses inherent when designing a DBMS, especially for the time it was written. However, it does feel rushed in some places where Stonebraker fails to offer adequate solutions or references to potential solutions. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper Review - Anatomy of a Database System</title>
   <link href="/eecs584/2014/09/06/eecs584/"/>
   <updated>2014-09-06T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/06/eecs584</id>
   <content type="html">&lt;p&gt;This educational paper, written by Hellerstein and Stonebraker in 2005, serves to provide an overview of the Database Management System (DBMS) and offer lessons in their design to students and researchers. &lt;/p&gt;

&lt;p&gt;Relational database management systems are currently the most commonly deployed type of DBMS, and have one of the longest histories in computing, stemming from their use in early mainframes and business computing. Just as the authors argued in “What Goes Around Comes Around”, young researchers and developers who were not present in the early days of databases will likely make similar mistakes as their predecessors. This problem is compounded by the lack of complete information on database systems. The authors argue that because of the driving industry forces behind DBMS implementations, it’s difficult to find an academic treatment of the systems as a whole. This paper attempts to fill that void by providing context for academic examples and a birds eye view of the apparently monolithic database management systems.&lt;/p&gt;

&lt;p&gt;As a learning resource, the paper is written in an informative manner and attempts to avoid any clear bias, although we must remember in a critical reading that the authors were responsible for research on Object Relational Database Models and the PostgreSQL project.&lt;/p&gt;

&lt;p&gt;This paper is organized into four main sections (a fifth section exists but isn’t included in this copy). These sections are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Overall architecture&lt;/li&gt;
  &lt;li&gt;Storage Models&lt;/li&gt;
  &lt;li&gt;The Query Processor&lt;/li&gt;
  &lt;li&gt;The Transactional Storage Manager&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h3&gt;
&lt;p&gt;The authors take a look at different architecture models and issues, such as how to split up the processing in the server (processes vs threads) and different historic workarounds (DBMS threads). We see different server setups, from shared memory to shared-nothing and the middleground shared disk. There is a short treatment of admission control (eg how to best degrade performance), and an overview of the modern standard practices.&lt;/p&gt;

&lt;h3 id=&quot;storage-models&quot;&gt;Storage Models&lt;/h3&gt;
&lt;p&gt;This section examines various storage options, the two largest being choosing between direct disk access via the DBMS or operating system disk access via the filesystem. For each option, the two major considerations are spatial control (being able to store sequentially on disk for best performance) and temporal control (being able to tell the system exactly when to write to disk). As a general rule, older systems would manage disk access themselves because operating systems were not yet mature enough to guarantee these controls. Modern operating systems tend to offer better control, but large DBMS products support both options due to legacy.&lt;/p&gt;

&lt;h3 id=&quot;query-processor&quot;&gt;Query Processor&lt;/h3&gt;
&lt;p&gt;The paper now turns to a more detailed discussion of subsystems, starting with the query processor. The processor takes in declarative statements, parses and authorizes the statements, rewires the query to simplify it, and then optimizes the internal representation of the query into a query plan. Finally the executor invokes procedures for the dataflow graph or op-code representation. The executor does this via iterators, which couple the dataflow with control flow. This section also provides a look at how data is located and accessed, as well as modified or deleted. Access methods that change disc data provide additional complications, and this transitions nicely to the final section.&lt;/p&gt;

&lt;h3 id=&quot;transactional-storage-manager&quot;&gt;Transactional Storage Manager&lt;/h3&gt;
&lt;p&gt;The transactional storage manager is a large component of the DBMS that provides concurrency control, recovery, I/O staging, and access methods for organization of data. This section is weak, however, in comparison to the remainder of the paper. This is partly because of the massive scope of this component - the authors assert that an entire undergraduate course in databases is required, as well as a number of other seminal papers. While a full examination of the material would indeed be tedious, a short coverage of some of the history of these methods would be appreciated. The point of this paper is to prevent mistakes by educating students on the history of these systems, and a better treatment of that history would be appreciated.&lt;/p&gt;

&lt;p&gt;The remainder of this section addresses ACID transactions, specifically how locking and logging manage the majority of these requirements (consistency is enforced by the query executor). This section is one of the most technical in the paper, and explores specific issues in each of the subsections of the transactional storage manager. There is a discussion of locks vs latches, as well as various isolation levels available in most DBMSes. This section also covers logging, as well as various combinations of issues pertaining to logs and locks together. Like the transactional storage manager itself, this part of the paper is intertwined and quite nuanced.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;This paper is written in a manner similar to a textbook, but without some of the background information included in each section. Although this paper attempts to bridge the gap between academia and industrial applications, there is highly technical and complex material that requires requisite background knowledge to understand. I think in some cases it would be have helpful to further explain concepts, instead of simply referencing an equally dense academic paper or product manual. In general, however, the paper is quite good and I especially appreciate the author’s inclusion of typical practice in modern database management systems.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Data Models</title>
   <link href="/eecs584/2014/09/05/eecs584/"/>
   <updated>2014-09-05T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/05/eecs584</id>
   <content type="html">&lt;p&gt;This lecture will cover the paper “Anatomy of a Database System” by Hellerstein and Stonebraker. This paper is a retrospective of different data models and a history of the DBMS space in the past 50 or so years.&lt;/p&gt;

&lt;h3 id=&quot;the-beginning---hierarchical&quot;&gt;The Beginning - Hierarchical&lt;/h3&gt;
&lt;p&gt;The first modern database was IMS, developed by IBM for the Apollo program to keep a BOM for the Saturn V. This database was tree structured; every model required a parent and children. This did a good job of separating local schema from the application but had bad physical independence because it specified an implementation. IMS had other problems because representing relationships were tricky or impossible with the tree. &lt;/p&gt;

&lt;p&gt;Additionally it was extremely hard to program because the programmer must do a record at a time search and hand optimize each query.&lt;/p&gt;

&lt;h3 id=&quot;codasyl---networkgraph&quot;&gt;CODASYL - Network/Graph&lt;/h3&gt;
&lt;p&gt;A standard was proposed to fix problems with the IMS model. Called CODASYL, it was a directed graph that made it much easier to model complex relationships. However, the result was so complex that it was slow and complicated to work with. &lt;/p&gt;

&lt;h3 id=&quot;relational&quot;&gt;Relational&lt;/h3&gt;
&lt;p&gt;Ted Codd proposed a new model based on tables. Although is query language was complex, others (including SQL) were proposed. There was heavy pushback from the database community because of the new paradigm, but with IBM’s introduction of DB/2 the model took off and SQL became the de facto query language.&lt;/p&gt;

&lt;h3 id=&quot;alternatives-and-extensions&quot;&gt;Alternatives and Extensions&lt;/h3&gt;
&lt;p&gt;The entity-relationship model was introduced in the mid 70s. This model was less complex than CODASYL but still allowed complex modeling based on relationships with attributes and multiplicities. There was a wave of R++ models which attempted to extend the relational model. Some of the improvements were good (like sets and tuples), but in the end never caught on. &lt;/p&gt;

&lt;h3 id=&quot;object-oriented&quot;&gt;Object Oriented&lt;/h3&gt;
&lt;p&gt;The semantic data model was introduced in the early 80s, and had a ‘class’ relationship system that supported inheritance. Although never adopted, there was a string of object oriented databases that attempted to use OO languages such as C++ to support native database interaction. This too failed as the market was small and niche.&lt;/p&gt;

&lt;h3 id=&quot;object-relational&quot;&gt;Object Relational&lt;/h3&gt;
&lt;p&gt;The first new technique to improve performance, the ORDB allowed the programmer to extend the relational system by specifying his or her own datatypes, operators, and access methods. This was good for things like 2d data or systems that don’t deal with traditional values. Postgres comes out of this model. &lt;/p&gt;

&lt;h3 id=&quot;schema-first-or-later&quot;&gt;Schema First or Later&lt;/h3&gt;
&lt;p&gt;Quick digression to discuss the difference between schema first vs schema later. The authors argue that schema later databases are rare and niche. &lt;/p&gt;

&lt;h3 id=&quot;semi-structured-and-xml&quot;&gt;Semi-structured and XML&lt;/h3&gt;
&lt;p&gt;New models based on XML are out, and while the authors say XML for documents are here to stay they dislike XML for database models and think they’re a CODASYL2. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs598/2014/09/04/eecs598/"/>
   <updated>2014-09-04T00:00:00-04:00</updated>
   <id>/eecs598/2014/09/04/eecs598</id>
   <content type="html">&lt;h2 id=&quot;computational-medicine&quot;&gt;Computational Medicine&lt;/h2&gt;
&lt;p&gt;Why?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discover&lt;/li&gt;
  &lt;li&gt;Predict&lt;/li&gt;
  &lt;li&gt;Manage&lt;/li&gt;
  &lt;li&gt;Treat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Take atrial fibrillation for example. Almost 300,000 people die a year, but we can implant a defibrillator to revive them. However, who do we implant into? This is what we want to answer. This class will focus on using predictive data analysis to supplement medicine.&lt;/p&gt;

&lt;p&gt;Overview of biomedical decision system:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Acquisition&lt;/li&gt;
  &lt;li&gt;Representation&lt;/li&gt;
  &lt;li&gt;Manipulation&lt;/li&gt;
  &lt;li&gt;Analysis&lt;/li&gt;
  &lt;li&gt;Visualization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Class consists of three homeworks and a large project. Each project group will work with a mentor to guide them along their work. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Agents and Rationality</title>
   <link href="/eecs492/2014/09/04/eecs492/"/>
   <updated>2014-09-04T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/04/eecs492</id>
   <content type="html">&lt;h2 id=&quot;agents&quot;&gt;Agents&lt;/h2&gt;
&lt;p&gt;An agent is anything that acts on its environment and perceives its environment. Sensing is required, so a windup toy isn’t an agent. Agents also must pursue a set of pre-specified goals.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Percepts&lt;/strong&gt; - The perceptual inputs at any time&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Percept sequence&lt;/strong&gt; - The history of everything an agent has perceived&lt;/p&gt;

&lt;p&gt;Agents actions can only depend on things it perceives. Note that this can include non-states such as a &lt;em&gt;lack&lt;/em&gt; of a wall.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent function&lt;/strong&gt; - Maps percepts to actions, what we’ll focus on&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent program&lt;/strong&gt; - Implementation&lt;/p&gt;

&lt;p&gt;We must somehow evaluate an Agent’s behavior. The answer generally depends on the task environment. Since rationality depends on the agent’s capabilities, we can say a rational agent tries to maximize a performance measure with respect to its capabilities and prior knowledge.&lt;/p&gt;

&lt;p&gt;Learning is considered a part of rationality - you must learn to be rational.&lt;/p&gt;

&lt;p&gt;Exam note: Proper definition of rationality will be on the exam.&lt;/p&gt;

&lt;p&gt;Rationality - For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.&lt;/p&gt;

&lt;h2 id=&quot;task-environment&quot;&gt;Task Environment&lt;/h2&gt;
&lt;p&gt;Task environments have seven different properties.&lt;/p&gt;

&lt;h4 id=&quot;fully-vs-partially-observable&quot;&gt;Fully vs Partially Observable&lt;/h4&gt;
&lt;p&gt;Can an agent see everything in the environment? If fully observable, there’s no internal state needed because we can see everything.&lt;/p&gt;

&lt;h4 id=&quot;singe-agent-vs-multiagent&quot;&gt;Singe Agent vs Multiagent&lt;/h4&gt;
&lt;p&gt;Multiagent environments mean that an there is another agent that impacts you.&lt;/p&gt;

&lt;h4 id=&quot;deterministic-vs-stochastic&quot;&gt;Deterministic vs Stochastic&lt;/h4&gt;
&lt;p&gt;Deterministic means we can predict perfectly the state of the world. In practice almost every environment is stochastic.&lt;/p&gt;

&lt;h4 id=&quot;episodic-vs-sequential&quot;&gt;Episodic vs Sequential&lt;/h4&gt;
&lt;p&gt;Episodic results in independent events - previous action doesn’t impact the now.&lt;/p&gt;

&lt;h4 id=&quot;static-vs-dynamic&quot;&gt;Static vs Dynamic&lt;/h4&gt;
&lt;p&gt;Dynamic means that the environment changes &lt;em&gt;while&lt;/em&gt; the agent is thinking. &lt;/p&gt;

&lt;h4 id=&quot;discrete-vs-continuous&quot;&gt;Discrete vs Continuous&lt;/h4&gt;
&lt;p&gt;Refers to the way time is handled. Straightforward, same idea as DSP or control theory. Interesting idea, chess is discrete, but chess with a clock is continuous.&lt;/p&gt;

&lt;h4 id=&quot;known-vs-unknown&quot;&gt;Known vs Unknown&lt;/h4&gt;
&lt;p&gt;A known environment means that the agent knows the rules of the environment going in.&lt;/p&gt;

&lt;h2 id=&quot;agent-structure&quot;&gt;Agent Structure&lt;/h2&gt;
&lt;p&gt;An agent consists of its architecture and program. &lt;/p&gt;

&lt;p&gt;A few model types of agents:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stateless&lt;/strong&gt; - Effects of time aren’t modeled&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fixed Model&lt;/strong&gt; - Designer provides model, contrast to &lt;strong&gt;learning model&lt;/strong&gt; where the agent can adapt.&lt;/p&gt;

&lt;p&gt;A few planning types of agents:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reflexive&lt;/strong&gt; - Actions based on preprogrammed conditions&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Predictive&lt;/strong&gt; - Actions based on potential effects&lt;/p&gt;

&lt;h4 id=&quot;table-driven-agent&quot;&gt;Table-Driven Agent&lt;/h4&gt;
&lt;p&gt;Agents choose actions based on states, explicit structure. This is based on percept sequences, which grows large quickly. Table driven &lt;em&gt;must&lt;/em&gt; look at all states, including all past states.&lt;/p&gt;

&lt;h4 id=&quot;simple-reflex-agent&quot;&gt;Simple-Reflex Agent&lt;/h4&gt;
&lt;p&gt;Similar to table driven, but based on what we see &lt;em&gt;right now&lt;/em&gt;. This is a quite simple structure, and in partially observable worlds we can’t make correct decisions.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs584/2014/09/03/eecs584/"/>
   <updated>2014-09-03T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/03/eecs584</id>
   <content type="html">&lt;p&gt;Course will cover advanced database topics. You’re expected to come in with basic database knowledge (which I don’t have). Course will also consist of paper readings. Everyone will sign up for a paper to present and must meet with instructor for feedback a week before actually presenting.&lt;/p&gt;

&lt;p&gt;Student presentations begin on Tuesday Sept 16, which is really soon. Reviews for each week are due Monday before class at midnight.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction</title>
   <link href="/eecs492/2014/09/02/eecs492/"/>
   <updated>2014-09-02T00:00:00-04:00</updated>
   <id>/eecs492/2014/09/02/eecs492</id>
   <content type="html">&lt;h2 id=&quot;what-is-ai&quot;&gt;What is AI?&lt;/h2&gt;
&lt;p&gt;The main four quadrants of AI, although we mostly deal with the last two in this class:&lt;/p&gt;

&lt;h4 id=&quot;thinking-human&quot;&gt;Thinking Human&lt;/h4&gt;
&lt;p&gt;We can learn about our mind, but assessment is hard because &lt;strong&gt;we&lt;/strong&gt; don’t know everything about human thought. &lt;/p&gt;

&lt;h4 id=&quot;acting-human&quot;&gt;Acting Human&lt;/h4&gt;
&lt;p&gt;Generally assessed via Turing test but really this just proves a machine can trick us. Helps us learn about human behavior and let’s us make people more sympathetic to agents.&lt;/p&gt;

&lt;h4 id=&quot;thinking-rationally&quot;&gt;Thinking Rationally&lt;/h4&gt;
&lt;p&gt;Logic provides a strong formal framework for knowledge, but many times it’s hard to translate ideas into concrete logic or formulas.&lt;/p&gt;

&lt;h4 id=&quot;acting-rationally&quot;&gt;Acting Rationally&lt;/h4&gt;
&lt;p&gt;We want to make the best decision given uncertainty; we do this by maximizing some metric.&lt;/p&gt;

&lt;h2 id=&quot;history-of-ai&quot;&gt;History of AI&lt;/h2&gt;
&lt;p&gt;Much of AI’s history is based on philosophy. Eventually the field drew in formal mathematics, along with other fields such as economics. &lt;/p&gt;

&lt;h2 id=&quot;notes-about-this-class&quot;&gt;Notes about this class&lt;/h2&gt;
&lt;p&gt;General and large survey course, won’t cover machine learning or acting human in depth. Six problem sets, programming is in &lt;strong&gt;C++&lt;/strong&gt; only. Class has one midterm subject to change and one final on December 17th.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Information</title>
   <link href="/eecs584/2014/09/01/eecs584/"/>
   <updated>2014-09-01T00:00:00-04:00</updated>
   <id>/eecs584/2014/09/01/eecs584</id>
   <content type="html">&lt;p&gt;EECS 584 will cover a number of advanced topics in big data, databases, and modern data-intensive systems. The specific topics include advanced concurrency control techniques, query processing and optimization strategies for relational database systems, advanced indexing methods, parallel and distributed database systems, map-reduce/hadoop, NoSQL, database-as-a-service (DB clouds), data mining on large databases, data on the web, and topics in database security and privacy.&lt;/p&gt;

&lt;p&gt;In addition to learning about advanced topics in databases, this course will also give you the opportunity to practice important research skills:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You will gain experience reading and critically evaluating original research papers.You will gain experience reading and critically evaluating original research papers.&lt;/li&gt;
  &lt;li&gt;You will practice communicating complex technical material, both orally and in written form.&lt;/li&gt;
  &lt;li&gt;You will complete a small-scale original research project of your own choosing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The prerequisite for this course is EECS 484, equivalent coursework, or permission from the instructor.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Information</title>
   <link href="/eecs598/2014/08/29/eecs598-intro/"/>
   <updated>2014-08-29T00:00:00-04:00</updated>
   <id>/eecs598/2014/08/29/eecs598-intro</id>
   <content type="html">&lt;h2 id=&quot;data-science-for-medicine&quot;&gt;Data Science for Medicine&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; Zeeshan Syed&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Term:&lt;/strong&gt; Fall 2014&lt;/p&gt;

&lt;h3 id=&quot;school-description&quot;&gt;School Description&lt;/h3&gt;
&lt;p&gt;With increasing amounts of medical data becoming available through clinical electronic health record (EHR) systems and consumer-facing fitness and wellness-related wearables there is an opportunity to significantly reduce the burden imposed by major diseases in a data-driven manner. This course focuses on computational advances offering significant improvements in our ability to understand, diagnose, and treat major healthcare conditions. We will explore several foundational topics in data science for medicine, including data representation, data manipulation, data analysis, and data visualization with a review of organ system physiology and common medical data elements. The class will focus on breadth with topics being covered briefly instead of a focus on any single topic in depth to provide an opportunity to sample and apply data science techniques. &lt;/p&gt;

&lt;p&gt;This course has a substantial hands-on component. Students will be expected to supplement theory in data science for medicine with a small number of problem sets and a semester long project on real-world medical data with close mentoring from clinical instructors at the University of Michigan Hospital and the Henry Ford Hospital. Students will be encouraged to think creatively about traditionally hard problems and required to perform independent research. Students will also be exposed to research and potential entrepreneurship opportunities beyond the class.&lt;/p&gt;

&lt;p&gt;The prerequisites for the course include an ability to program and manipulate datasets (EECS281 or equivalent) and knowledge of probability and statistics (STATS250 or STATS412 or equivalent). Knowledge of machine learning and signal processing is not required but may be helpful.&lt;/p&gt;

&lt;h3 id=&quot;meets&quot;&gt;Meets&lt;/h3&gt;
&lt;p&gt;Thursday 3:00pm - 6:00pm in Herbert H. Dow Building room 1017&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Information</title>
   <link href="/eecs492/2014/08/29/eecs492-intro/"/>
   <updated>2014-08-29T00:00:00-04:00</updated>
   <id>/eecs492/2014/08/29/eecs492-intro</id>
   <content type="html">&lt;h2 id=&quot;introduction-to-artificial-intelligence&quot;&gt;Introduction to Artificial Intelligence&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; Emily Mower Provost
&lt;strong&gt;Term:&lt;/strong&gt; Fall 2014&lt;/p&gt;

&lt;h3 id=&quot;school-description&quot;&gt;School Description&lt;/h3&gt;
&lt;p&gt;Fundamental concepts of AI, organized around the task of building computational agents. Core topics include search, logic, representation and reasoning, automated planning, decision making under uncertainty and machine learning.&lt;/p&gt;

&lt;h3 id=&quot;meets&quot;&gt;Meets&lt;/h3&gt;
&lt;p&gt;Tuesday/Thursday 9:00am - 10:30am in Francois Xavier Bagnoud Building room 1109&lt;/p&gt;
</content>
 </entry>
 

</feed>
