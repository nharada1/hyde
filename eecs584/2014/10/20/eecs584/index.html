<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Paper Review - A Comparison of Approaches to Large-Scale Data Analysis &middot; Notes
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Paper Review - A Comparison of Approaches to Large-Scale Data Analysis</h1>
  <span class="post-date">20 Oct 2014</span>
  <p>When Google announced MapReduce in 2004, it was widely viewed as a paradigm shift in distributed data analysis. The architecture was quickly developed into an open source deriviative, designed to run atop the HDFS, itself modeled after Google’s GFS. This open source combination of software, along with other packages, became Hadoop and positioned itself as the spearhead in the data science and big data movement. In this paper (somehow not surprisingly featuring Michael Stonebreaker), the authors argue that MapReduce is not the silver bullet to data warehouse analysis and that parallel DBMSs are more established and outperform MapReduce in most tasks, while offering ease of use that MapReduce lacks.</p>

<p>MapReduce works by running a set of mapper and reducer tasks over a large set of data, distributing the computation in such a way to make restarts easy in the case of hardware failure. This paper highlights the weaknesses of MapReduce, including but not limited to its runtime performance. The authors equate MapReduce’s requirement to program in Java to the domain specific and hardware specific language of the failed CODASYL, and make a similar argument against the system that several of the authors did years ago against graph based models. This is a reasonable complaint, and at the time this was a strength of the paper. Since publication, it has been accepted that a higher level of abstraction would be required, and SQL to MapReduce translators such as Hive were developed. Additionally, the authors argue against the requirement of placing intermediate data on disk, and point out that parallel DBMSs don’t have this requirement. Again, modern development prevails, and Spark/Shark are designed to do exactly this. It’s clear that this paper had some influence on those systems, as the authors have direct influence on the Berkeley database group that developed Spark/Shark. </p>

<p>The paper also presents several comparisons of MapReduce performance against DBMS-X (almost certainly Teradata), and Vertica (another Stonebraker enterprise). Across all benchmarks, the parallel DBMSs appear to be superior. The authors do note that MapReduce has better failsafe mechanisms than the databases tested, but assert that the performance penalty is not worthwhile. I like that the authors make claims as to why the parallel RDBMSs outperform MapReduce. However, many of the performance reasons are the result of such a mature field. Indexing and internal optimizations developed over years and years of industry have made the RDBMS very powerful at extracting as much performance as possible out of the machines it is installed on. While the authors do make valid claims, it’s important to remember how young this new architecture is. I think one weakness of this paper is that it fails to recognize how important MapReduce’s “many low power machines” architecture is to it’s design. The other database systems cannot even be installed on thousands of machines, and in some cases it may be significantly cheaper to buy many commodity machines (especially for large companies like Google who do so at massive scale). This is overlooked, however, and should be addressed by the authors. Overall this paper is a good reality check for developers who get easily excited over shiny new technology - parallel DBMSs have existed since the Gamma project and for mission critical and time critical applications it is likely worth paying the extra for the much faster system.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/eecs583/2015/11/11/">
            Classic Optimization
            <small>11 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/14/eecs583/">
            Pin Paper
            <small>14 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/eecs583/2015/10/12/">
            Paper Reviews 1 and 2
            <small>12 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
