<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/11/06/eecs492/">
        Bayesian Networks
      </a>
    </h1>

    <span class="post-date">06 Nov 2014</span>

    <h2 id="more-probability">More Probability</h2>

<h3 id="marginal-distributions">Marginal Distributions</h3>
<p><script type="math/tex"> P(Y) = \sum\limits_{z \in Z} P(Y, z) </script></p>

<p>We can sum over parts of the over distribution to determine specific attributes. </p>

<h3 id="independence">Independence</h3>
<p>Independent variables mean that the occurence one event tells you nothing about the occurence of another.</p>

<h3 id="bayes-theorem">Bayes Theorem</h3>
<p><script type="math/tex"> P(a|b) = \frac{P(b|a)P(a)}{P(b)} </script></p>

<h3 id="conditional-independence">Conditional Independence</h3>
<p><script type="math/tex"> P(X,Y|Z) = P(X|Z)P(Y|Z) </script></p>

<h2 id="bayesian-networks">Bayesian Networks</h2>
<p>In a Bayesian network we assume conditional independence to create our distribution. By using the product rule we’ll be able to build a model of probabilities and we can infer results from the network.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/11/04/eecs492/">
        Uncertainty
      </a>
    </h1>

    <span class="post-date">04 Nov 2014</span>

    <h2 id="some-final-planning">Some Final Planning</h2>

<h3 id="contingent-planning">Contingent Planning</h3>
<p>If we don’t know about a conditional before hand we need to gather information before planning more actions. This leads to continuous replanning where we can’t plan everything beforehand. This is generally how things work in the real world and with real time. In this case we want <strong>online replanning</strong> to change our action plan as new information comes in. This requires <strong>execution monitoring</strong> in the real world. </p>

<h3 id="multi-agent-planning">Multi-agent Planning</h3>
<p>Having multiple agents whose actions depend on each other introduces complexity. There are a few types of multi-actor setting, but the most complicated is <strong>multiagent</strong> which creates a shared nothing system. Our focus is on multiple simultaneous actions. The way we do this is to decouple actions from actors as much as possible. We want to be <strong>loosely coupled</strong>. One common way to reconcile joint solutions is to use social laws and communication. An example is in sports - play on your side of the court and communicate when things become questionable (“I got it!”).</p>

<h2 id="probability-yay">Probability (yay)</h2>
<p>Describes uncertainty. Cool.</p>

<p>We talk about probability in AI in terms of “possible worlds”, aka the <strong>sample space</strong>. </p>

<h3 id="some-really-chill-axioms">Some really chill axioms:</h3>

<p>Probability expressed from 0 to 1:
<script type="math/tex"> 0 \leq P(\omega) \leq 1 </script></p>

<p>Probability sums to 1:
<script type="math/tex"> \sum\limits_{\omega \in \Omega} P(\omega) = 1 </script></p>

<p>Probabilities add for mutually exclusive events:
<script type="math/tex"> P(a \vee b) = P(a) + P(b) </script></p>

<p>Probabilities add but must subtract intersect for non-mutually exclusive events:
<script type="math/tex"> P(a \vee b) = P(a) + P(b) - P(a \wedge b) </script></p>

<h3 id="types-of-probabilities">Types of Probabilities:</h3>
<p>Priors: Unconditional chances
<script type="math/tex">P(X=x)</script></p>

<p>Conditional Probabilities: 
<script type="math/tex">P(a|b) = \frac{P(a \wedge b)}{P(b)}</script></p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/11/03/eecs584/">
        Paper Review - Generic Schema Matching with Cupid
      </a>
    </h1>

    <span class="post-date">03 Nov 2014</span>

    <p>While the previous paper told us the hassle that schema matching brought the developer, this paper actually presents a working solution to schema matching, developed by Microsoft. As outlined in the survey paper, schema matching is a pervasive and particularly difficult problem that is best performed by domain expert humans, even if the work is tedious and seemingly brain-dead. This paper describes Cupid, a comprehensive schema matching tool that, much like many of the new databases we study in this class, builds its solution on the partially complete work of others instead of designing a fully novel solution. In this sense we can see it almost like the Gamma project in that it took many incomplete or non-commercial ideas and implemented them into one design. </p>

<p>The paper owes so much of its design to previous schema matchers that it first outlines the various types of matching techniques in use today, followed by the common products available and how they use those techniques. I actually really like this system and wish more papers did this, it presented a very good overview of the previous work in the field. Cupid considers itself most similar to DIKE and MOMIS in terms of general schema matching, and thus compares itself to these packages. Both of these packages are schema matchers that use both element and structure level information. Cupid also seems to mostly take this approach, and so it seems that, like in machine learning, the kitchen sink technique works best in the real world (see Netflix algorithm, etc). The paper outlines the various techniques it uses for both linguistic and structure matching. To deal with similar schema names, Cupid uses a measure of word similarity along with a database of similar words and homonyms to try and match schema that use different but similar labels. For structure matching, Cupid uses a fairly complex system based on structural similarity. This similarity matching uses both the linguistic similarity described previously, as well as the local structure of the schema to make inferences about matching elements. While the first section describes a tree schema, the paper later expands the tree into a graphical model to allow schema to express non-hierarchical possibilities. This map acts as a directed graph, allowing three different types of nodes that can describe one to one, one to many, and inheritance relationships. While no doubt this model is powerful, it introduces a considerable amount of complexity. The paper finally outlines other features and compares Cupid to other schema matchers.</p>

<p>One weakness of this paper is possibly owed to its release date (2001), but its linguistic model seems quite limited. The paper uses a thesaurus to determine abbreviations and similar words, but these are only useful in an academic sense. A better solution would be to create a translation mapping using real world data, such as something you would use in the Google “did you mean” algorithm. This would allow for the variability of human writing, as well as provide a constantly evolving solution for slang and new words or abbreviations. I’m also surprised that the similarity of the tree is not computed in a probabilistic sense. The authors structure identification is inherently a deterministic process, and thus is likely to make more errors compared to a system that could be run multiple times with different parameters (until convergence) to allow for higher accuracy. I do think the biggest strength is still probably in the structure mapping technique, however, and I like the idea of using a directed graph for a schema model, as this creates a very general framework.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/11/01/eecs584/">
        Paper Review - Why Your Data Wont Mix: Semantic Heterogeneity
      </a>
    </h1>

    <span class="post-date">01 Nov 2014</span>

    <p>Data is ubiquitous in any company or organization, but in many cases incorporating data from different sources presents a problem. Even within one organization, different data sources are designed using different schemas. Trying to reconcile all those slightly different schemas is currently an arduous process that must be done by hand. A human user must use their own domain knowledge to match schemas and determine which fields match and how to deal with missing or incomplete fields. This paper, which is more of a survey paper than anything else, offers a look at the problems that data mixing presents, and explains why the problem is so hard. The author also examines potential approaches that could work in the future, as well as problems he forsees moving forward.</p>

<p>Schema matching is a hard problem, but an automated solution offers many benefits. The most general and largest benefit is that as data becomes more fluid, the amount of analysis and calculation that is possible in a fixed amount of development time grows. The author presents specific instances where data matching becomes useful, for example indexing the deep web. The deep web refers to any dynamically generated content behind search forms, and a search engine that could access and index these databases would provide magnitudes more information in some queries. To do this, the search engine would need a way to interpret the many different forms available from various websites, and this is not currently possible. The author argues that the domain knowledge required combined with technical expertise makes it hard to find people to match by hand. He then examines several state of the art data matching systems, and points out that each system has flaws and a single method is fragile and unlikely to succeed. While some systems attempt to combine solutions, the paper argues for machine learning based techniques that learn from a training set how to match schema. Such a system, the author argues, would be much more stable and likely to offer an automated solution that can handle real world data. </p>

<p>One strength of this paper is the looking forward section where the author gives his outlook of the future and the problems facing semantic heterogeneity. He correctly predicts that the size of data in the near future will make many of the techniques in the state of the art intractable. He also points out that managing a <em>dataspace</em> as opposed to a database is going to be a problem. A dataspace is comprised of participants and relationships, and as technology becomes more and more ubiquitous in our lives we can expect this problem of dataspaces to grow just as the paper predicts. One can imagine that the data for a human of the future would contain boundless dimensions, ranging from health data to smart devices to internet connected appliances. </p>

<p>The strength of this paper is also a weakness in some respects - the author doesn’t really offer any ideas on how to face the problems he presents. The paper claims that the future roadblocks are just seeing research, but clearly did not expect the amount of heterogeneous data that would be available in only a few years. I also think the discussion about machine learning is lacking - it reads as a cursory glance but we’re never enlightened with how exactly machine learning could be applied to the problem of schema matching. I assume that a thorough treatment is available in other resources.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/10/30/eecs492/">
        Real World Search
      </a>
    </h1>

    <span class="post-date">30 Oct 2014</span>

    <h2 id="continue-less-real-planning">Continue Less Real Planning</h2>
<p>Defining the search problem:</p>

<p>Initial state, actions avail, resulting state. This is backwards search as applied to a planning graph instead of a search tree. In GraphPlan finding a solution is intractable in the worst case, so we need heuristics to guide action selection. Also, this tells us nothing about timing.</p>

<h2 id="scheduling">Scheduling</h2>
<p>Up until now we’ve been assuming everything takes one time step. How do we solve scheduling? We’ll use the <strong>critical path method</strong>. In this way, a critical path is one that has the earliest start and end times for each action. Think critical path from business scheduling.</p>

<p>The path goes through a graph, a partial order plan, and a linearly ordered sequence of actions. For each action off the critical path, we have some slack time. We want to calculate early start and late start for each action. In this new model we can represent variables in our sentences. </p>

<p>Partially observable environments add complexity: during planning we need a model of sensors. The agent has to reason in real time while executing the plan. We’lll introduce a thing called ‘percept schema’ that augments the percepts from PDDL.</p>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page16">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page14">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
