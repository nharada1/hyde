<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/02/11/eecs586/">
        Optimization
      </a>
    </h1>

    <span class="post-date">11 Feb 2015</span>

    <h2 id="different-types-of-optimizing">Different Types of Optimizing</h2>
<ul>
  <li>NP-hard: Try every possible solution</li>
  <li>Dynamic Programming: Try everything but do it efficiently</li>
  <li>Greedy: Don’t try everything</li>
</ul>

<h2 id="optimal-binary-search">Optimal Binary Search</h2>
<p>Given keys and probabilities in advance. Our first try is an exhaustive search: just try everything. Obviously that sucks. We could also try a quick-sortish method where we find the optimal tree on smaller trees until the base case. This is better, but not optimal – we’ve already seen many of the trees we create. We are solving subproblems repeatedly. We need to <em>memoize</em> our values we’ve seen. This is a key part of dynamic programming.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/02/11/eecs545/">
        Bayesian Learning
      </a>
    </h1>

    <span class="post-date">11 Feb 2015</span>

    <h2 id="gaussian-processes">Gaussian Processes</h2>
<p>A Gaussian process is a probability distribution over functions y(x), such that the set of values of y(x) evaluated jointly have a Guassian distribution. What determines a GP? A mean function, a covariance function, and that’s all. Normally we say the mean is zero, so the prior is represented by the kernel entirely. </p>

<p>Linear regression, and Gaussian processes. Bayesian linear regression is just a special instance of a Guassian process. Features in Bayesian LR are kernel functions in GPs.</p>

<p><strong>Fix this</strong>
<script type="math/tex">y(x) = w^T \phi(x)</script> with a weight prior <script type="math/tex">p(w) = N(w \mid 0, </script></p>

<p>A Gaussian process will generate data from its parameters (plus noise) but how do we actually learn one of these processes? We can find the next covariance given the current data. The predictive distribution is a Gaussian whose mean and variance depend on X(n+1). Try and ifnd a correspondence between GP and linear regression on your own time.</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs586/2015/02/09/eecs586/">
        More Trees
      </a>
    </h1>

    <span class="post-date">09 Feb 2015</span>

    <h2 id="red-black-trees">Red Black Trees</h2>
<p>Red black trees are always balanced by nature. For red black tree insertion we generally need to rotate trees. </p>

<h2 id="problem-for-freshman">Problem for Freshman</h2>
<p>We want a BST with insert, delete, search, and search by rank (order statistic). By order statistic, something like “find the 17th key”. Once we have the count and the tree constructed, the second two become trivial. We want insert and delete. We can use “augmented trees” where the trees have a count at each node. When we rotate, we update each node with the count of the child nodes. This additional operation is supported by the augment, and we can still insert and delete in log(n) time. </p>

<p>To update the count of each node, we only require two rotations at most during insertion, and we can compute size attributes in constant time, so insertion is lg(n) because insertion is just lg(n). Deletion is similar, reqiring only 3 rotations at most and is again lg(n). Doing augmentaion like this changes our indexing, and changes which query is efficient. We’ll also look at interval trees, read the book for this one.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/02/09/eecs545/">
        SVM and Algebra
      </a>
    </h1>

    <span class="post-date">09 Feb 2015</span>

    <h2 id="svm-reviews">SVM Reviews</h2>
<p>It’s an optimization problem, we want to solve a dual optimization with a Lagrangian function. We introduce slack variables to allow misclassifications. The dual formulation just introduces more constraints. The higher the C value the stricter the classifier is. A value of C=inf will result in the hard boundary classifier.</p>

<h2 id="multivariate-gaussians">Multivariate Gaussians</h2>
<p>Covariance matrix <script type="math/tex">\Sigma</script> is the outer product of <script type="math/tex">[x-E[x]]</script></p>

<p>Partitioned Gaussians: We can combine Gaussians via partitions xa and xb, which allows us to partition the covariance matrix into only four covariances! Also notational is that the inverse of the covariance is the precision matrix <script type="math/tex">\Lambda</script>. The partitioned Guassian can use the Woodbury Matrix Inversion Lemma to invert which is easier than computing it by hand. </p>

<p>In general we can treat matricies that are partitioned as matricies themselves. That is, linear algebra we do with numbers can extend to matricies. Fact: given a linear combo of Gaussian random vars, its distribution is also Gaussian. Its marginal distribution is also Gaussian. Its conditional distribution is also Gaussian.</p>

<h2 id="bayes-theorem-for-gaussians">Bayes’ Theorem for Gaussians</h2>
<p>We define our probabilities in terms of Guassians. Then we can find the posterior Gaussians just like with normal numbers but in symbolic form.</p>

<h1 id="bayesian-linear-regression">Bayesian Linear Regression</h1>
<p>Doing regression from a truly Bayesian perspective. We have a likelihood and a prior, which gives us a posterior. We also now have probabilities for the weights, which we might assume has zero mean and identity covariance. We get the variance of the weights fo’ free, which allows us to do regularization using weight variance instead of L2 norm. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs545/2015/02/04/eecs545/">
        Trees
      </a>
    </h1>

    <span class="post-date">04 Feb 2015</span>

    <h2 id="binary-search-trees">Binary Search Trees</h2>
<p>Binary search has insert, delete, and search. These operations are the important part, because searching is easy. To search we just do a compare at each level of the tree, we can search in log(n) no problem. Insert is also easy, just do a search and if it’s not where we expect put in our element. Delete is a little more complicated. If our element to delete is a leaf we remove it. If it’s a node with one child, just remove it and connect the severed parts. If it’s a node with two children, we could just promote one subtree and then hang the other one off of that. This is valid but poor. The better option is to take the smallest part of the right tree and promote it to the delete node’s location.</p>

<p>For a given tree, the worst cases are:</p>

<ul>
  <li>Search: <script type="math/tex">\Theta(n)</script> for an unbalanced tree</li>
  <li>Insert: <script type="math/tex">\Theta(n)</script></li>
  <li>Delete: </li>
</ul>

<p>The best case to build up a tree is n log n, because any faster would mean that we have a sorting algorithm that’s better than n log n, and that’s provably impossible. </p>

<p>Expected Search time is different. If all trees are equally likely the expected depth is about <script type="math/tex">\sqrt(\pi n)</script>. This assumption is dumb though…we don’t build trees to be equally likely. We’ll instead assume that all input orderings are equally likely (no duplicates). </p>

<p>Time to insert = time to find
average search time = time to build/n</p>

<p>When we build up trees the balanced tree is most likely. The average number of comparisons to build a tree is the same as quicksort. Building a tree is basically quicksort anyway. </p>

<p>Now something totally different:</p>

<p>Assume each new insertion is a new key we’ve never seen.</p>

<p>IIIDI, this is 72 equally likely trees (4!x3 options for each D). There are 25 possible balanced trees, which is better than 1/3 which is good. </p>

<h2 id="red-black-trees">Red-Black Trees</h2>
<p>Every node will be either red or black (these are NOT keys). Rule are:</p>

<ul>
  <li>Red parent cannot have red child</li>
  <li>From any node to a leaf, the number of blacks is always the same</li>
</ul>


  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page9">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page7">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
