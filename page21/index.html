<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Notes &middot; Class notes for Nate Harada
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-146052-15', 'getpoole.com');
    ga('send', 'pageview');
  </script>

  <!-- LaTeX Support -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Notes
        </a>
      </h1>
      <p class="lead">Class notes for Nate Harada</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>
      <a class="sidebar-nav-item" href="about">About</a>
      <a class="sidebar-nav-item" href="http://blog.nateharada.com">Blog</a>

      <p></p>
      
          <a class="sidebar-nav-item" href="/categories/eecs492">EECS492</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs598">EECS598</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs584">EECS584</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs545">EECS545</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs586">EECS586</a> 
      
          <a class="sidebar-nav-item" href="/categories/eecs583">EECS583</a> 
      

    </nav>

    <p>&copy; 2015. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/23/eecs584/">
        Paper Review - OTLP Through the Looking Glass and What We Found There
      </a>
    </h1>

    <span class="post-date">23 Sep 2014</span>

    <p>Online transaction processing is an important subset of general database management systems that generally deals with business or commercial transactions. OTLP systems require high throughput and low latency, as most serve many users and make frequent updates. However, because the number of customers and their transaction information does not scale with Moore’s law, OTLP databases are relatively small by today’s standards. Written in 2008, this paper argues that it’s time for a complete overhaul of OTLP database systems. The authors prove their point by completely removing most of the functionality of the Shore OTLP database to show how much code is wasted.</p>

<p>A key point in this paper is the use of main memory databases. In contrast to a traditional DBMS which lives on disk, a main memory database keeps all data in memory. This reduces or eliminates many problems with disk based DBMS systems, specifically rendering most of the disk access optimizations obsolete. Many of the architecture observations the authors make in this paper are implemented in their H-Store database system. The paper discusses the idea of a cluster of shared nothing machines that are memory resident and single threaded. Each CPU core acts as its own transaction manager, with its own memory space to completely eliminate locking. Instead of logging to disk, these systems would simply migrate data to available sites when there is a failure, thus maintaining availability without disk overhead.</p>

<p>The authors attempt to demonstrate the amount of CPU instruction that could be saved by eliminating and optimizing Shore. They remove sections one at a time, including locking, latching, multithreading, buffer management, and bookkeeping. At the end, they have a bare minimum database system that is both simpler and much faster. The resulting database system can be seen as a proof of concept for the performance of the H-Store system that the lab later develops. This paper lays the groundwork for H-Store and presents concrete evidence that much of the machinery of a modern DMBS is outdated and performance of memory-resident DBMSes can be improved by adapting a new architecture. In the final section of the paper the authors concretely state their vision for a new OTLP engine, and most of these features appear in H-Store.</p>

<p>This paper fails to address catastrophic failure events in main memory DBMSes, for example if the power goes out in a database center. I assume that a form of checkpointing or snapshotting would take place in any production main memory database, but this paper fails to address this. Additionally, even by removing most of the code from Shore, the authors fail to consider pre-compiled transactions in their speed comparisons or recommendations. A main memory, distributed DBMS must trade flexibility for speed and consistency. This tradeoff occurs in H-Store, and indeed must take place to remove locking and latching. However, this paper removes locking and latching without mentioning the flexibility trade offs that must take place in order to produce a production system similar to the one they are advocating.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs492/2014/09/23/eecs492/">
        Adversarial Search
      </a>
    </h1>

    <span class="post-date">23 Sep 2014</span>

    <h2 id="two-player-games">Two Player Games</h2>
<p>For now we’ll deal with one other agent only, in fully observable deterministic environments. How do we characterize the other agent? We say the opponent is exactly opposed to us. We’ll say it’s a <em>zero sum game</em>, meaning the rewards don’t change game to game.</p>

<p>These problems are hard, because the search trees are generally massive and exponential. How do we deal with this?</p>

<ul>
  <li>New approaches must define the optimal move differently.</li>
  <li>We need to prune our search tree.</li>
  <li>Likely need a Heuristic evaluation function.</li>
</ul>

<h2 id="minimax">Minimax</h2>
<p><strong>Max ALWAYS goes first</strong> (this will be on the exam). Also note there are rewards <em>and</em> penalties for the winner and loser.</p>

<ul>
  <li>Terminal test - Returns true when the game is over.</li>
  <li>Utility - Defines a numeric value for a game that ends in a terminal state.</li>
</ul>

<p>Game tree now models both players explicitly. </p>

<h4 id="example---tic-tac-toe">Example - Tic Tac Toe</h4>
<p>We’ll say a win is +1 and a loss is -1, a tie is 0. </p>

<h2 id="decisions-in-minimax">Decisions in Minimax</h2>
<p>We will assume the opponent is playing optimally. Then we re-run minimax next turn.</p>

<h2 id="alpha-beta-pruning"><script type="math/tex">\alpha-\beta</script> Pruning</h2>
<p>Minimax is inefficient, has <script type="math/tex">O(b^m)</script> time complexity. We’ll prune the tree by ignoring the moves <em>min</em> will ignore. Pruning like this lets us remove entire subtrees in many cases. This depth first search depends heavily on the ordering of the tree. Alpha-beta pruning can generally double our search depth!</p>

<h2 id="evaluating-in-real-time">Evaluating in Real Time</h2>
<p>With deep trees we can’t even get to the end. We’ll need to introduce an evaluation function to score a mid-level node. The heuristic should be strongly correlated with a good outcome. Introducing this adds uncertainty, however. Uncertainty doesn’t change anything about the environment, but changes the computation. </p>

<p>What if we evaluate the function via features?</p>

<script type="math/tex; mode=display">E(s) = \sum \limits_{i=1}^n w_i f_i(s)</script>

<p>This misses dependencies however. We can use a heuristic in place of final state once we hit a certain depth. </p>

<ul>
  <li>Evaluation Function - Guess of utility function</li>
  <li>Utility Function - Final state of a game</li>
</ul>

<h2 id="stochastic-games">Stochastic Games</h2>
<p>We can extend games to include random elements but this increases complexity by a large margin. We’ll add <em>chance nodes</em> and calculate expected values. This is called <strong>expectiminimax</strong>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/21/eecs584/">
        Paper Review - On Optimistic Methods of Concurrency Control
      </a>
    </h1>

    <span class="post-date">21 Sep 2014</span>

    <p>Traditional database systems require stringent concurrency controls the ensure that there is no data loss or corruption when multiple agents are accessing the system. In most modern DBMSes, this is managed through the use of locks, which provide exclusive access to some subset of the database. While this approach manages concurrency while preventing corruption, it is inherently a heavy overhead system, and locks introduce complexity at both runtime and during system design. </p>

<p>This paper proposes a new approach to concurrency, which it refers to as optimistic methods of concurrency control. The new method is motivated by the observation that in most cases, locks aren’t actually required to prevent corruption. Only in a small number of cases will locks actually be utilized, and the remainder of the time they simply create overhead and performance issues. Thus, the new system does away with locking entirely, writing and reading as if the database is being accessed only by a single system at a time and then using a strong validation algorithm to discard and retry operations that would result in an inconsistent state. The validation phase of the new method checks to see if the operations performed are serializable, and if they are not it assumes corruption occurred and aborts the transaction. It accomplishes this validation through the use of transaction numbers, which are assigned at the end of the read phase to maximize system throughput.</p>

<p>The authors make other considerations, such as what to do when a validation repeatedly fails. This represents the worst case scenario for the performance of the system, and after a set number of tries the transaction is given exclusive access to the entire database, forcing it to complete. The paper examines the “critical section” of transactions, and examines cases of parallel operations (such as multiple CPUs and parallel validation). Finally, the authors do a rigorous analysis of how their method works on B-trees, including calculating the probability that a calculation requires a restart. I really like that the authors include this calculation, although the authors find a good case for their calculations. The optimistic approach seems like a good idea when domain knowledge tells you that you will likely not have many conflicts, but clearly performance will degrade significantly if this assumption isn’t true. As the authors assert, a technique combining locking and optimism would be ideal to support cases not on either extreme of the spectrum (which most real world cases would be). It would also be interesting to examine lock-free methods to supporting the other ANSI SQL isolation levels besides simply serializability, such as READ COMMITTED or UNCOMMITTED. It’s unclear whether or not the authors implemented the idea they proposed, but if they did I would have liked to see real world comparisons of access performance on both real and synthetic data.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/20/eecs584/">
        Paper Review - A Critique of ANSI SQL Isolation Levels
      </a>
    </h1>

    <span class="post-date">20 Sep 2014</span>

    <p>The ANSI SQL standard defines isolation levels in databases in terms of what is allowed to occur in the database. In this influential 1995 paper, the authors argue that this type of specification fails to properly describe isolation levels, and will allow certain corner cases to occur anyway. In the ANSI standard, phenomena are expressed in a written, English form that allows room for interpretation. This paper claims that isolation, instead of being expressed in terms of phenomena, should rather be expressed in terms of locking levels. The authors also characterize locking in a rigorous and more mathematically formal way that removes ambiguity from isolation levels. The mathematical formalism introduced to SQL isolation is an important contribution introduced by this paper, and terminology introduced here such as “loose” vs “strong” phenomena interpretation creates a framework that allows the authors to offer criticisms of the ANSI SQL standard.</p>

<p>The mathematical tools introduced in this paper allow the authors to introduce additional phenomena types: cursor stability and snapshot isolation. Of particular importance is snapshot isolation, which is widely deployed in modern database systems. Snapshot isolation offers better performance than serializable isolation while offering most of the same protections against anomalies. The paper shows, however, that snapshot isolation is <em>not</em> the serializable. This fact implies a failing of the ANSI SQL isolation levels - snapshot isolation exhibits no prohibited phenomena as defined by the ANSI standard, yet is not serializable and can still allow errors. The paper also offers a short description of other multi-version systems and compares them to snapshot isolation.</p>

<p>I like that this paper offers such a comprehensive analysis of isolation levels and that it offers precise definitions where ANSI failed. I would be interested to see how database definitions such as ANSI have adapted in the face of new evidence. A cursory Google search indicated that although this paper has been well cited and well received, there has not been an overhaul in the ANSI isolation level standards. I also think that the “first-committer-wins” feature seems like an easy but limiting solution to preventing lost updates in the snapshot transaction system. In a system with heavy latency or transaction costs, it seems that the DBMS could possibly resolve conflicts automatically. This would require some sort of merging algorithm, which is beyond the scope of this paper and would have tradeoffs in lost accuracy. </p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/eecs584/2014/09/19/eecs584/">
        Paper Review - Optimization of Sequence Queries in Database Systems
      </a>
    </h1>

    <span class="post-date">19 Sep 2014</span>

    <p>Time series data has important applications in finance, biology, business, and many other disciplines. Additionally, time series analysis has gained traction in the machine learning and data mining community as a method of non-linear system analysis. This paper, written in 2001 at UCLA, offers a look at how to deal with time series data in the context of a database query. Especially in cases where large amounts of data are stored over time (eg stock market), efficient and easy queries on the database should be possible. For traditional DBMSes and standard SQL, it is difficult to concisely and efficiently create time series queries. </p>

<p>This paper begins by proposing an extension to the SQL language, called SQL-TS. This superset of SQL includes several constructs to deal with time series data. Of most importance is the FROM .. AS clause that will preserve time series ordering in query requests. This clause can contain star (wildcard) terms to allow for an variable amount of data returned while maintaining proper ordering. This is a powerful construct in terms of expressiveness, and to me is the major syntactical contribution of this paper. SQL-TS also includes CLUSTER BY and SEQUENCE BY clauses to simplify return ordering.</p>

<p>The most important contribution of this paper is the actual implementation of an efficient search method in time series databases, modeled after the Knuth-Morris-Pratt string search algorithm. The KMP algorithm uses data available in the search string itself to prevent backtracking. Starting from the beginning, the algorithm compares corresponding characters until matching fails. However, if the matching fails after <em>i</em> matches, the algorithm remembers the previous comparisons to potentially skip comparing up to <em>i</em> characters of the string its searching through. The paper introduces a heavily generalized method of the KMP string search that they call Optimized Pattern Search. This search generalizes KMP to support general predicates and more general objects. Importantly, the new algorithm also includes support for wildcard queries, which is a powerful feature of the SQL-TS language. The algorithm cleverly captures all pairwise relations between pattern elements by computing “precondition logic matricies”, and uses shifts within these matricies to define pattern matches.</p>

<p>The experimental results in this paper are quite weak. The authors use a single experiment to test their algorithm. One of the primary strengths of the KMP algorithm is that is offers a good worst case runtime. General, random string searches will likely perform just as well with a naive search when compared to KMP. However, in corner cases, such as long repeating elements, the naive version will degrade immensely. The tests performed here are on real data only, and with complex queries that may or may not represent real world loads. There is also no mention of space complexity for this method, or a comparison of this methods space requirements to a naive pattern match (which, admittedly, would not support most functions of OPS). Lastly, I would be interested to see this algorithm of pattern matching compared to other work in the field. </p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page22">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page20">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
